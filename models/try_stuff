/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fd7eef30eb0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fd7eeec5970>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fd7eeec5a60>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fd75fa99040>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fd75fa990a0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fd75fa994c0>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fd75e8de550>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fd75e8de5b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fd75e8de6d0>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f001ac2ef10>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f001a4990d0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f001a4991c0>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7eff8baae760>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7eff8b292d60>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7eff8b824700>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fb61ddecf10>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fb61d65d0d0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fb61d65d1c0>]
===TRAIN MODELS===

run_model
('Training Accuracy mean: ', 0.2192638821899891)
('Validation Accuracy mean: ', 0.16298355907201767)
('Training Loss mean: ', 3.3815868377685545)
('Validation Loss mean: ', 3.7700061798095703)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking (Masking)            (None, 500, 21)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, 500, 128)          44032     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 500, 128)          98816     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 500, 64)           41216     
_________________________________________________________________
attention (attention)        (None, 64)                564       
_________________________________________________________________
dense (Dense)                (None, 32)                2080      
_________________________________________________________________
batch_normalization (BatchNo (None, 32)                128       
_________________________________________________________________
dropout (Dropout)            (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16)                64        
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               2176      
=================================================================
Total params: 189,604
Trainable params: 189,508
Non-trainable params: 96
_________________________________________________________________Finished run_model in 919.8892 secs


===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7fb61ddecfd0>, 'x_test': None, 'y_test': None, 'model': None}
report

              precision    recall  f1-score   support

           0       0.25      0.73      0.37       358
           1       0.50      0.08      0.14        12
           2       0.00      0.00      0.00        19
           3       0.00      0.00      0.00        79
           4       0.25      0.02      0.03        55
           5       0.00      0.00      0.00        58
           6       0.50      0.04      0.08        45
           7       0.00      0.00      0.00        48
           8       0.00      0.00      0.00        10
           9       0.00      0.00      0.00        21
          10       0.00      0.00      0.00        15
          11       0.38      0.58      0.46        36
          12       0.00      0.00      0.00        12
          13       0.00      0.00      0.00        25
          14       0.00      0.00      0.00        20
          15       0.58      0.32      0.41        22
          16       0.00      0.00      0.00        23
          17       0.26      0.42      0.32       118
          18       0.00      0.00      0.00        18
          19       0.00      0.00      0.00        13
          20       0.00      0.00      0.00        90
          21       0.00      0.00      0.00        12
          22       0.00      0.00      0.00        25
          23       0.00      0.00      0.00        12
          24       0.00      0.00      0.00        22
          25       0.60      0.08      0.14        37
          26       0.00      0.00      0.00        17
          27       0.00      0.00      0.00        35
          28       0.00      0.00      0.00        12
          29       0.00      0.00      0.00        36
          30       0.14      0.34      0.20        32
          31       0.57      0.21      0.30        39
          32       0.37      0.36      0.36       747
          33       0.19      0.73      0.30        74
          34       0.85      0.29      0.44        58
          35       0.41      0.19      0.26        48
          36       0.21      0.10      0.13       502
          37       0.70      0.48      0.57       241
          38       0.00      0.00      0.00        33
          39       0.32      0.53      0.40       344
          40       0.58      0.26      0.36       191
          41       0.00      0.00      0.00        32
          42       0.32      0.35      0.33       384
          43       0.18      0.56      0.28       117
          44       0.50      0.52      0.51       436
          45       0.00      0.00      0.00        49
          46       0.53      0.87      0.66       402
          47       0.00      0.00      0.00        17
          48       0.50      0.07      0.12        42
          49       0.37      0.40      0.39        77
          50       0.68      0.81      0.74       172
          51       0.00      0.00      0.00        20
          52       0.39      0.31      0.34       499
          53       0.37      0.35      0.36       100
          54       0.00      0.00      0.00        11
          55       0.19      0.33      0.24       103
          56       0.00      0.00      0.00        18
          57       0.00      0.00      0.00        10
          58       0.82      0.51      0.63        35
          59       0.23      0.32      0.27       231
          60       0.96      0.47      0.63        58
          61       0.00      0.00      0.00        29
          62       1.00      0.04      0.08        48
          63       0.00      0.00      0.00        50
          64       0.00      0.00      0.00        34
          65       0.00      0.00      0.00       155
          66       0.00      0.00      0.00        14
          67       0.19      0.02      0.03       314
          68       0.00      0.00      0.00        63
          69       0.26      0.82      0.40       307
          70       0.00      0.00      0.00        68
          71       0.18      0.03      0.05        66
          72       0.00      0.00      0.00        14
          73       0.00      0.00      0.00        25
          74       0.00      0.00      0.00        18
          75       0.00      0.00      0.00        59
          76       0.33      0.31      0.32       206
          77       0.00      0.00      0.00        77
          78       0.56      0.17      0.26        59
          79       0.35      0.24      0.29       139
          80       0.00      0.00      0.00        42
          81       0.04      0.03      0.03       175
          82       0.33      0.02      0.04        43
          83       0.00      0.00      0.00        26
          84       0.60      0.06      0.10       105
          85       0.00      0.00      0.00        14
          86       0.28      0.27      0.27       242
          87       0.62      0.64      0.63       309
          88       0.00      0.00      0.00        59
          89       0.00      0.00      0.00        11
          90       0.07      0.01      0.02       187
          91       0.00      0.00      0.00        46
          92       0.00      0.00      0.00        40
          93       0.00      0.00      0.00        33
          94       0.17      0.20      0.19       289
          95       0.00      0.00      0.00        32
          96       0.00      0.00      0.00        75
          97       0.00      0.00      0.00        27
          98       0.00      0.00      0.00        37
          99       0.75      0.25      0.38        24
         100       0.00      0.00      0.00        25
         101       0.00      0.00      0.00        65
         102       0.00      0.00      0.00        22
         103       0.05      0.02      0.02        64
         104       0.00      0.00      0.00        40
         105       0.00      0.00      0.00        12
         106       0.60      0.51      0.55       113
         107       0.22      0.65      0.33       161
         108       0.00      0.00      0.00        24
         109       0.43      0.38      0.40        52
         110       1.00      0.13      0.24        15
         111       0.28      0.32      0.30       123
         112       0.22      0.05      0.08        41
         113       0.35      0.93      0.51       430
         114       0.38      0.09      0.15        65
         115       0.27      0.10      0.14        31
         116       0.51      0.17      0.25       173
         117       1.00      0.06      0.12        31
         118       0.36      0.68      0.47       117
         119       0.21      0.72      0.33       136
         120       0.35      0.45      0.40        62
         121       0.52      0.33      0.40       224
         122       0.65      0.31      0.42        35
         123       0.55      0.46      0.50        37
         124       0.14      0.03      0.05        31
         125       0.00      0.00      0.00        16
         126       0.41      0.33      0.37        21
         127       0.47      0.75      0.58        73

    accuracy                           0.34     12227
   macro avg       0.22      0.17      0.16     12227
weighted avg       0.31      0.34      0.29     12227


===confusion_matrix===

[[262   0   0 ...   0   0   0]
 [  1   1   0 ...   0   0   0]
 [  0   0   0 ...   0   0   0]
 ...
 [  0   0   0 ...   0   0  11]
 [  0   0   0 ...   0   7   4]
 [  0   0   0 ...   0   7  55]]

===multilabel confusion matrix===

[[[11091   778]
  [   96   262]]

 [[12214     1]
  [   11     1]]

 [[12208     0]
  [   19     0]]

 [[12145     3]
  [   79     0]]

 [[12169     3]
  [   54     1]]

 [[12167     2]
  [   58     0]]

 [[12180     2]
  [   43     2]]

 [[12178     1]
  [   48     0]]

 [[12217     0]
  [   10     0]]

 [[12206     0]
  [   21     0]]

 [[12212     0]
  [   15     0]]

 [[12157    34]
  [   15    21]]

 [[12215     0]
  [   12     0]]

 [[12202     0]
  [   25     0]]

 [[12207     0]
  [   20     0]]

 [[12200     5]
  [   15     7]]

 [[12204     0]
  [   23     0]]

 [[11969   140]
  [   68    50]]

 [[12209     0]
  [   18     0]]

 [[12214     0]
  [   13     0]]

 [[12137     0]
  [   90     0]]

 [[12215     0]
  [   12     0]]

 [[12202     0]
  [   25     0]]

 [[12215     0]
  [   12     0]]

 [[12205     0]
  [   22     0]]

 [[12188     2]
  [   34     3]]

 [[12210     0]
  [   17     0]]

 [[12192     0]
  [   35     0]]

 [[12215     0]
  [   12     0]]

 [[12189     2]
  [   36     0]]

 [[12127    68]
  [   21    11]]

 [[12182     6]
  [   31     8]]

 [[11029   451]
  [  480   267]]

 [[11925   228]
  [   20    54]]

 [[12166     3]
  [   41    17]]

 [[12166    13]
  [   39     9]]

 [[11548   177]
  [  454    48]]

 [[11936    50]
  [  125   116]]

 [[12194     0]
  [   33     0]]

 [[11503   380]
  [  162   182]]

 [[12001    35]
  [  142    49]]

 [[12195     0]
  [   32     0]]

 [[11550   293]
  [  248   136]]

 [[11814   296]
  [   51    66]]

 [[11561   230]
  [  210   226]]

 [[12178     0]
  [   49     0]]

 [[11518   307]
  [   52   350]]

 [[12210     0]
  [   17     0]]

 [[12182     3]
  [   39     3]]

 [[12097    53]
  [   46    31]]

 [[11990    65]
  [   33   139]]

 [[12207     0]
  [   20     0]]

 [[11486   242]
  [  345   154]]

 [[12067    60]
  [   65    35]]

 [[12216     0]
  [   11     0]]

 [[11982   142]
  [   69    34]]

 [[12209     0]
  [   18     0]]

 [[12217     0]
  [   10     0]]

 [[12188     4]
  [   17    18]]

 [[11747   249]
  [  156    75]]

 [[12168     1]
  [   31    27]]

 [[12198     0]
  [   29     0]]

 [[12179     0]
  [   46     2]]

 [[12177     0]
  [   50     0]]

 [[12193     0]
  [   34     0]]

 [[12072     0]
  [  155     0]]

 [[12213     0]
  [   14     0]]

 [[11891    22]
  [  309     5]]

 [[12164     0]
  [   63     0]]

 [[11209   711]
  [   56   251]]

 [[12159     0]
  [   68     0]]

 [[12152     9]
  [   64     2]]

 [[12213     0]
  [   14     0]]

 [[12202     0]
  [   25     0]]

 [[12209     0]
  [   18     0]]

 [[12168     0]
  [   59     0]]

 [[11892   129]
  [  143    63]]

 [[12150     0]
  [   77     0]]

 [[12160     8]
  [   49    10]]

 [[12026    62]
  [  105    34]]

 [[12185     0]
  [   42     0]]

 [[11943   109]
  [  170     5]]

 [[12182     2]
  [   42     1]]

 [[12201     0]
  [   26     0]]

 [[12118     4]
  [   99     6]]

 [[12213     0]
  [   14     0]]

 [[11812   173]
  [  176    66]]

 [[11799   119]
  [  111   198]]

 [[12168     0]
  [   59     0]]

 [[12216     0]
  [   11     0]]

 [[12015    25]
  [  185     2]]

 [[12176     5]
  [   46     0]]

 [[12187     0]
  [   40     0]]

 [[12194     0]
  [   33     0]]

 [[11663   275]
  [  231    58]]

 [[12195     0]
  [   32     0]]

 [[12152     0]
  [   75     0]]

 [[12200     0]
  [   27     0]]

 [[12190     0]
  [   37     0]]

 [[12201     2]
  [   18     6]]

 [[12202     0]
  [   25     0]]

 [[12161     1]
  [   65     0]]

 [[12202     3]
  [   22     0]]

 [[12145    18]
  [   63     1]]

 [[12187     0]
  [   40     0]]

 [[12215     0]
  [   12     0]]

 [[12075    39]
  [   55    58]]

 [[11689   377]
  [   56   105]]

 [[12203     0]
  [   24     0]]

 [[12148    27]
  [   32    20]]

 [[12212     0]
  [   13     2]]

 [[12004   100]
  [   84    39]]

 [[12179     7]
  [   39     2]]

 [[11040   757]
  [   28   402]]

 [[12152    10]
  [   59     6]]

 [[12188     8]
  [   28     3]]

 [[12026    28]
  [  144    29]]

 [[12196     0]
  [   29     2]]

 [[11969   141]
  [   37    80]]

 [[11732   359]
  [   38    98]]

 [[12114    51]
  [   34    28]]

 [[11935    68]
  [  150    74]]

 [[12186     6]
  [   24    11]]

 [[12176    14]
  [   20    17]]

 [[12190     6]
  [   30     1]]

 [[12210     1]
  [   16     0]]

 [[12196    10]
  [   14     7]]

 [[12093    61]
  [   18    55]]]

===scores report===
metrics	scores
Accuracy	0.3395
MCC	0.3252
log_loss	2.7493
f1 score weighted	0.2863
f1 score macro	0.1603
f1 score micro	0.3395
roc_auc ovr	0.9030
roc_auc ovo	0.9062
precision	0.3100
recall	0.3395
/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fb4fe8174c0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fb61ddecf40>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fb61ddecfd0>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f0702f75250>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f0562098400>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f05620985b0>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f05618ab730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f056192f280>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f056192f1c0>]/home/amsequeira/enzymeClassification/models/try_stuff
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f98b7bc03a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f98b7bdaee0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f98b7bdafa0>]
===TRAIN MODELS===

run_model
('Training Accuracy mean: ', 0.4912051856517792)
('Validation Accuracy mean: ', 0.46951454877853394)
('Training Loss mean: ', 1.406288492679596)
('Validation Loss mean: ', 1.4521068930625916)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking (Masking)            (None, 500, 21)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, 500, 128)          44032     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 500, 128)          98816     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 500, 64)           41216     
_________________________________________________________________
attention (attention)        (None, 64)                564       
_________________________________________________________________
dense (Dense)                (None, 32)                2080      
_________________________________________________________________
batch_normalization (BatchNo (None, 32)                128       
_________________________________________________________________
dropout (Dropout)            (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16)                64        
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 7)                 119       
=================================================================
Total params: 187,547
Trainable params: 187,451
Non-trainable params: 96
_________________________________________________________________Finished run_model in 1018.4789 secs


===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f98b7bdae20>, 'x_test': None, 'y_test': None, 'model': None}
report

              precision    recall  f1-score   support

           0       0.62      0.63      0.62      1792
           1       0.62      0.80      0.70      4921
           2       0.74      0.41      0.53      3576
           3       0.51      0.26      0.34       944
           4       0.43      0.29      0.35       695
           5       0.45      0.84      0.58      1073
           6       0.61      0.47      0.53       471

    accuracy                           0.60     13472
   macro avg       0.57      0.53      0.52     13472
weighted avg       0.62      0.60      0.59     13472


===confusion_matrix===

[[1124  357  125   38   17  127    4]
 [ 226 3934  196   70   65  412   18]
 [ 259 1251 1465   94  132  264  111]
 [  93  355   84  245   45  118    4]
 [  64  166   68   14  203  179    1]
 [  26   93   22   17   12  902    1]
 [  19  185   29    5    0   12  221]]

===multilabel confusion matrix===

[[[10993   687]
  [  668  1124]]

 [[ 6144  2407]
  [  987  3934]]

 [[ 9372   524]
  [ 2111  1465]]

 [[12290   238]
  [  699   245]]

 [[12506   271]
  [  492   203]]

 [[11287  1112]
  [  171   902]]

 [[12862   139]
  [  250   221]]]

===scores report===
metrics	scores
Accuracy	0.6008
MCC	0.4807
log_loss	1.1070
f1 score weighted	0.5851
f1 score macro	0.5223
f1 score micro	0.6008
roc_auc ovr	0.8633
roc_auc ovo	0.8743
precision	0.6195
recall	0.6008
