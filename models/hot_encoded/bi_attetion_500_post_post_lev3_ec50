/home/amsequeira/enzymeClassification/models/hot_encoded/bi_attetion_500_post_post_lev3_ec50
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f34c038f730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f34c038f5b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f34c038f790>]
===TRAIN MODELS===

run_model
('Training Accuracy mean: ', 0.6153089252368706)
('Validation Accuracy mean: ', 0.41487451461030217)
('Training Loss mean: ', 1.7662862408451918)
('Validation Loss mean: ', 3.0619486076075857)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking (Masking)            (None, 500, 21)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, 500, 128)          44032     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 500, 128)          98816     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 500, 64)           41216     
_________________________________________________________________
attention (attention)        (None, 64)                564       
_________________________________________________________________
dense (Dense)                (None, 32)                2080      
_________________________________________________________________
batch_normalization (BatchNo (None, 32)                128       
_________________________________________________________________
dropout (Dropout)            (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16)                64        
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 128)               2176      
=================================================================
Total params: 189,604
Trainable params: 189,508
Non-trainable params: 96
_________________________________________________________________Finished run_model in 6822.0635 secs


===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f34c038f910>, 'x_test': None, 'y_test': None, 'model': None}
report

              precision    recall  f1-score   support

           0       0.82      0.94      0.88       358
           1       1.00      0.50      0.67        12
           2       0.58      0.74      0.65        19
           3       0.79      0.86      0.82        79
           4       0.55      0.60      0.57        55
           5       0.69      0.60      0.64        58
           6       0.64      0.64      0.64        45
           7       0.80      0.75      0.77        48
           8       0.50      0.30      0.37        10
           9       0.94      0.71      0.81        21
          10       0.90      0.60      0.72        15
          11       0.83      0.83      0.83        36
          12       0.90      0.75      0.82        12
          13       0.88      0.92      0.90        25
          14       0.89      0.80      0.84        20
          15       0.84      0.95      0.89        22
          16       0.90      0.78      0.84        23
          17       0.99      0.84      0.91       118
          18       1.00      0.06      0.11        18
          19       1.00      0.23      0.38        13
          20       0.79      0.73      0.76        90
          21       1.00      0.58      0.74        12
          22       0.80      0.48      0.60        25
          23       1.00      0.50      0.67        12
          24       0.69      0.50      0.58        22
          25       0.85      0.78      0.82        37
          26       1.00      1.00      1.00        17
          27       0.95      0.54      0.69        35
          28       0.57      0.33      0.42        12
          29       0.87      0.92      0.89        36
          30       0.78      0.88      0.82        32
          31       0.95      0.92      0.94        39
          32       0.83      0.91      0.87       747
          33       0.94      0.99      0.96        74
          34       0.98      0.98      0.98        58
          35       0.94      0.98      0.96        48
          36       0.82      0.79      0.81       502
          37       0.84      0.85      0.85       241
          38       0.94      0.88      0.91        33
          39       0.72      0.89      0.80       344
          40       0.93      0.89      0.91       191
          41       0.91      0.66      0.76        32
          42       0.83      0.94      0.88       384
          43       0.84      0.91      0.87       117
          44       0.83      0.90      0.86       436
          45       0.88      0.57      0.69        49
          46       0.80      0.92      0.86       402
          47       0.86      0.35      0.50        17
          48       0.82      0.67      0.74        42
          49       1.00      0.71      0.83        77
          50       0.87      0.93      0.90       172
          51       1.00      0.70      0.82        20
          52       0.77      0.85      0.81       499
          53       0.86      0.88      0.87       100
          54       0.00      0.00      0.00        11
          55       0.98      0.97      0.98       103
          56       0.75      0.67      0.71        18
          57       1.00      0.30      0.46        10
          58       1.00      0.89      0.94        35
          59       0.80      0.74      0.77       231
          60       0.98      0.79      0.88        58
          61       0.64      0.24      0.35        29
          62       0.84      0.79      0.82        48
          63       0.70      0.66      0.68        50
          64       0.00      0.00      0.00        34
          65       0.98      0.52      0.68       155
          66       1.00      0.14      0.25        14
          67       0.75      0.72      0.73       314
          68       0.35      0.10      0.15        63
          69       0.61      0.92      0.73       307
          70       0.87      0.66      0.75        68
          71       0.81      0.85      0.83        66
          72       0.67      0.14      0.24        14
          73       0.82      0.72      0.77        25
          74       1.00      0.06      0.11        18
          75       0.77      0.61      0.68        59
          76       0.76      0.83      0.79       206
          77       0.85      0.66      0.74        77
          78       0.83      0.81      0.82        59
          79       0.80      0.83      0.81       139
          80       0.80      0.93      0.86        42
          81       0.68      0.81      0.74       175
          82       0.79      0.88      0.84        43
          83       0.91      0.77      0.83        26
          84       0.70      0.92      0.80       105
          85       0.87      0.93      0.90        14
          86       0.73      0.71      0.72       242
          87       0.85      0.84      0.85       309
          88       0.85      0.58      0.69        59
          89       0.83      0.45      0.59        11
          90       0.78      0.88      0.83       187
          91       0.67      0.63      0.65        46
          92       0.75      0.45      0.56        40
          93       0.88      0.85      0.86        33
          94       0.79      0.85      0.82       289
          95       0.50      0.06      0.11        32
          96       0.97      0.95      0.96        75
          97       0.94      0.56      0.70        27
          98       0.90      1.00      0.95        37
          99       0.96      1.00      0.98        24
         100       0.79      0.44      0.56        25
         101       0.73      0.66      0.69        65
         102       0.88      1.00      0.94        22
         103       0.88      0.95      0.92        64
         104       0.72      0.57      0.64        40
         105       1.00      0.83      0.91        12
         106       0.84      0.87      0.86       113
         107       0.91      0.94      0.92       161
         108       0.81      0.71      0.76        24
         109       0.96      0.88      0.92        52
         110       1.00      1.00      1.00        15
         111       0.80      0.92      0.85       123
         112       0.79      0.80      0.80        41
         113       0.92      1.00      0.96       430
         114       0.86      0.88      0.87        65
         115       0.90      0.90      0.90        31
         116       0.93      0.91      0.92       173
         117       0.96      0.84      0.90        31
         118       0.93      0.90      0.91       117
         119       0.94      0.99      0.97       136
         120       0.93      0.82      0.87        62
         121       0.91      0.40      0.56       224
         122       1.00      0.83      0.91        35
         123       0.85      0.62      0.72        37
         124       0.80      0.77      0.79        31
         125       1.00      0.81      0.90        16
         126       0.86      0.86      0.86        21
         127       0.73      0.89      0.80        73

    accuracy                           0.82     12227
   macro avg       0.83      0.72      0.75     12227
weighted avg       0.82      0.82      0.81     12227


===confusion_matrix===

[[336   0   0 ...   0   0   0]
 [  0   6   0 ...   0   0   0]
 [  0   0  14 ...   0   0   0]
 ...
 [  0   0   0 ...  13   0   2]
 [  0   0   0 ...   0  18   2]
 [  0   0   0 ...   0   1  65]]

===multilabel confusion matrix===

[[[11796    73]
  [   22   336]]

 [[12215     0]
  [    6     6]]

 [[12198    10]
  [    5    14]]

 [[12130    18]
  [   11    68]]

 [[12145    27]
  [   22    33]]

 [[12153    16]
  [   23    35]]

 [[12166    16]
  [   16    29]]

 [[12170     9]
  [   12    36]]

 [[12214     3]
  [    7     3]]

 [[12205     1]
  [    6    15]]

 [[12211     1]
  [    6     9]]

 [[12185     6]
  [    6    30]]

 [[12214     1]
  [    3     9]]

 [[12199     3]
  [    2    23]]

 [[12205     2]
  [    4    16]]

 [[12201     4]
  [    1    21]]

 [[12202     2]
  [    5    18]]

 [[12108     1]
  [   19    99]]

 [[12209     0]
  [   17     1]]

 [[12214     0]
  [   10     3]]

 [[12119    18]
  [   24    66]]

 [[12215     0]
  [    5     7]]

 [[12199     3]
  [   13    12]]

 [[12215     0]
  [    6     6]]

 [[12200     5]
  [   11    11]]

 [[12185     5]
  [    8    29]]

 [[12210     0]
  [    0    17]]

 [[12191     1]
  [   16    19]]

 [[12212     3]
  [    8     4]]

 [[12186     5]
  [    3    33]]

 [[12187     8]
  [    4    28]]

 [[12186     2]
  [    3    36]]

 [[11346   134]
  [   70   677]]

 [[12148     5]
  [    1    73]]

 [[12168     1]
  [    1    57]]

 [[12176     3]
  [    1    47]]

 [[11639    86]
  [  103   399]]

 [[11946    40]
  [   35   206]]

 [[12192     2]
  [    4    29]]

 [[11765   118]
  [   39   305]]

 [[12023    13]
  [   21   170]]

 [[12193     2]
  [   11    21]]

 [[11768    75]
  [   22   362]]

 [[12090    20]
  [   11   106]]

 [[11708    83]
  [   43   393]]

 [[12174     4]
  [   21    28]]

 [[11733    92]
  [   32   370]]

 [[12209     1]
  [   11     6]]

 [[12179     6]
  [   14    28]]

 [[12150     0]
  [   22    55]]

 [[12031    24]
  [   12   160]]

 [[12207     0]
  [    6    14]]

 [[11602   126]
  [   73   426]]

 [[12113    14]
  [   12    88]]

 [[12216     0]
  [   11     0]]

 [[12122     2]
  [    3   100]]

 [[12205     4]
  [    6    12]]

 [[12217     0]
  [    7     3]]

 [[12192     0]
  [    4    31]]

 [[11952    44]
  [   60   171]]

 [[12168     1]
  [   12    46]]

 [[12194     4]
  [   22     7]]

 [[12172     7]
  [   10    38]]

 [[12163    14]
  [   17    33]]

 [[12193     0]
  [   34     0]]

 [[12070     2]
  [   75    80]]

 [[12213     0]
  [   12     2]]

 [[11838    75]
  [   89   225]]

 [[12153    11]
  [   57     6]]

 [[11737   183]
  [   26   281]]

 [[12152     7]
  [   23    45]]

 [[12148    13]
  [   10    56]]

 [[12212     1]
  [   12     2]]

 [[12198     4]
  [    7    18]]

 [[12209     0]
  [   17     1]]

 [[12157    11]
  [   23    36]]

 [[11966    55]
  [   34   172]]

 [[12141     9]
  [   26    51]]

 [[12158    10]
  [   11    48]]

 [[12059    29]
  [   24   115]]

 [[12175    10]
  [    3    39]]

 [[11986    66]
  [   34   141]]

 [[12174    10]
  [    5    38]]

 [[12199     2]
  [    6    20]]

 [[12080    42]
  [    8    97]]

 [[12211     2]
  [    1    13]]

 [[11923    62]
  [   71   171]]

 [[11874    44]
  [   50   259]]

 [[12162     6]
  [   25    34]]

 [[12215     1]
  [    6     5]]

 [[11995    45]
  [   23   164]]

 [[12167    14]
  [   17    29]]

 [[12181     6]
  [   22    18]]

 [[12190     4]
  [    5    28]]

 [[11873    65]
  [   42   247]]

 [[12193     2]
  [   30     2]]

 [[12150     2]
  [    4    71]]

 [[12199     1]
  [   12    15]]

 [[12186     4]
  [    0    37]]

 [[12202     1]
  [    0    24]]

 [[12199     3]
  [   14    11]]

 [[12146    16]
  [   22    43]]

 [[12202     3]
  [    0    22]]

 [[12155     8]
  [    3    61]]

 [[12178     9]
  [   17    23]]

 [[12215     0]
  [    2    10]]

 [[12096    18]
  [   15    98]]

 [[12051    15]
  [   10   151]]

 [[12199     4]
  [    7    17]]

 [[12173     2]
  [    6    46]]

 [[12212     0]
  [    0    15]]

 [[12075    29]
  [   10   113]]

 [[12177     9]
  [    8    33]]

 [[11761    36]
  [    1   429]]

 [[12153     9]
  [    8    57]]

 [[12193     3]
  [    3    28]]

 [[12042    12]
  [   15   158]]

 [[12195     1]
  [    5    26]]

 [[12102     8]
  [   12   105]]

 [[12083     8]
  [    1   135]]

 [[12161     4]
  [   11    51]]

 [[11994     9]
  [  134    90]]

 [[12192     0]
  [    6    29]]

 [[12186     4]
  [   14    23]]

 [[12190     6]
  [    7    24]]

 [[12211     0]
  [    3    13]]

 [[12203     3]
  [    3    18]]

 [[12130    24]
  [    8    65]]]

===scores report===
metrics	scores
Accuracy	0.8193
MCC	0.8154
log_loss	0.8683
f1 score weighted	0.8101
f1 score macro	0.7491
f1 score micro	0.8193
roc_auc ovr	0.9890
roc_auc ovo	0.9874
precision	0.8219
recall	0.8193

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f34c038f730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f34c038f5b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f34c038f790>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f34c038f910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([42., 42., 42., ..., 78., 76., 88.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.66      0.79      0.72       358
         1.0       0.41      0.58      0.48        12
         2.0       0.73      0.42      0.53        19
         3.0       0.44      0.69      0.54        80
         4.0       0.27      0.24      0.25        54
         5.0       0.32      0.14      0.19        58
         6.0       0.58      0.33      0.42        45
         7.0       0.68      0.58      0.63        48
         8.0       0.00      0.00      0.00        11
         9.0       0.53      0.38      0.44        21
        10.0       0.67      0.27      0.38        15
        11.0       0.81      0.58      0.68        36
        12.0       0.70      0.58      0.64        12
        13.0       0.85      0.68      0.76        25
        14.0       0.50      0.05      0.10        19
        15.0       0.62      0.59      0.60        22
        16.0       0.88      0.61      0.72        23
        17.0       0.65      0.89      0.75       119
        18.0       0.57      0.44      0.50        18
        19.0       0.38      0.25      0.30        12
        20.0       0.79      0.34      0.48        90
        21.0       0.80      0.33      0.47        12
        22.0       0.76      0.88      0.81        25
        23.0       0.00      0.00      0.00        12
        24.0       0.29      0.09      0.14        22
        25.0       0.60      0.47      0.53        38
        26.0       0.59      1.00      0.74        17
        27.0       1.00      0.11      0.21        35
        28.0       0.00      0.00      0.00        11
        29.0       0.60      0.50      0.55        36
        30.0       0.69      0.75      0.72        32
        31.0       0.76      0.82      0.78        38
        32.0       0.91      0.75      0.82       747
        33.0       0.83      0.81      0.82        74
        34.0       0.90      0.88      0.89        59
        35.0       0.82      0.85      0.84        48
        36.0       0.67      0.71      0.69       502
        37.0       0.64      0.76      0.69       241
        38.0       0.56      0.58      0.57        33
        39.0       0.66      0.69      0.68       344
        40.0       0.82      0.69      0.75       191
        41.0       0.89      0.50      0.64        32
        42.0       0.83      0.75      0.79       384
        43.0       0.76      0.69      0.72       118
        44.0       0.82      0.71      0.76       436
        45.0       0.47      0.38      0.42        48
        46.0       0.64      0.87      0.74       402
        47.0       0.00      0.00      0.00        17
        48.0       0.33      0.60      0.42        42
        49.0       0.97      0.87      0.92        78
        50.0       0.94      0.85      0.89       172
        51.0       1.00      0.40      0.57        20
        52.0       0.58      0.79      0.67       499
        53.0       0.87      0.75      0.81       100
        54.0       0.40      0.36      0.38        11
        55.0       0.86      0.82      0.84       103
        56.0       0.83      0.56      0.67        18
        57.0       0.50      0.10      0.17        10
        58.0       0.87      0.97      0.92        34
        59.0       0.71      0.58      0.64       231
        60.0       0.91      0.74      0.82        58
        61.0       1.00      0.03      0.06        30
        62.0       0.78      0.44      0.56        48
        63.0       0.88      0.14      0.24        50
        64.0       0.76      0.65      0.70        34
        65.0       0.88      0.74      0.80       155
        66.0       0.60      0.21      0.32        14
        67.0       0.57      0.64      0.60       314
        68.0       0.25      0.08      0.12        63
        69.0       0.57      0.60      0.59       308
        70.0       0.68      0.34      0.45        68
        71.0       0.26      0.68      0.37        66
        72.0       0.00      0.00      0.00        14
        73.0       0.78      0.72      0.75        25
        74.0       0.00      0.00      0.00        18
        75.0       0.39      0.42      0.40        60
        76.0       0.64      0.66      0.65       205
        77.0       0.74      0.34      0.46        77
        78.0       0.90      0.76      0.83        59
        79.0       0.32      0.51      0.39       139
        80.0       0.77      0.81      0.79        42
        81.0       0.38      0.47      0.42       175
        82.0       0.51      0.49      0.50        43
        83.0       0.56      0.35      0.43        26
        84.0       0.33      0.68      0.45       106
        85.0       0.67      0.43      0.52        14
        86.0       0.71      0.74      0.73       242
        87.0       0.82      0.81      0.81       309
        88.0       0.92      0.83      0.87        58
        89.0       1.00      0.09      0.17        11
        90.0       0.42      0.68      0.52       187
        91.0       0.65      0.48      0.55        46
        92.0       0.43      0.07      0.13        40
        93.0       0.79      0.34      0.48        32
        94.0       0.68      0.67      0.67       289
        95.0       0.33      0.03      0.06        31
        96.0       0.78      0.78      0.78        74
        97.0       0.57      0.44      0.50        27
        98.0       0.95      0.49      0.64        37
        99.0       0.88      0.88      0.88        24
       100.0       0.00      0.00      0.00        25
       101.0       0.48      0.46      0.47        65
       102.0       0.80      0.73      0.76        22
       103.0       0.67      0.78      0.72        64
       104.0       0.65      0.28      0.39        40
       105.0       1.00      0.92      0.96        12
       106.0       0.81      0.77      0.79       114
       107.0       0.89      0.74      0.81       161
       108.0       1.00      0.17      0.29        24
       109.0       0.88      0.67      0.76        52
       110.0       0.81      0.87      0.84        15
       111.0       0.82      0.64      0.72       123
       112.0       0.80      0.57      0.67        42
       113.0       0.76      0.95      0.85       430
       114.0       0.59      0.75      0.66        65
       115.0       0.94      0.48      0.64        31
       116.0       0.85      0.79      0.82       173
       117.0       0.96      0.87      0.92        31
       118.0       0.89      0.88      0.88       117
       119.0       0.83      0.89      0.86       136
       120.0       0.81      0.68      0.74        62
       121.0       0.84      0.88      0.86       224
       122.0       0.72      0.80      0.76        35
       123.0       0.51      0.81      0.62        37
       124.0       0.68      0.42      0.52        31
       125.0       0.73      0.73      0.73        15
       126.0       0.77      0.95      0.85        21
       127.0       0.75      0.74      0.74        73

    accuracy                           0.68     12227
   macro avg       0.66      0.56      0.57     12227
weighted avg       0.70      0.68      0.68     12227


===confusion_matrix===

[[283   0   0 ...   0   0   0]
 [  0   7   0 ...   0   0   0]
 [  0   0   8 ...   0   0   0]
 ...
 [  0   0   0 ...  11   1   1]
 [  0   0   0 ...   0  20   0]
 [  0   0   0 ...   0   2  54]]

===multilabel confusion matrix===

[[[11724   145]
  [   75   283]]

 [[12205    10]
  [    5     7]]

 [[12205     3]
  [   11     8]]

 [[12077    70]
  [   25    55]]

 [[12137    36]
  [   41    13]]

 [[12152    17]
  [   50     8]]

 [[12171    11]
  [   30    15]]

 [[12166    13]
  [   20    28]]

 [[12215     1]
  [   11     0]]

 [[12199     7]
  [   13     8]]

 [[12210     2]
  [   11     4]]

 [[12186     5]
  [   15    21]]

 [[12212     3]
  [    5     7]]

 [[12199     3]
  [    8    17]]

 [[12207     1]
  [   18     1]]

 [[12197     8]
  [    9    13]]

 [[12202     2]
  [    9    14]]

 [[12051    57]
  [   13   106]]

 [[12203     6]
  [   10     8]]

 [[12210     5]
  [    9     3]]

 [[12129     8]
  [   59    31]]

 [[12214     1]
  [    8     4]]

 [[12195     7]
  [    3    22]]

 [[12215     0]
  [   12     0]]

 [[12200     5]
  [   20     2]]

 [[12177    12]
  [   20    18]]

 [[12198    12]
  [    0    17]]

 [[12192     0]
  [   31     4]]

 [[12215     1]
  [   11     0]]

 [[12179    12]
  [   18    18]]

 [[12184    11]
  [    8    24]]

 [[12179    10]
  [    7    31]]

 [[11422    58]
  [  187   560]]

 [[12141    12]
  [   14    60]]

 [[12162     6]
  [    7    52]]

 [[12170     9]
  [    7    41]]

 [[11548   177]
  [  148   354]]

 [[11881   105]
  [   58   183]]

 [[12179    15]
  [   14    19]]

 [[11763   120]
  [  107   237]]

 [[12008    28]
  [   60   131]]

 [[12193     2]
  [   16    16]]

 [[11786    57]
  [   97   287]]

 [[12084    25]
  [   37    81]]

 [[11723    68]
  [  128   308]]

 [[12159    20]
  [   30    18]]

 [[11632   193]
  [   54   348]]

 [[12210     0]
  [   17     0]]

 [[12134    51]
  [   17    25]]

 [[12147     2]
  [   10    68]]

 [[12045    10]
  [   25   147]]

 [[12207     0]
  [   12     8]]

 [[11445   283]
  [  104   395]]

 [[12116    11]
  [   25    75]]

 [[12210     6]
  [    7     4]]

 [[12110    14]
  [   19    84]]

 [[12207     2]
  [    8    10]]

 [[12216     1]
  [    9     1]]

 [[12188     5]
  [    1    33]]

 [[11940    56]
  [   96   135]]

 [[12165     4]
  [   15    43]]

 [[12197     0]
  [   29     1]]

 [[12173     6]
  [   27    21]]

 [[12176     1]
  [   43     7]]

 [[12186     7]
  [   12    22]]

 [[12057    15]
  [   41   114]]

 [[12211     2]
  [   11     3]]

 [[11759   154]
  [  113   201]]

 [[12149    15]
  [   58     5]]

 [[11778   141]
  [  122   186]]

 [[12148    11]
  [   45    23]]

 [[12030   131]
  [   21    45]]

 [[12208     5]
  [   14     0]]

 [[12197     5]
  [    7    18]]

 [[12207     2]
  [   18     0]]

 [[12128    39]
  [   35    25]]

 [[11946    76]
  [   69   136]]

 [[12141     9]
  [   51    26]]

 [[12163     5]
  [   14    45]]

 [[11936   152]
  [   68    71]]

 [[12175    10]
  [    8    34]]

 [[11919   133]
  [   93    82]]

 [[12164    20]
  [   22    21]]

 [[12194     7]
  [   17     9]]

 [[11976   145]
  [   34    72]]

 [[12210     3]
  [    8     6]]

 [[11911    74]
  [   62   180]]

 [[11862    56]
  [   60   249]]

 [[12165     4]
  [   10    48]]

 [[12216     0]
  [   10     1]]

 [[11861   179]
  [   60   127]]

 [[12169    12]
  [   24    22]]

 [[12183     4]
  [   37     3]]

 [[12192     3]
  [   21    11]]

 [[11845    93]
  [   95   194]]

 [[12194     2]
  [   30     1]]

 [[12137    16]
  [   16    58]]

 [[12191     9]
  [   15    12]]

 [[12189     1]
  [   19    18]]

 [[12200     3]
  [    3    21]]

 [[12199     3]
  [   25     0]]

 [[12130    32]
  [   35    30]]

 [[12201     4]
  [    6    16]]

 [[12138    25]
  [   14    50]]

 [[12181     6]
  [   29    11]]

 [[12215     0]
  [    1    11]]

 [[12093    20]
  [   26    88]]

 [[12051    15]
  [   42   119]]

 [[12203     0]
  [   20     4]]

 [[12170     5]
  [   17    35]]

 [[12209     3]
  [    2    13]]

 [[12087    17]
  [   44    79]]

 [[12179     6]
  [   18    24]]

 [[11669   128]
  [   20   410]]

 [[12128    34]
  [   16    49]]

 [[12195     1]
  [   16    15]]

 [[12030    24]
  [   37   136]]

 [[12195     1]
  [    4    27]]

 [[12097    13]
  [   14   103]]

 [[12066    25]
  [   15   121]]

 [[12155    10]
  [   20    42]]

 [[11964    39]
  [   26   198]]

 [[12181    11]
  [    7    28]]

 [[12161    29]
  [    7    30]]

 [[12190     6]
  [   18    13]]

 [[12208     4]
  [    4    11]]

 [[12200     6]
  [    1    20]]

 [[12136    18]
  [   19    54]]]

===scores report===
metrics	scores
Accuracy	0.6844
MCC	0.6777
log_loss	1.4446
f1 score weighted	0.6782
f1 score macro	0.5733
f1 score micro	0.6844
roc_auc ovr	0.9751
roc_auc ovo	0.9721
precision	0.7036
recall	0.6844

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f34c038f730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f34c038f5b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f34c038f790>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f34c038f910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([42., 21., 21., ..., 36., 37., 32.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.59      0.87      0.71       357
         1.0       0.53      0.67      0.59        12
         2.0       0.44      0.21      0.29        19
         3.0       0.57      0.62      0.60        80
         4.0       0.58      0.20      0.30        54
         5.0       0.24      0.14      0.18        58
         6.0       0.36      0.27      0.31        44
         7.0       0.51      0.56      0.53        48
         8.0       0.00      0.00      0.00        11
         9.0       0.85      0.52      0.65        21
        10.0       1.00      0.07      0.12        15
        11.0       0.96      0.69      0.81        36
        12.0       0.56      0.42      0.48        12
        13.0       0.74      0.56      0.64        25
        14.0       0.75      0.30      0.43        20
        15.0       0.88      0.61      0.72        23
        16.0       0.92      0.52      0.67        23
        17.0       0.78      0.75      0.76       119
        18.0       1.00      0.18      0.30        17
        19.0       0.00      0.00      0.00        13
        20.0       0.69      0.39      0.50        90
        21.0       1.00      0.33      0.50        12
        22.0       0.76      0.64      0.70        25
        23.0       1.00      0.08      0.15        12
        24.0       0.67      0.09      0.16        22
        25.0       0.56      0.51      0.54        37
        26.0       0.95      1.00      0.97        18
        27.0       0.75      0.09      0.15        35
        28.0       0.00      0.00      0.00        12
        29.0       0.94      0.46      0.62        37
        30.0       0.78      0.66      0.71        32
        31.0       0.93      0.72      0.81        39
        32.0       0.62      0.87      0.72       746
        33.0       0.94      0.88      0.91        74
        34.0       0.69      0.88      0.77        58
        35.0       0.73      0.69      0.71        48
        36.0       0.62      0.72      0.67       502
        37.0       0.66      0.66      0.66       241
        38.0       0.63      0.36      0.46        33
        39.0       0.46      0.75      0.57       344
        40.0       0.71      0.65      0.68       191
        41.0       1.00      0.48      0.65        31
        42.0       0.71      0.74      0.73       384
        43.0       0.80      0.83      0.82       118
        44.0       0.71      0.76      0.73       436
        45.0       0.47      0.44      0.45        48
        46.0       0.67      0.80      0.73       402
        47.0       1.00      0.24      0.38        17
        48.0       0.65      0.52      0.58        42
        49.0       0.86      0.91      0.89        77
        50.0       0.87      0.84      0.85       172
        51.0       0.92      0.60      0.73        20
        52.0       0.83      0.60      0.70       499
        53.0       0.92      0.57      0.70        99
        54.0       0.00      0.00      0.00        11
        55.0       0.82      0.70      0.75       103
        56.0       0.86      0.33      0.48        18
        57.0       0.00      0.00      0.00        11
        58.0       1.00      0.88      0.94        34
        59.0       0.36      0.77      0.49       231
        60.0       0.85      0.69      0.76        58
        61.0       0.14      0.03      0.05        30
        62.0       0.70      0.29      0.41        48
        63.0       0.42      0.27      0.33        49
        64.0       0.92      0.65      0.76        34
        65.0       0.84      0.73      0.78       154
        66.0       0.67      0.14      0.24        14
        67.0       0.42      0.67      0.51       314
        68.0       0.14      0.02      0.03        63
        69.0       0.49      0.69      0.57       308
        70.0       0.70      0.38      0.49        69
        71.0       0.62      0.53      0.57        66
        72.0       0.00      0.00      0.00        14
        73.0       0.43      0.76      0.55        25
        74.0       0.00      0.00      0.00        18
        75.0       0.12      0.08      0.10        59
        76.0       0.66      0.67      0.66       205
        77.0       0.34      0.29      0.31        77
        78.0       0.71      0.51      0.59        59
        79.0       0.73      0.59      0.65       139
        80.0       0.84      0.78      0.81        41
        81.0       0.66      0.35      0.46       175
        82.0       0.49      0.51      0.50        43
        83.0       0.86      0.23      0.36        26
        84.0       0.71      0.33      0.45       105
        85.0       0.55      0.43      0.48        14
        86.0       0.81      0.68      0.74       242
        87.0       0.81      0.76      0.78       309
        88.0       0.86      0.84      0.85        58
        89.0       0.50      0.09      0.15        11
        90.0       0.59      0.52      0.55       187
        91.0       0.37      0.30      0.33        46
        92.0       1.00      0.03      0.05        40
        93.0       0.92      0.33      0.49        33
        94.0       0.46      0.67      0.54       289
        95.0       0.21      0.16      0.18        32
        96.0       0.82      0.62      0.71        74
        97.0       0.48      0.52      0.50        27
        98.0       0.86      0.49      0.62        37
        99.0       1.00      0.88      0.93        24
       100.0       0.33      0.08      0.12        26
       101.0       0.50      0.34      0.40        65
       102.0       0.89      0.36      0.52        22
       103.0       0.91      0.67      0.77        64
       104.0       0.40      0.42      0.41        40
       105.0       1.00      0.69      0.82        13
       106.0       0.73      0.78      0.76       113
       107.0       0.82      0.74      0.78       162
       108.0       0.88      0.62      0.73        24
       109.0       0.82      0.87      0.84        52
       110.0       1.00      0.73      0.85        15
       111.0       0.81      0.64      0.71       123
       112.0       0.70      0.56      0.62        41
       113.0       0.95      0.89      0.92       430
       114.0       0.84      0.72      0.78        65
       115.0       0.83      0.48      0.61        31
       116.0       0.98      0.69      0.81       173
       117.0       0.77      0.80      0.79        30
       118.0       0.93      0.81      0.87       118
       119.0       0.91      0.85      0.88       136
       120.0       0.97      0.59      0.73        61
       121.0       0.93      0.86      0.90       225
       122.0       1.00      0.94      0.97        35
       123.0       0.79      0.58      0.67        38
       124.0       0.83      0.65      0.73        31
       125.0       1.00      0.81      0.90        16
       126.0       0.64      0.86      0.73        21
       127.0       0.67      0.68      0.68        73

    accuracy                           0.67     12227
   macro avg       0.68      0.52      0.56     12227
weighted avg       0.69      0.67      0.66     12227


===confusion_matrix===

[[312   0   0 ...   0   0   0]
 [  0   8   0 ...   0   0   0]
 [  1   0   4 ...   0   0   0]
 ...
 [  0   0   0 ...  13   2   0]
 [  0   0   0 ...   0  18   2]
 [  0   0   1 ...   0   7  50]]

===multilabel confusion matrix===

[[[11655   215]
  [   45   312]]

 [[12208     7]
  [    4     8]]

 [[12203     5]
  [   15     4]]

 [[12110    37]
  [   30    50]]

 [[12165     8]
  [   43    11]]

 [[12144    25]
  [   50     8]]

 [[12162    21]
  [   32    12]]

 [[12153    26]
  [   21    27]]

 [[12216     0]
  [   11     0]]

 [[12204     2]
  [   10    11]]

 [[12212     0]
  [   14     1]]

 [[12190     1]
  [   11    25]]

 [[12211     4]
  [    7     5]]

 [[12197     5]
  [   11    14]]

 [[12205     2]
  [   14     6]]

 [[12202     2]
  [    9    14]]

 [[12203     1]
  [   11    12]]

 [[12083    25]
  [   30    89]]

 [[12210     0]
  [   14     3]]

 [[12214     0]
  [   13     0]]

 [[12121    16]
  [   55    35]]

 [[12215     0]
  [    8     4]]

 [[12197     5]
  [    9    16]]

 [[12215     0]
  [   11     1]]

 [[12204     1]
  [   20     2]]

 [[12175    15]
  [   18    19]]

 [[12208     1]
  [    0    18]]

 [[12191     1]
  [   32     3]]

 [[12215     0]
  [   12     0]]

 [[12189     1]
  [   20    17]]

 [[12189     6]
  [   11    21]]

 [[12186     2]
  [   11    28]]

 [[11084   397]
  [   97   649]]

 [[12149     4]
  [    9    65]]

 [[12146    23]
  [    7    51]]

 [[12167    12]
  [   15    33]]

 [[11505   220]
  [  141   361]]

 [[11903    83]
  [   83   158]]

 [[12187     7]
  [   21    12]]

 [[11584   299]
  [   85   259]]

 [[11985    51]
  [   66   125]]

 [[12196     0]
  [   16    15]]

 [[11728   115]
  [   99   285]]

 [[12085    24]
  [   20    98]]

 [[11655   136]
  [  106   330]]

 [[12155    24]
  [   27    21]]

 [[11666   159]
  [   79   323]]

 [[12210     0]
  [   13     4]]

 [[12173    12]
  [   20    22]]

 [[12139    11]
  [    7    70]]

 [[12034    21]
  [   28   144]]

 [[12206     1]
  [    8    12]]

 [[11669    59]
  [  201   298]]

 [[12123     5]
  [   43    56]]

 [[12216     0]
  [   11     0]]

 [[12108    16]
  [   31    72]]

 [[12208     1]
  [   12     6]]

 [[12215     1]
  [   11     0]]

 [[12193     0]
  [    4    30]]

 [[11680   316]
  [   52   179]]

 [[12162     7]
  [   18    40]]

 [[12191     6]
  [   29     1]]

 [[12173     6]
  [   34    14]]

 [[12160    18]
  [   36    13]]

 [[12191     2]
  [   12    22]]

 [[12051    22]
  [   42   112]]

 [[12212     1]
  [   12     2]]

 [[11624   289]
  [  105   209]]

 [[12158     6]
  [   62     1]]

 [[11698   221]
  [   97   211]]

 [[12147    11]
  [   43    26]]

 [[12140    21]
  [   31    35]]

 [[12211     2]
  [   14     0]]

 [[12177    25]
  [    6    19]]

 [[12209     0]
  [   18     0]]

 [[12131    37]
  [   54     5]]

 [[11951    71]
  [   68   137]]

 [[12107    43]
  [   55    22]]

 [[12156    12]
  [   29    30]]

 [[12058    30]
  [   57    82]]

 [[12180     6]
  [    9    32]]

 [[12020    32]
  [  114    61]]

 [[12161    23]
  [   21    22]]

 [[12200     1]
  [   20     6]]

 [[12108    14]
  [   70    35]]

 [[12208     5]
  [    8     6]]

 [[11946    39]
  [   78   164]]

 [[11863    55]
  [   75   234]]

 [[12161     8]
  [    9    49]]

 [[12215     1]
  [   10     1]]

 [[11971    69]
  [   89    98]]

 [[12157    24]
  [   32    14]]

 [[12187     0]
  [   39     1]]

 [[12193     1]
  [   22    11]]

 [[11706   232]
  [   95   194]]

 [[12176    19]
  [   27     5]]

 [[12143    10]
  [   28    46]]

 [[12185    15]
  [   13    14]]

 [[12187     3]
  [   19    18]]

 [[12203     0]
  [    3    21]]

 [[12197     4]
  [   24     2]]

 [[12140    22]
  [   43    22]]

 [[12204     1]
  [   14     8]]

 [[12159     4]
  [   21    43]]

 [[12162    25]
  [   23    17]]

 [[12214     0]
  [    4     9]]

 [[12082    32]
  [   25    88]]

 [[12038    27]
  [   42   120]]

 [[12201     2]
  [    9    15]]

 [[12165    10]
  [    7    45]]

 [[12212     0]
  [    4    11]]

 [[12085    19]
  [   44    79]]

 [[12176    10]
  [   18    23]]

 [[11778    19]
  [   47   383]]

 [[12153     9]
  [   18    47]]

 [[12193     3]
  [   16    15]]

 [[12052     2]
  [   53   120]]

 [[12190     7]
  [    6    24]]

 [[12102     7]
  [   22    96]]

 [[12080    11]
  [   21   115]]

 [[12165     1]
  [   25    36]]

 [[11988    14]
  [   31   194]]

 [[12192     0]
  [    2    33]]

 [[12183     6]
  [   16    22]]

 [[12192     4]
  [   11    20]]

 [[12211     0]
  [    3    13]]

 [[12196    10]
  [    3    18]]

 [[12129    25]
  [   23    50]]]

===scores report===
metrics	scores
Accuracy	0.6653
MCC	0.6580
log_loss	1.5199
f1 score weighted	0.6574
f1 score macro	0.5610
f1 score micro	0.6653
roc_auc ovr	0.9716
roc_auc ovo	0.9674
precision	0.6904
recall	0.6653

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f34c038f730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f34c038f5b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f34c038f790>]/home/amsequeira/enzymeClassification/models/hot_encoded/bi_attetion_500_post_post_lev3_ec50
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7efed4616c40>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7efd7cfe2b50>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7efd7cfe2c40>]/home/amsequeira/enzymeClassification/models/hot_encoded/bi_attetion_500_post_post_lev3_ec50
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7efc9fa2c3a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7efbc70288e0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7efbc7028790>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f34c038f910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([42., 42., 42., ..., 88., 87., 37.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.84      0.67      0.74       357
         1.0       0.83      0.38      0.53        13
         2.0       0.88      0.37      0.52        19
         3.0       0.82      0.42      0.55        79
         4.0       0.44      0.20      0.28        55
         5.0       0.45      0.15      0.23        59
         6.0       0.55      0.52      0.53        44
         7.0       0.70      0.58      0.64        48
         8.0       0.00      0.00      0.00        10
         9.0       0.57      0.38      0.46        21
        10.0       1.00      0.07      0.12        15
        11.0       0.76      0.81      0.78        36
        12.0       0.67      0.50      0.57        12
        13.0       0.90      0.76      0.83        25
        14.0       0.28      0.35      0.31        20
        15.0       0.93      0.59      0.72        22
        16.0       0.88      0.61      0.72        23
        17.0       0.87      0.81      0.84       118
        18.0       1.00      0.22      0.36        18
        19.0       0.08      0.08      0.08        13
        20.0       0.58      0.44      0.50        89
        21.0       0.60      0.25      0.35        12
        22.0       0.50      0.83      0.62        24
        23.0       0.50      0.08      0.14        12
        24.0       0.67      0.26      0.38        23
        25.0       0.65      0.59      0.62        37
        26.0       0.93      0.82      0.87        17
        27.0       0.62      0.14      0.23        36
        28.0       0.00      0.00      0.00        12
        29.0       0.74      0.54      0.62        37
        30.0       0.57      0.53      0.55        32
        31.0       0.64      0.82      0.72        39
        32.0       0.67      0.88      0.76       746
        33.0       0.97      0.85      0.91        74
        34.0       0.70      0.78      0.74        58
        35.0       0.85      0.58      0.69        48
        36.0       0.78      0.60      0.68       502
        37.0       0.61      0.70      0.65       240
        38.0       0.66      0.64      0.65        33
        39.0       0.74      0.65      0.69       344
        40.0       0.83      0.74      0.78       191
        41.0       0.92      0.39      0.55        31
        42.0       0.50      0.83      0.63       384
        43.0       0.69      0.76      0.72       117
        44.0       0.82      0.71      0.76       436
        45.0       0.59      0.33      0.42        49
        46.0       0.88      0.74      0.80       402
        47.0       0.00      0.00      0.00        17
        48.0       0.96      0.52      0.68        42
        49.0       1.00      0.79      0.88        77
        50.0       0.96      0.90      0.93       172
        51.0       1.00      0.42      0.59        19
        52.0       0.80      0.60      0.69       499
        53.0       0.93      0.64      0.75        99
        54.0       0.00      0.00      0.00        11
        55.0       0.38      0.88      0.53       103
        56.0       0.45      0.50      0.47        18
        57.0       1.00      0.36      0.53        11
        58.0       0.87      0.94      0.90        35
        59.0       0.67      0.60      0.64       231
        60.0       0.87      0.81      0.84        57
        61.0       0.08      0.03      0.05        29
        62.0       0.40      0.44      0.42        48
        63.0       0.30      0.27      0.28        49
        64.0       0.67      0.59      0.62        34
        65.0       0.52      0.81      0.63       155
        66.0       0.62      0.57      0.59        14
        67.0       0.56      0.60      0.58       315
        68.0       0.15      0.03      0.05        63
        69.0       0.44      0.81      0.57       307
        70.0       0.58      0.36      0.45        69
        71.0       0.78      0.32      0.45        66
        72.0       0.00      0.00      0.00        15
        73.0       0.68      0.52      0.59        25
        74.0       0.00      0.00      0.00        18
        75.0       0.23      0.22      0.22        59
        76.0       0.92      0.47      0.62       206
        77.0       0.24      0.41      0.30        76
        78.0       0.85      0.58      0.69        59
        79.0       0.57      0.45      0.50       140
        80.0       0.94      0.81      0.87        42
        81.0       0.65      0.39      0.49       175
        82.0       0.64      0.37      0.47        43
        83.0       0.42      0.20      0.27        25
        84.0       0.31      0.63      0.41       105
        85.0       0.86      0.43      0.57        14
        86.0       0.79      0.70      0.74       242
        87.0       0.91      0.77      0.84       310
        88.0       0.88      0.73      0.80        59
        89.0       0.00      0.00      0.00        11
        90.0       0.59      0.57      0.58       187
        91.0       0.55      0.39      0.46        46
        92.0       0.52      0.30      0.38        40
        93.0       0.27      0.64      0.38        33
        94.0       0.33      0.75      0.46       289
        95.0       0.00      0.00      0.00        32
        96.0       0.84      0.72      0.78        75
        97.0       0.67      0.07      0.13        28
        98.0       0.80      0.76      0.78        37
        99.0       0.95      0.83      0.88        23
       100.0       0.17      0.04      0.06        25
       101.0       0.75      0.36      0.49        66
       102.0       0.94      0.81      0.87        21
       103.0       0.83      0.80      0.81        65
       104.0       0.44      0.28      0.34        40
       105.0       1.00      0.75      0.86        12
       106.0       0.95      0.68      0.79       113
       107.0       0.81      0.79      0.80       162
       108.0       0.77      0.42      0.54        24
       109.0       0.76      0.85      0.80        53
       110.0       0.64      0.64      0.64        14
       111.0       0.65      0.74      0.69       123
       112.0       0.90      0.46      0.61        41
       113.0       0.88      0.94      0.91       429
       114.0       0.86      0.66      0.75        65
       115.0       0.64      0.74      0.69        31
       116.0       0.98      0.75      0.85       173
       117.0       0.78      0.83      0.81        30
       118.0       0.92      0.80      0.86       117
       119.0       0.91      0.85      0.88       136
       120.0       0.37      0.79      0.51        61
       121.0       0.70      0.95      0.80       225
       122.0       0.93      0.77      0.84        35
       123.0       0.61      0.45      0.52        38
       124.0       0.76      0.87      0.81        30
       125.0       0.87      0.81      0.84        16
       126.0       0.82      0.41      0.55        22
       127.0       0.55      0.85      0.67        73

    accuracy                           0.67     12226
   macro avg       0.65      0.54      0.56     12226
weighted avg       0.70      0.67      0.66     12226


===confusion_matrix===

[[238   0   0 ...   0   0   1]
 [  0   5   0 ...   0   0   0]
 [  0   0   7 ...   0   0   0]
 ...
 [  0   0   0 ...  13   1   2]
 [  0   0   0 ...   0   9  11]
 [  0   0   0 ...   1   1  62]]

===multilabel confusion matrix===

[[[11823    46]
  [  119   238]]

 [[12212     1]
  [    8     5]]

 [[12206     1]
  [   12     7]]

 [[12140     7]
  [   46    33]]

 [[12157    14]
  [   44    11]]

 [[12156    11]
  [   50     9]]

 [[12163    19]
  [   21    23]]

 [[12166    12]
  [   20    28]]

 [[12210     6]
  [   10     0]]

 [[12199     6]
  [   13     8]]

 [[12211     0]
  [   14     1]]

 [[12181     9]
  [    7    29]]

 [[12211     3]
  [    6     6]]

 [[12199     2]
  [    6    19]]

 [[12188    18]
  [   13     7]]

 [[12203     1]
  [    9    13]]

 [[12201     2]
  [    9    14]]

 [[12094    14]
  [   23    95]]

 [[12208     0]
  [   14     4]]

 [[12201    12]
  [   12     1]]

 [[12109    28]
  [   50    39]]

 [[12212     2]
  [    9     3]]

 [[12182    20]
  [    4    20]]

 [[12213     1]
  [   11     1]]

 [[12200     3]
  [   17     6]]

 [[12177    12]
  [   15    22]]

 [[12208     1]
  [    3    14]]

 [[12187     3]
  [   31     5]]

 [[12213     1]
  [   12     0]]

 [[12182     7]
  [   17    20]]

 [[12181    13]
  [   15    17]]

 [[12169    18]
  [    7    32]]

 [[11155   325]
  [   90   656]]

 [[12150     2]
  [   11    63]]

 [[12149    19]
  [   13    45]]

 [[12173     5]
  [   20    28]]

 [[11641    83]
  [  199   303]]

 [[11879   107]
  [   73   167]]

 [[12182    11]
  [   12    21]]

 [[11803    79]
  [  120   224]]

 [[12006    29]
  [   50   141]]

 [[12194     1]
  [   19    12]]

 [[11524   318]
  [   64   320]]

 [[12069    40]
  [   28    89]]

 [[11720    70]
  [  126   310]]

 [[12166    11]
  [   33    16]]

 [[11784    40]
  [  106   296]]

 [[12209     0]
  [   17     0]]

 [[12183     1]
  [   20    22]]

 [[12149     0]
  [   16    61]]

 [[12048     6]
  [   17   155]]

 [[12207     0]
  [   11     8]]

 [[11650    77]
  [  198   301]]

 [[12122     5]
  [   36    63]]

 [[12214     1]
  [   11     0]]

 [[11975   148]
  [   12    91]]

 [[12197    11]
  [    9     9]]

 [[12215     0]
  [    7     4]]

 [[12186     5]
  [    2    33]]

 [[11928    67]
  [   92   139]]

 [[12162     7]
  [   11    46]]

 [[12186    11]
  [   28     1]]

 [[12146    32]
  [   27    21]]

 [[12146    31]
  [   36    13]]

 [[12182    10]
  [   14    20]]

 [[11954   117]
  [   30   125]]

 [[12207     5]
  [    6     8]]

 [[11765   146]
  [  127   188]]

 [[12152    11]
  [   61     2]]

 [[11603   316]
  [   59   248]]

 [[12139    18]
  [   44    25]]

 [[12154     6]
  [   45    21]]

 [[12211     0]
  [   15     0]]

 [[12195     6]
  [   12    13]]

 [[12208     0]
  [   18     0]]

 [[12123    44]
  [   46    13]]

 [[12012     8]
  [  109    97]]

 [[12052    98]
  [   45    31]]

 [[12161     6]
  [   25    34]]

 [[12039    47]
  [   77    63]]

 [[12182     2]
  [    8    34]]

 [[12014    37]
  [  107    68]]

 [[12174     9]
  [   27    16]]

 [[12194     7]
  [   20     5]]

 [[11971   150]
  [   39    66]]

 [[12211     1]
  [    8     6]]

 [[11939    45]
  [   72   170]]

 [[11893    23]
  [   70   240]]

 [[12161     6]
  [   16    43]]

 [[12211     4]
  [   11     0]]

 [[11965    74]
  [   80   107]]

 [[12165    15]
  [   28    18]]

 [[12175    11]
  [   28    12]]

 [[12137    56]
  [   12    21]]

 [[11487   450]
  [   71   218]]

 [[12194     0]
  [   32     0]]

 [[12141    10]
  [   21    54]]

 [[12197     1]
  [   26     2]]

 [[12182     7]
  [    9    28]]

 [[12202     1]
  [    4    19]]

 [[12196     5]
  [   24     1]]

 [[12152     8]
  [   42    24]]

 [[12204     1]
  [    4    17]]

 [[12150    11]
  [   13    52]]

 [[12172    14]
  [   29    11]]

 [[12214     0]
  [    3     9]]

 [[12109     4]
  [   36    77]]

 [[12034    30]
  [   34   128]]

 [[12199     3]
  [   14    10]]

 [[12159    14]
  [    8    45]]

 [[12207     5]
  [    5     9]]

 [[12053    50]
  [   32    91]]

 [[12183     2]
  [   22    19]]

 [[11743    54]
  [   27   402]]

 [[12154     7]
  [   22    43]]

 [[12182    13]
  [    8    23]]

 [[12050     3]
  [   44   129]]

 [[12189     7]
  [    5    25]]

 [[12101     8]
  [   23    94]]

 [[12078    12]
  [   20   116]]

 [[12084    81]
  [   13    48]]

 [[11908    93]
  [   11   214]]

 [[12189     2]
  [    8    27]]

 [[12177    11]
  [   21    17]]

 [[12188     8]
  [    4    26]]

 [[12208     2]
  [    3    13]]

 [[12202     2]
  [   13     9]]

 [[12103    50]
  [   11    62]]]

===scores report===
metrics	scores
Accuracy	0.6661
MCC	0.6595
log_loss	1.5309
f1 score weighted	0.6639
f1 score macro	0.5608
f1 score micro	0.6661
roc_auc ovr	0.9741
roc_auc ovo	0.9701
precision	0.7037
recall	0.6661

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f34c038f730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f34c038f5b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f34c038f790>]/home/amsequeira/enzymeClassification/models/hot_encoded/bi_attetion_500_post_post_lev3_ec50
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f9677ef9b50>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f9827632df0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f9827632e80>]
===TRAIN MODELS===

run_model
('Training Accuracy mean: ', 0.17613331973552704)
('Validation Accuracy mean: ', 0.1583708181977272)
('Training Loss mean: ', 3.6297687530517577)
('Validation Loss mean: ', 3.798559808731079)
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_5 (Masking)          (None, 500, 21)           0         
_________________________________________________________________
bidirectional_13 (Bidirectio (None, 500, 128)          44032     
_________________________________________________________________
bidirectional_14 (Bidirectio (None, 500, 128)          98816     
_________________________________________________________________
bidirectional_15 (Bidirectio (None, 500, 64)           41216     
_________________________________________________________________
attention_2 (attention)      (None, 64)                564       
_________________________________________________________________
dense_3 (Dense)              (None, 32)                2080      
_________________________________________________________________
batch_normalization_2 (Batch (None, 32)                128       
_________________________________________________________________
dropout_2 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 16)                528       
_________________________________________________________________
batch_normalization_3 (Batch (None, 16)                64        
_________________________________________________________________
dropout_3 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_5 (Dense)              (None, 128)               2176      
=================================================================
Total params: 189,604
Trainable params: 189,508
Non-trainable params: 96
_________________________________________________________________Finished run_model in 478.3373 secs


===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f9827632d60>, 'x_test': None, 'y_test': None, 'model': None}
report

              precision    recall  f1-score   support

           0       0.28      0.49      0.35       358
           1       0.00      0.00      0.00        12
           2       0.00      0.00      0.00        19
           3       0.10      0.03      0.04        79
           4       0.00      0.00      0.00        55
           5       0.14      0.02      0.03        58
           6       0.00      0.00      0.00        45
           7       0.24      0.10      0.14        48
           8       0.00      0.00      0.00        10
           9       0.00      0.00      0.00        21
          10       0.00      0.00      0.00        15
          11       0.55      0.33      0.41        36
          12       0.00      0.00      0.00        12
          13       0.00      0.00      0.00        25
          14       0.00      0.00      0.00        20
          15       0.23      0.14      0.17        22
          16       0.00      0.00      0.00        23
          17       0.30      0.33      0.31       118
          18       0.00      0.00      0.00        18
          19       0.00      0.00      0.00        13
          20       0.00      0.00      0.00        90
          21       0.00      0.00      0.00        12
          22       0.00      0.00      0.00        25
          23       0.00      0.00      0.00        12
          24       0.00      0.00      0.00        22
          25       0.00      0.00      0.00        37
          26       0.00      0.00      0.00        17
          27       0.00      0.00      0.00        35
          28       0.00      0.00      0.00        12
          29       0.00      0.00      0.00        36
          30       0.00      0.00      0.00        32
          31       0.50      0.05      0.09        39
          32       0.22      0.48      0.30       747
          33       0.17      0.45      0.24        74
          34       0.00      0.00      0.00        58
          35       0.00      0.00      0.00        48
          36       0.20      0.12      0.15       502
          37       0.32      0.54      0.40       241
          38       0.00      0.00      0.00        33
          39       0.33      0.22      0.26       344
          40       0.22      0.14      0.17       191
          41       0.00      0.00      0.00        32
          42       0.19      0.18      0.19       384
          43       0.37      0.19      0.25       117
          44       0.27      0.47      0.34       436
          45       0.00      0.00      0.00        49
          46       0.40      0.80      0.53       402
          47       0.00      0.00      0.00        17
          48       0.41      0.38      0.40        42
          49       0.34      0.51      0.40        77
          50       0.36      0.71      0.48       172
          51       0.00      0.00      0.00        20
          52       0.13      0.29      0.18       499
          53       0.22      0.15      0.18       100
          54       0.00      0.00      0.00        11
          55       0.21      0.07      0.10       103
          56       0.00      0.00      0.00        18
          57       0.00      0.00      0.00        10
          58       0.31      0.31      0.31        35
          59       0.18      0.11      0.14       231
          60       0.27      0.50      0.35        58
          61       0.00      0.00      0.00        29
          62       0.00      0.00      0.00        48
          63       0.00      0.00      0.00        50
          64       0.00      0.00      0.00        34
          65       0.00      0.00      0.00       155
          66       0.00      0.00      0.00        14
          67       0.09      0.04      0.05       314
          68       0.00      0.00      0.00        63
          69       0.39      0.61      0.47       307
          70       0.00      0.00      0.00        68
          71       0.00      0.00      0.00        66
          72       0.00      0.00      0.00        14
          73       0.00      0.00      0.00        25
          74       0.00      0.00      0.00        18
          75       0.00      0.00      0.00        59
          76       0.36      0.18      0.24       206
          77       0.00      0.00      0.00        77
          78       1.00      0.02      0.03        59
          79       0.45      0.16      0.23       139
          80       0.00      0.00      0.00        42
          81       0.24      0.02      0.04       175
          82       0.00      0.00      0.00        43
          83       0.00      0.00      0.00        26
          84       0.67      0.02      0.04       105
          85       0.00      0.00      0.00        14
          86       0.08      0.06      0.07       242
          87       0.28      0.75      0.40       309
          88       0.00      0.00      0.00        59
          89       0.00      0.00      0.00        11
          90       0.00      0.00      0.00       187
          91       0.00      0.00      0.00        46
          92       0.00      0.00      0.00        40
          93       0.00      0.00      0.00        33
          94       0.21      0.09      0.13       289
          95       0.00      0.00      0.00        32
          96       0.00      0.00      0.00        75
          97       0.00      0.00      0.00        27
          98       0.00      0.00      0.00        37
          99       0.00      0.00      0.00        24
         100       0.00      0.00      0.00        25
         101       0.00      0.00      0.00        65
         102       0.00      0.00      0.00        22
         103       1.00      0.02      0.03        64
         104       0.00      0.00      0.00        40
         105       0.00      0.00      0.00        12
         106       0.27      0.64      0.38       113
         107       0.37      0.30      0.33       161
         108       0.00      0.00      0.00        24
         109       0.15      0.15      0.15        52
         110       0.00      0.00      0.00        15
         111       0.17      0.12      0.14       123
         112       0.29      0.22      0.25        41
         113       0.29      0.78      0.42       430
         114       0.00      0.00      0.00        65
         115       0.00      0.00      0.00        31
         116       0.27      0.34      0.30       173
         117       0.00      0.00      0.00        31
         118       0.20      0.20      0.20       117
         119       0.21      0.27      0.23       136
         120       0.39      0.58      0.47        62
         121       0.63      0.18      0.28       224
         122       0.50      0.49      0.49        35
         123       0.32      0.32      0.32        37
         124       0.00      0.00      0.00        31
         125       0.00      0.00      0.00        16
         126       0.28      0.24      0.26        21
         127       0.42      0.63      0.50        73

    accuracy                           0.27     12227
   macro avg       0.13      0.12      0.10     12227
weighted avg       0.22      0.27      0.21     12227


===confusion_matrix===

[[176   0   0 ...   0   0   0]
 [  3   0   0 ...   0   0   0]
 [  0   0   0 ...   0   0   0]
 ...
 [  0   0   0 ...   0   1  13]
 [  0   0   0 ...   0   5   4]
 [  0   0   0 ...   0   3  46]]

===multilabel confusion matrix===

[[[11405   464]
  [  182   176]]

 [[12215     0]
  [   12     0]]

 [[12208     0]
  [   19     0]]

 [[12130    18]
  [   77     2]]

 [[12171     1]
  [   55     0]]

 [[12163     6]
  [   57     1]]

 [[12182     0]
  [   45     0]]

 [[12163    16]
  [   43     5]]

 [[12217     0]
  [   10     0]]

 [[12206     0]
  [   21     0]]

 [[12212     0]
  [   15     0]]

 [[12181    10]
  [   24    12]]

 [[12215     0]
  [   12     0]]

 [[12202     0]
  [   25     0]]

 [[12207     0]
  [   20     0]]

 [[12195    10]
  [   19     3]]

 [[12204     0]
  [   23     0]]

 [[12016    93]
  [   79    39]]

 [[12209     0]
  [   18     0]]

 [[12214     0]
  [   13     0]]

 [[12137     0]
  [   90     0]]

 [[12215     0]
  [   12     0]]

 [[12202     0]
  [   25     0]]

 [[12215     0]
  [   12     0]]

 [[12205     0]
  [   22     0]]

 [[12190     0]
  [   37     0]]

 [[12210     0]
  [   17     0]]

 [[12192     0]
  [   35     0]]

 [[12215     0]
  [   12     0]]

 [[12191     0]
  [   36     0]]

 [[12193     2]
  [   32     0]]

 [[12186     2]
  [   37     2]]

 [[10188  1292]
  [  391   356]]

 [[11986   167]
  [   41    33]]

 [[12169     0]
  [   58     0]]

 [[12179     0]
  [   48     0]]

 [[11485   240]
  [  441    61]]

 [[11703   283]
  [  110   131]]

 [[12194     0]
  [   33     0]]

 [[11728   155]
  [  268    76]]

 [[11941    95]
  [  164    27]]

 [[12195     0]
  [   32     0]]

 [[11551   292]
  [  314    70]]

 [[12072    38]
  [   95    22]]

 [[11243   548]
  [  232   204]]

 [[12178     0]
  [   49     0]]

 [[11337   488]
  [   82   320]]

 [[12210     0]
  [   17     0]]

 [[12162    23]
  [   26    16]]

 [[12073    77]
  [   38    39]]

 [[11839   216]
  [   50   122]]

 [[12207     0]
  [   20     0]]

 [[10790   938]
  [  356   143]]

 [[12073    54]
  [   85    15]]

 [[12216     0]
  [   11     0]]

 [[12098    26]
  [   96     7]]

 [[12209     0]
  [   18     0]]

 [[12217     0]
  [   10     0]]

 [[12168    24]
  [   24    11]]

 [[11877   119]
  [  205    26]]

 [[12091    78]
  [   29    29]]

 [[12198     0]
  [   29     0]]

 [[12179     0]
  [   48     0]]

 [[12177     0]
  [   50     0]]

 [[12193     0]
  [   34     0]]

 [[12072     0]
  [  155     0]]

 [[12213     0]
  [   14     0]]

 [[11803   110]
  [  303    11]]

 [[12164     0]
  [   63     0]]

 [[11627   293]
  [  121   186]]

 [[12159     0]
  [   68     0]]

 [[12160     1]
  [   66     0]]

 [[12213     0]
  [   14     0]]

 [[12202     0]
  [   25     0]]

 [[12209     0]
  [   18     0]]

 [[12168     0]
  [   59     0]]

 [[11954    67]
  [  169    37]]

 [[12150     0]
  [   77     0]]

 [[12168     0]
  [   58     1]]

 [[12061    27]
  [  117    22]]

 [[12185     0]
  [   42     0]]

 [[12039    13]
  [  171     4]]

 [[12184     0]
  [   43     0]]

 [[12201     0]
  [   26     0]]

 [[12121     1]
  [  103     2]]

 [[12213     0]
  [   14     0]]

 [[11829   156]
  [  228    14]]

 [[11316   602]
  [   78   231]]

 [[12168     0]
  [   59     0]]

 [[12216     0]
  [   11     0]]

 [[12040     0]
  [  187     0]]

 [[12181     0]
  [   46     0]]

 [[12187     0]
  [   40     0]]

 [[12194     0]
  [   33     0]]

 [[11843    95]
  [  263    26]]

 [[12195     0]
  [   32     0]]

 [[12152     0]
  [   75     0]]

 [[12200     0]
  [   27     0]]

 [[12190     0]
  [   37     0]]

 [[12203     0]
  [   24     0]]

 [[12202     0]
  [   25     0]]

 [[12158     4]
  [   65     0]]

 [[12205     0]
  [   22     0]]

 [[12163     0]
  [   63     1]]

 [[12187     0]
  [   40     0]]

 [[12215     0]
  [   12     0]]

 [[11921   193]
  [   41    72]]

 [[11983    83]
  [  112    49]]

 [[12203     0]
  [   24     0]]

 [[12128    47]
  [   44     8]]

 [[12212     0]
  [   15     0]]

 [[12032    72]
  [  108    15]]

 [[12164    22]
  [   32     9]]

 [[10979   818]
  [   94   336]]

 [[12161     1]
  [   65     0]]

 [[12196     0]
  [   31     0]]

 [[11895   159]
  [  115    58]]

 [[12196     0]
  [   31     0]]

 [[12017    93]
  [   94    23]]

 [[11948   143]
  [   99    37]]

 [[12109    56]
  [   26    36]]

 [[11979    24]
  [  183    41]]

 [[12175    17]
  [   18    17]]

 [[12164    26]
  [   25    12]]

 [[12191     5]
  [   31     0]]

 [[12211     0]
  [   16     0]]

 [[12193    13]
  [   16     5]]

 [[12090    64]
  [   27    46]]]

===scores report===
metrics	scores
Accuracy	0.2656
MCC	0.2451
log_loss	3.0942
f1 score weighted	0.2070
f1 score macro	0.1048
f1 score micro	0.2656
roc_auc ovr	0.8697
roc_auc ovo	0.8721
precision	0.2175
recall	0.2656

===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f34c038f910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([42., 42., 21., ..., 32., 70., 87.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.54      0.90      0.68       358
         1.0       1.00      0.58      0.74        12
         2.0       0.50      0.17      0.25        18
         3.0       0.45      0.67      0.54        79
         4.0       0.53      0.15      0.23        55
         5.0       0.33      0.03      0.06        58
         6.0       0.29      0.33      0.31        45
         7.0       0.64      0.64      0.64        47
         8.0       1.00      0.10      0.18        10
         9.0       0.72      0.62      0.67        21
        10.0       0.57      0.27      0.36        15
        11.0       0.81      0.81      0.81        36
        12.0       1.00      0.17      0.29        12
        13.0       0.81      0.52      0.63        25
        14.0       1.00      0.30      0.46        20
        15.0       0.83      0.86      0.84        22
        16.0       0.65      0.57      0.60        23
        17.0       0.79      0.78      0.79       118
        18.0       0.50      0.06      0.10        18
        19.0       0.00      0.00      0.00        13
        20.0       0.58      0.36      0.44        89
        21.0       1.00      0.38      0.56        13
        22.0       0.87      0.80      0.83        25
        23.0       1.00      0.17      0.29        12
        24.0       0.40      0.09      0.14        23
        25.0       0.77      0.62      0.69        37
        26.0       1.00      0.88      0.94        17
        27.0       0.75      0.08      0.15        36
        28.0       0.00      0.00      0.00        12
        29.0       0.73      0.61      0.67        36
        30.0       0.74      0.53      0.62        32
        31.0       0.89      0.87      0.88        39
        32.0       0.75      0.84      0.79       747
        33.0       1.00      0.77      0.87        74
        34.0       0.94      0.78      0.85        58
        35.0       0.85      0.60      0.70        47
        36.0       0.69      0.63      0.66       502
        37.0       0.89      0.52      0.66       240
        38.0       0.93      0.41      0.57        34
        39.0       0.33      0.84      0.47       344
        40.0       0.54      0.76      0.63       191
        41.0       0.86      0.59      0.70        32
        42.0       0.56      0.80      0.66       384
        43.0       0.70      0.76      0.73       117
        44.0       0.86      0.68      0.76       437
        45.0       0.61      0.22      0.33        49
        46.0       0.78      0.81      0.80       401
        47.0       0.00      0.00      0.00        17
        48.0       0.46      0.38      0.42        42
        49.0       0.94      0.78      0.85        77
        50.0       0.95      0.84      0.89       171
        51.0       0.65      0.75      0.70        20
        52.0       0.49      0.73      0.58       499
        53.0       0.61      0.54      0.57       100
        54.0       0.00      0.00      0.00        11
        55.0       0.87      0.83      0.85       104
        56.0       0.14      0.16      0.15        19
        57.0       0.00      0.00      0.00        11
        58.0       0.86      0.91      0.89        35
        59.0       0.62      0.61      0.62       230
        60.0       0.87      0.81      0.84        58
        61.0       0.00      0.00      0.00        29
        62.0       0.65      0.41      0.50        49
        63.0       0.11      0.02      0.03        50
        64.0       1.00      0.68      0.81        34
        65.0       0.88      0.75      0.81       155
        66.0       0.13      0.14      0.14        14
        67.0       0.71      0.56      0.63       314
        68.0       0.29      0.03      0.06        62
        69.0       0.49      0.71      0.58       307
        70.0       0.75      0.31      0.44        68
        71.0       0.68      0.39      0.50        66
        72.0       0.00      0.00      0.00        15
        73.0       0.82      0.56      0.67        25
        74.0       0.00      0.00      0.00        19
        75.0       1.00      0.03      0.07        59
        76.0       0.89      0.63      0.74       206
        77.0       0.55      0.44      0.49        77
        78.0       0.78      0.49      0.60        59
        79.0       0.55      0.77      0.64       139
        80.0       1.00      0.57      0.73        42
        81.0       0.74      0.30      0.43       174
        82.0       0.33      0.67      0.45        43
        83.0       1.00      0.08      0.15        25
        84.0       0.80      0.45      0.57       105
        85.0       1.00      0.60      0.75        15
        86.0       0.75      0.69      0.72       242
        87.0       0.66      0.87      0.75       309
        88.0       0.94      0.76      0.84        59
        89.0       1.00      0.18      0.31        11
        90.0       0.53      0.48      0.50       188
        91.0       0.64      0.34      0.44        47
        92.0       1.00      0.03      0.05        40
        93.0       0.81      0.39      0.53        33
        94.0       0.50      0.69      0.58       288
        95.0       0.00      0.00      0.00        32
        96.0       0.88      0.71      0.79        75
        97.0       0.50      0.11      0.18        27
        98.0       0.52      0.63      0.57        38
        99.0       1.00      0.87      0.93        23
       100.0       0.00      0.00      0.00        25
       101.0       0.38      0.36      0.37        66
       102.0       0.72      0.95      0.82        22
       103.0       0.94      0.52      0.67        64
       104.0       0.45      0.38      0.42        39
       105.0       0.67      0.67      0.67        12
       106.0       0.89      0.68      0.77       113
       107.0       0.80      0.73      0.76       161
       108.0       0.50      0.17      0.26        23
       109.0       0.98      0.85      0.91        53
       110.0       0.88      0.50      0.64        14
       111.0       0.77      0.73      0.75       123
       112.0       0.80      0.29      0.43        41
       113.0       0.81      0.93      0.87       429
       114.0       0.79      0.89      0.84        65
       115.0       0.65      0.55      0.60        31
       116.0       0.80      0.66      0.72       173
       117.0       0.96      0.87      0.92        31
       118.0       0.95      0.71      0.81       117
       119.0       0.98      0.72      0.83       135
       120.0       0.92      0.71      0.80        62
       121.0       0.89      0.83      0.86       224
       122.0       0.91      0.83      0.87        35
       123.0       0.86      0.51      0.64        37
       124.0       0.69      0.67      0.68        30
       125.0       0.90      0.56      0.69        16
       126.0       0.76      0.59      0.67        22
       127.0       0.67      0.88      0.76        73

    accuracy                           0.66     12226
   macro avg       0.68      0.51      0.55     12226
weighted avg       0.69      0.66      0.65     12226


===confusion_matrix===

[[323   0   0 ...   0   0   0]
 [  1   7   0 ...   0   0   0]
 [  0   0   3 ...   0   0   0]
 ...
 [  0   0   0 ...   9   2   1]
 [  0   0   0 ...   0  13   8]
 [  0   0   0 ...   0   2  64]]

===multilabel confusion matrix===

[[[11596   272]
  [   35   323]]

 [[12214     0]
  [    5     7]]

 [[12205     3]
  [   15     3]]

 [[12082    65]
  [   26    53]]

 [[12164     7]
  [   47     8]]

 [[12164     4]
  [   56     2]]

 [[12144    37]
  [   30    15]]

 [[12162    17]
  [   17    30]]

 [[12216     0]
  [    9     1]]

 [[12200     5]
  [    8    13]]

 [[12208     3]
  [   11     4]]

 [[12183     7]
  [    7    29]]

 [[12214     0]
  [   10     2]]

 [[12198     3]
  [   12    13]]

 [[12206     0]
  [   14     6]]

 [[12200     4]
  [    3    19]]

 [[12196     7]
  [   10    13]]

 [[12084    24]
  [   26    92]]

 [[12207     1]
  [   17     1]]

 [[12213     0]
  [   13     0]]

 [[12114    23]
  [   57    32]]

 [[12213     0]
  [    8     5]]

 [[12198     3]
  [    5    20]]

 [[12214     0]
  [   10     2]]

 [[12200     3]
  [   21     2]]

 [[12182     7]
  [   14    23]]

 [[12209     0]
  [    2    15]]

 [[12189     1]
  [   33     3]]

 [[12214     0]
  [   12     0]]

 [[12182     8]
  [   14    22]]

 [[12188     6]
  [   15    17]]

 [[12183     4]
  [    5    34]]

 [[11265   214]
  [  117   630]]

 [[12152     0]
  [   17    57]]

 [[12165     3]
  [   13    45]]

 [[12174     5]
  [   19    28]]

 [[11579   145]
  [  186   316]]

 [[11971    15]
  [  115   125]]

 [[12191     1]
  [   20    14]]

 [[11287   595]
  [   56   288]]

 [[11909   126]
  [   45   146]]

 [[12191     3]
  [   13    19]]

 [[11603   239]
  [   76   308]]

 [[12071    38]
  [   28    89]]

 [[11741    48]
  [  138   299]]

 [[12170     7]
  [   38    11]]

 [[11736    89]
  [   77   324]]

 [[12209     0]
  [   17     0]]

 [[12165    19]
  [   26    16]]

 [[12145     4]
  [   17    60]]

 [[12048     7]
  [   27   144]]

 [[12198     8]
  [    5    15]]

 [[11345   382]
  [  135   364]]

 [[12092    34]
  [   46    54]]

 [[12215     0]
  [   11     0]]

 [[12109    13]
  [   18    86]]

 [[12188    19]
  [   16     3]]

 [[12215     0]
  [   11     0]]

 [[12186     5]
  [    3    32]]

 [[11911    85]
  [   89   141]]

 [[12161     7]
  [   11    47]]

 [[12197     0]
  [   29     0]]

 [[12166    11]
  [   29    20]]

 [[12168     8]
  [   49     1]]

 [[12192     0]
  [   11    23]]

 [[12055    16]
  [   38   117]]

 [[12199    13]
  [   12     2]]

 [[11840    72]
  [  137   177]]

 [[12159     5]
  [   60     2]]

 [[11690   229]
  [   88   219]]

 [[12151     7]
  [   47    21]]

 [[12148    12]
  [   40    26]]

 [[12211     0]
  [   15     0]]

 [[12198     3]
  [   11    14]]

 [[12206     1]
  [   19     0]]

 [[12167     0]
  [   57     2]]

 [[12004    16]
  [   77   129]]

 [[12121    28]
  [   43    34]]

 [[12159     8]
  [   30    29]]

 [[11999    88]
  [   32   107]]

 [[12184     0]
  [   18    24]]

 [[12034    18]
  [  122    52]]

 [[12125    58]
  [   14    29]]

 [[12201     0]
  [   23     2]]

 [[12109    12]
  [   58    47]]

 [[12211     0]
  [    6     9]]

 [[11929    55]
  [   74   168]]

 [[11777   140]
  [   40   269]]

 [[12164     3]
  [   14    45]]

 [[12215     0]
  [    9     2]]

 [[11956    82]
  [   97    91]]

 [[12170     9]
  [   31    16]]

 [[12186     0]
  [   39     1]]

 [[12190     3]
  [   20    13]]

 [[11741   197]
  [   88   200]]

 [[12193     1]
  [   32     0]]

 [[12144     7]
  [   22    53]]

 [[12196     3]
  [   24     3]]

 [[12166    22]
  [   14    24]]

 [[12203     0]
  [    3    20]]

 [[12201     0]
  [   25     0]]

 [[12121    39]
  [   42    24]]

 [[12196     8]
  [    1    21]]

 [[12160     2]
  [   31    33]]

 [[12169    18]
  [   24    15]]

 [[12210     4]
  [    4     8]]

 [[12103    10]
  [   36    77]]

 [[12035    30]
  [   43   118]]

 [[12199     4]
  [   19     4]]

 [[12172     1]
  [    8    45]]

 [[12211     1]
  [    7     7]]

 [[12076    27]
  [   33    90]]

 [[12182     3]
  [   29    12]]

 [[11702    95]
  [   29   400]]

 [[12146    15]
  [    7    58]]

 [[12186     9]
  [   14    17]]

 [[12024    29]
  [   59   114]]

 [[12194     1]
  [    4    27]]

 [[12105     4]
  [   34    83]]

 [[12089     2]
  [   38    97]]

 [[12160     4]
  [   18    44]]

 [[11980    22]
  [   38   186]]

 [[12188     3]
  [    6    29]]

 [[12186     3]
  [   18    19]]

 [[12187     9]
  [   10    20]]

 [[12209     1]
  [    7     9]]

 [[12200     4]
  [    9    13]]

 [[12122    31]
  [    9    64]]]

===scores report===
metrics	scores
Accuracy	0.6621
MCC	0.6551
log_loss	1.5738
f1 score weighted	0.6507
f1 score macro	0.5461
f1 score micro	0.6621
roc_auc ovr	0.9718
roc_auc ovo	0.9688
precision	0.6940
recall	0.6621

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f34c038f730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f34c038f5b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f34c038f790>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f34c038f910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([ 42.,  42.,  42., ...,  58.,  76., 125.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.66      0.83      0.73       358
         1.0       0.28      0.75      0.41        12
         2.0       0.83      0.53      0.65        19
         3.0       0.86      0.47      0.61        79
         4.0       0.43      0.18      0.26        55
         5.0       0.32      0.12      0.17        58
         6.0       0.42      0.22      0.29        45
         7.0       0.77      0.51      0.62        47
         8.0       0.00      0.00      0.00        10
         9.0       0.53      0.38      0.44        21
        10.0       1.00      0.13      0.24        15
        11.0       0.84      0.75      0.79        36
        12.0       1.00      0.25      0.40        12
        13.0       0.95      0.76      0.84        25
        14.0       1.00      0.21      0.35        19
        15.0       0.81      0.59      0.68        22
        16.0       0.44      0.65      0.53        23
        17.0       0.84      0.84      0.84       118
        18.0       0.50      0.39      0.44        18
        19.0       0.50      0.08      0.14        12
        20.0       0.65      0.40      0.50        90
        21.0       0.50      0.38      0.43        13
        22.0       0.61      0.76      0.68        25
        23.0       0.00      0.00      0.00        13
        24.0       0.36      0.18      0.24        22
        25.0       0.56      0.50      0.53        38
        26.0       1.00      0.94      0.97        17
        27.0       0.23      0.09      0.13        35
        28.0       0.36      0.42      0.38        12
        29.0       0.62      0.50      0.55        36
        30.0       0.41      0.59      0.49        32
        31.0       0.89      0.89      0.89        38
        32.0       0.76      0.83      0.79       747
        33.0       0.82      0.91      0.86        75
        34.0       0.96      0.75      0.84        59
        35.0       0.76      0.53      0.62        47
        36.0       0.50      0.78      0.61       501
        37.0       0.71      0.64      0.67       241
        38.0       0.89      0.48      0.63        33
        39.0       0.48      0.82      0.61       344
        40.0       0.80      0.81      0.80       192
        41.0       0.77      0.62      0.69        32
        42.0       0.81      0.71      0.76       384
        43.0       0.64      0.82      0.72       117
        44.0       0.79      0.67      0.73       436
        45.0       0.79      0.22      0.35        49
        46.0       0.88      0.73      0.80       401
        47.0       0.00      0.00      0.00        17
        48.0       0.67      0.24      0.35        42
        49.0       0.63      0.92      0.75        77
        50.0       0.87      0.85      0.86       172
        51.0       1.00      0.55      0.71        20
        52.0       0.78      0.60      0.68       499
        53.0       0.87      0.60      0.71       100
        54.0       0.00      0.00      0.00        11
        55.0       0.94      0.70      0.80       104
        56.0       0.62      0.28      0.38        18
        57.0       0.00      0.00      0.00        10
        58.0       0.78      0.83      0.81        35
        59.0       0.45      0.72      0.55       230
        60.0       0.78      0.81      0.80        58
        61.0       0.00      0.00      0.00        29
        62.0       0.84      0.43      0.57        49
        63.0       0.21      0.30      0.24        50
        64.0       0.77      0.68      0.72        34
        65.0       0.87      0.76      0.81       155
        66.0       0.25      0.07      0.11        14
        67.0       0.56      0.55      0.56       314
        68.0       0.25      0.06      0.10        62
        69.0       0.39      0.78      0.52       307
        70.0       0.28      0.28      0.28        68
        71.0       0.63      0.41      0.50        66
        72.0       0.00      0.00      0.00        14
        73.0       0.47      0.67      0.55        24
        74.0       0.00      0.00      0.00        19
        75.0       0.58      0.12      0.19        60
        76.0       0.75      0.68      0.72       206
        77.0       0.37      0.30      0.33        77
        78.0       0.80      0.63      0.70        59
        79.0       0.74      0.50      0.59       139
        80.0       0.89      0.76      0.82        42
        81.0       0.51      0.32      0.39       174
        82.0       0.89      0.37      0.52        43
        83.0       1.00      0.12      0.21        26
        84.0       0.85      0.52      0.64       106
        85.0       0.89      0.53      0.67        15
        86.0       0.47      0.73      0.57       241
        87.0       0.56      0.87      0.68       309
        88.0       0.84      0.64      0.73        59
        89.0       0.50      0.20      0.29        10
        90.0       0.58      0.62      0.60       188
        91.0       0.45      0.50      0.47        46
        92.0       0.10      0.02      0.04        41
        93.0       0.75      0.28      0.41        32
        94.0       0.72      0.44      0.55       288
        95.0       0.33      0.16      0.22        31
        96.0       0.84      0.61      0.71        75
        97.0       0.71      0.19      0.29        27
        98.0       0.88      0.55      0.68        38
        99.0       0.79      0.92      0.85        24
       100.0       0.33      0.04      0.07        25
       101.0       0.53      0.54      0.53        65
       102.0       0.80      0.73      0.76        22
       103.0       0.96      0.75      0.84        64
       104.0       0.71      0.30      0.42        40
       105.0       1.00      0.83      0.91        12
       106.0       0.60      0.73      0.66       113
       107.0       0.66      0.81      0.72       161
       108.0       0.56      0.21      0.30        24
       109.0       0.69      0.83      0.75        52
       110.0       0.81      0.87      0.84        15
       111.0       0.73      0.75      0.74       124
       112.0       0.86      0.46      0.60        41
       113.0       0.85      0.94      0.89       430
       114.0       0.63      0.78      0.70        65
       115.0       1.00      0.45      0.62        31
       116.0       0.96      0.75      0.84       173
       117.0       0.88      0.74      0.81        31
       118.0       0.85      0.81      0.83       117
       119.0       0.91      0.82      0.86       136
       120.0       0.60      0.84      0.70        62
       121.0       0.83      0.87      0.85       224
       122.0       0.74      0.74      0.74        35
       123.0       0.85      0.59      0.70        37
       124.0       0.88      0.68      0.76        31
       125.0       0.69      0.73      0.71        15
       126.0       0.63      0.81      0.71        21
       127.0       0.74      0.81      0.77        73

    accuracy                           0.67     12226
   macro avg       0.64      0.53      0.55     12226
weighted avg       0.69      0.67      0.66     12226


===confusion_matrix===

[[296   0   0 ...   0   0   0]
 [  0   9   0 ...   0   0   0]
 [  0   0  10 ...   0   0   0]
 ...
 [  0   0   0 ...  11   0   0]
 [  0   0   0 ...   0  17   3]
 [  0   0   0 ...   2   8  59]]

===multilabel confusion matrix===

[[[11714   154]
  [   62   296]]

 [[12191    23]
  [    3     9]]

 [[12205     2]
  [    9    10]]

 [[12141     6]
  [   42    37]]

 [[12158    13]
  [   45    10]]

 [[12153    15]
  [   51     7]]

 [[12167    14]
  [   35    10]]

 [[12172     7]
  [   23    24]]

 [[12216     0]
  [   10     0]]

 [[12198     7]
  [   13     8]]

 [[12211     0]
  [   13     2]]

 [[12185     5]
  [    9    27]]

 [[12214     0]
  [    9     3]]

 [[12200     1]
  [    6    19]]

 [[12207     0]
  [   15     4]]

 [[12201     3]
  [    9    13]]

 [[12184    19]
  [    8    15]]

 [[12089    19]
  [   19    99]]

 [[12201     7]
  [   11     7]]

 [[12213     1]
  [   11     1]]

 [[12117    19]
  [   54    36]]

 [[12208     5]
  [    8     5]]

 [[12189    12]
  [    6    19]]

 [[12213     0]
  [   13     0]]

 [[12197     7]
  [   18     4]]

 [[12173    15]
  [   19    19]]

 [[12209     0]
  [    1    16]]

 [[12181    10]
  [   32     3]]

 [[12205     9]
  [    7     5]]

 [[12179    11]
  [   18    18]]

 [[12167    27]
  [   13    19]]

 [[12184     4]
  [    4    34]]

 [[11277   202]
  [  124   623]]

 [[12136    15]
  [    7    68]]

 [[12165     2]
  [   15    44]]

 [[12171     8]
  [   22    25]]

 [[11339   386]
  [  112   389]]

 [[11921    64]
  [   86   155]]

 [[12191     2]
  [   17    16]]

 [[11582   300]
  [   63   281]]

 [[11994    40]
  [   36   156]]

 [[12188     6]
  [   12    20]]

 [[11776    66]
  [  111   273]]

 [[12056    53]
  [   21    96]]

 [[11711    79]
  [  143   293]]

 [[12174     3]
  [   38    11]]

 [[11786    39]
  [  107   294]]

 [[12209     0]
  [   17     0]]

 [[12179     5]
  [   32    10]]

 [[12108    41]
  [    6    71]]

 [[12033    21]
  [   26   146]]

 [[12206     0]
  [    9    11]]

 [[11644    83]
  [  198   301]]

 [[12117     9]
  [   40    60]]

 [[12214     1]
  [   11     0]]

 [[12117     5]
  [   31    73]]

 [[12205     3]
  [   13     5]]

 [[12214     2]
  [   10     0]]

 [[12183     8]
  [    6    29]]

 [[11792   204]
  [   65   165]]

 [[12155    13]
  [   11    47]]

 [[12197     0]
  [   29     0]]

 [[12173     4]
  [   28    21]]

 [[12118    58]
  [   35    15]]

 [[12185     7]
  [   11    23]]

 [[12053    18]
  [   37   118]]

 [[12209     3]
  [   13     1]]

 [[11776   136]
  [  140   174]]

 [[12152    12]
  [   58     4]]

 [[11536   383]
  [   67   240]]

 [[12109    49]
  [   49    19]]

 [[12144    16]
  [   39    27]]

 [[12211     1]
  [   14     0]]

 [[12184    18]
  [    8    16]]

 [[12206     1]
  [   19     0]]

 [[12161     5]
  [   53     7]]

 [[11973    47]
  [   65   141]]

 [[12109    40]
  [   54    23]]

 [[12158     9]
  [   22    37]]

 [[12063    24]
  [   70    69]]

 [[12180     4]
  [   10    32]]

 [[12000    52]
  [  119    55]]

 [[12181     2]
  [   27    16]]

 [[12200     0]
  [   23     3]]

 [[12110    10]
  [   51    55]]

 [[12210     1]
  [    7     8]]

 [[11789   196]
  [   66   175]]

 [[11710   207]
  [   41   268]]

 [[12160     7]
  [   21    38]]

 [[12214     2]
  [    8     2]]

 [[11953    85]
  [   72   116]]

 [[12152    28]
  [   23    23]]

 [[12176     9]
  [   40     1]]

 [[12191     3]
  [   23     9]]

 [[11888    50]
  [  161   127]]

 [[12185    10]
  [   26     5]]

 [[12142     9]
  [   29    46]]

 [[12197     2]
  [   22     5]]

 [[12185     3]
  [   17    21]]

 [[12196     6]
  [    2    22]]

 [[12199     2]
  [   24     1]]

 [[12130    31]
  [   30    35]]

 [[12200     4]
  [    6    16]]

 [[12160     2]
  [   16    48]]

 [[12181     5]
  [   28    12]]

 [[12214     0]
  [    2    10]]

 [[12058    55]
  [   30    83]]

 [[11997    68]
  [   31   130]]

 [[12198     4]
  [   19     5]]

 [[12155    19]
  [    9    43]]

 [[12208     3]
  [    2    13]]

 [[12068    34]
  [   31    93]]

 [[12182     3]
  [   22    19]]

 [[11725    71]
  [   27   403]]

 [[12131    30]
  [   14    51]]

 [[12195     0]
  [   17    14]]

 [[12048     5]
  [   43   130]]

 [[12192     3]
  [    8    23]]

 [[12092    17]
  [   22    95]]

 [[12079    11]
  [   25   111]]

 [[12129    35]
  [   10    52]]

 [[11963    39]
  [   29   195]]

 [[12182     9]
  [    9    26]]

 [[12185     4]
  [   15    22]]

 [[12192     3]
  [   10    21]]

 [[12206     5]
  [    4    11]]

 [[12195    10]
  [    4    17]]

 [[12132    21]
  [   14    59]]]

===scores report===
metrics	scores
Accuracy	0.6659
MCC	0.6589
log_loss	1.5588
f1 score weighted	0.6562
f1 score macro	0.5515
f1 score micro	0.6659
roc_auc ovr	0.9722
roc_auc ovo	0.9673
precision	0.6870
recall	0.6659

===TRAIN MODELS with CV===

train_model_cv	Accuracy	MCC	log_loss	f1 score weighted	f1 score macro	f1 score micro	roc_auc ovr	roc_auc ovo	precision	recall
0	0.6843870123497179	0.6777378762570793	1.4446480655078486	0.6781812951425755	0.573289897024373	0.6843870123497179	0.9750598118374345	0.9720739120561651	0.7035870651333912	0.6843870123497179
1	0.6653308252228675	0.6579781584056275	1.5199462742906573	0.6574014889049111	0.5609910938197556	0.6653308252228675	0.9716203416728977	0.9673859995205746	0.690403973138299	0.6653308252228675
2	0.6661213806641584	0.659534792091655	1.5309271435835592	0.6638645311994689	0.5607501387777546	0.6661213806641584	0.9740663406861627	0.9700554511073676	0.703670225765437	0.6661213806641584
3	0.6621135285457223	0.6550968003332905	1.5737524219717611	0.6506824793898872	0.5460936636694819	0.6621135285457223	0.9718123923136675	0.9687811883307207	0.6939977491149846	0.6621135285457223
4	0.6658760019630297	0.6588867485229487	1.5588477594692411	0.656178900850672	0.5515470021584115	0.6658760019630297	0.9722397741959351	0.9672541328306207	0.6870311686777465	0.6658760019630297
mean	0.6687657497490992	0.6618468751221201	1.5256243329646135	0.661261739097503	0.5585343590899553	0.6687657497490992	0.9729597321412194	0.9691101367690897	0.6957380363659718	0.6687657497490992
std	0.00794251759584502	0.00808903545772204	0.04480541275824412	0.009441434378763035	0.009298012522311366	0.00794251759584502	0.0013611474682555524	0.0018001772188797368	0.006809071249939849	0.00794251759584502

Accuracy
MCC
log_loss
f1 score weighted
f1 score macro
f1 score micro
roc_auc ovr
roc_auc ovo
precision
recallFinished train_model_cv in 27227.4422 secs

