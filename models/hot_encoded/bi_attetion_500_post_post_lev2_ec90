/home/amsequeira/enzymeClassification/models/hot_encoded/bi_attetion_500_post_post_lev2_ec90
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f2d7c4e26d0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f2d7c4e25b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f2d7c4e2730>]
===TRAIN MODELS===

run_model
('Training Accuracy mean: ', 0.7866550703843435)
('Validation Accuracy mean: ', 0.5335166597366333)
('Training Loss mean: ', 0.9990275287628174)
('Validation Loss mean: ', 2.228842018445333)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking (Masking)            (None, 500, 21)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, 500, 128)          44032     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 500, 128)          98816     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 500, 64)           41216     
_________________________________________________________________
attention (attention)        (None, 64)                564       
_________________________________________________________________
dense (Dense)                (None, 32)                2080      
_________________________________________________________________
batch_normalization (BatchNo (None, 32)                128       
_________________________________________________________________
dropout (Dropout)            (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16)                64        
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 59)                1003      
=================================================================
Total params: 188,431
Trainable params: 188,335
Non-trainable params: 96
_________________________________________________________________Finished run_model in 15235.3350 secs


===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f2d7c4e2910>, 'x_test': None, 'y_test': None, 'model': None}
report

              precision    recall  f1-score   support

           0       0.86      0.97      0.91       911
           1       0.79      0.85      0.82        53
           2       0.97      0.84      0.90       179
           3       0.50      0.36      0.42        25
           4       0.87      0.74      0.80       112
           5       0.95      0.84      0.89       491
           6       0.97      0.94      0.95        64
           7       0.94      0.76      0.84        38
           8       0.98      0.96      0.97       205
           9       0.96      0.94      0.95        71
          10       0.94      0.71      0.81       404
          11       0.93      0.82      0.87        17
          12       0.91      0.85      0.88       378
          13       0.90      0.93      0.92       191
          14       0.67      0.16      0.26        76
          15       0.90      0.91      0.90        66
          16       0.95      0.84      0.89       140
          17       0.75      0.90      0.82       182
          18       1.00      1.00      1.00        12
          19       1.00      0.89      0.94        37
          20       0.97      0.92      0.94      2162
          21       0.95      0.99      0.97       168
          22       0.88      0.81      0.84      1470
          23       0.92      0.94      0.93      1259
          24       0.97      0.96      0.97       956
          25       0.94      0.83      0.88       282
          26       0.88      0.93      0.90      3919
          27       0.99      0.96      0.98       531
          28       1.00      0.08      0.14        13
          29       0.77      0.82      0.80      2346
          30       0.74      0.88      0.81       615
          31       1.00      0.06      0.12        32
          32       0.74      0.90      0.81      1449
          33       0.84      0.96      0.90       893
          34       0.94      0.79      0.86      1377
          35       1.00      0.91      0.95        22
          36       0.90      0.94      0.92       844
          37       0.84      0.92      0.88      1142
          38       0.98      0.92      0.95       314
          39       0.91      0.86      0.88        56
          40       0.88      0.75      0.81       154
          41       0.96      0.94      0.95        52
          42       0.87      0.91      0.89       247
          43       0.95      0.89      0.92       197
          44       0.97      0.96      0.96       529
          45       0.93      0.96      0.94       540
          46       1.00      0.50      0.67        20
          47       0.90      0.94      0.92        80
          48       0.99      0.99      0.99      1466
          49       0.97      0.96      0.96       148
          50       0.94      0.98      0.96      1453
          51       0.86      0.50      0.63        12
          52       0.98      0.84      0.91       151
          53       0.98      0.53      0.69       903
          54       0.61      0.77      0.68       108
          55       0.97      0.81      0.88        93
          56       0.97      0.94      0.95        33
          57       0.73      0.96      0.83        50
          58       0.81      0.82      0.82       154

    accuracy                           0.89     29892
   macro avg       0.90      0.82      0.84     29892
weighted avg       0.89      0.89      0.89     29892


===confusion_matrix===

[[883   0   0 ...   0   0   0]
 [  0  45   0 ...   0   0   0]
 [  0   0 150 ...   0   0   0]
 ...
 [  0   0   0 ...  31   1   0]
 [  0   0   0 ...   0  48   2]
 [  0   0   0 ...   0   9 127]]

===multilabel confusion matrix===

[[[28837   144]
  [   28   883]]

 [[29827    12]
  [    8    45]]

 [[29709     4]
  [   29   150]]

 [[29858     9]
  [   16     9]]

 [[29768    12]
  [   29    83]]

 [[29379    22]
  [   79   412]]

 [[29826     2]
  [    4    60]]

 [[29852     2]
  [    9    29]]

 [[29684     3]
  [    8   197]]

 [[29818     3]
  [    4    67]]

 [[29468    20]
  [  116   288]]

 [[29874     1]
  [    3    14]]

 [[29483    31]
  [   57   321]]

 [[29681    20]
  [   13   178]]

 [[29810     6]
  [   64    12]]

 [[29819     7]
  [    6    60]]

 [[29746     6]
  [   23   117]]

 [[29655    55]
  [   18   164]]

 [[29880     0]
  [    0    12]]

 [[29855     0]
  [    4    33]]

 [[27672    58]
  [  180  1982]]

 [[29716     8]
  [    1   167]]

 [[28257   165]
  [  275  1195]]

 [[28528   105]
  [   70  1189]]

 [[28906    30]
  [   36   920]]

 [[29596    14]
  [   48   234]]

 [[25471   502]
  [  286  3633]]

 [[29358     3]
  [   20   511]]

 [[29879     0]
  [   12     1]]

 [[26981   565]
  [  423  1923]]

 [[29091   186]
  [   73   542]]

 [[29860     0]
  [   30     2]]

 [[27983   460]
  [  150  1299]]

 [[28842   157]
  [   40   853]]

 [[28441    74]
  [  286  1091]]

 [[29870     0]
  [    2    20]]

 [[28960    88]
  [   48   796]]

 [[28547   203]
  [   93  1049]]

 [[29572     6]
  [   25   289]]

 [[29831     5]
  [    8    48]]

 [[29723    15]
  [   39   115]]

 [[29838     2]
  [    3    49]]

 [[29612    33]
  [   23   224]]

 [[29685    10]
  [   21   176]]

 [[29345    18]
  [   22   507]]

 [[29311    41]
  [   24   516]]

 [[29872     0]
  [   10    10]]

 [[29804     8]
  [    5    75]]

 [[28406    20]
  [    9  1457]]

 [[29739     5]
  [    6   142]]

 [[28356    83]
  [   30  1423]]

 [[29879     1]
  [    6     6]]

 [[29739     2]
  [   24   127]]

 [[28977    12]
  [  420   483]]

 [[29732    52]
  [   25    83]]

 [[29797     2]
  [   18    75]]

 [[29858     1]
  [    2    31]]

 [[29824    18]
  [    2    48]]

 [[29709    29]
  [   27   127]]]

===scores report===
metrics	scores
Accuracy	0.8883
MCC	0.8824
log_loss	0.5095
f1 score weighted	0.8862
f1 score macro	0.8397
f1 score micro	0.8883
roc_auc ovr	0.9932
roc_auc ovo	0.9916
precision	0.8941
recall	0.8883

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f2d7c4e26d0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f2d7c4e25b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f2d7c4e2730>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f2d7c4e2910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([24., 24., 24., ..., 32., 22., 22.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.88      0.88      0.88       911
         1.0       0.97      0.58      0.73        53
         2.0       0.90      0.77      0.83       179
         3.0       0.71      0.20      0.31        25
         4.0       0.65      0.45      0.53       112
         5.0       0.93      0.61      0.74       491
         6.0       0.91      0.81      0.86        64
         7.0       0.86      0.16      0.27        37
         8.0       0.90      0.85      0.88       206
         9.0       0.93      0.77      0.85        71
        10.0       0.96      0.88      0.92       404
        11.0       0.35      0.44      0.39        16
        12.0       0.82      0.79      0.81       378
        13.0       0.89      0.73      0.80       191
        14.0       0.40      0.05      0.09        76
        15.0       0.83      0.59      0.69        66
        16.0       0.96      0.78      0.86       141
        17.0       0.61      0.80      0.69       182
        18.0       1.00      0.75      0.86        12
        19.0       1.00      0.87      0.93        38
        20.0       0.89      0.92      0.90      2162
        21.0       0.98      0.94      0.96       168
        22.0       0.92      0.71      0.80      1470
        23.0       0.92      0.81      0.86      1259
        24.0       0.96      0.86      0.91       956
        25.0       0.80      0.93      0.86       283
        26.0       0.96      0.82      0.88      3919
        27.0       0.83      0.92      0.87       531
        28.0       1.00      0.58      0.74        12
        29.0       0.71      0.84      0.77      2345
        30.0       0.54      0.84      0.65       615
        31.0       1.00      0.66      0.79        32
        32.0       0.64      0.79      0.71      1449
        33.0       0.62      0.89      0.73       893
        34.0       0.95      0.87      0.91      1377
        35.0       1.00      0.45      0.62        22
        36.0       0.84      0.86      0.85       844
        37.0       0.95      0.86      0.90      1142
        38.0       0.98      0.85      0.91       314
        39.0       0.77      0.59      0.67        56
        40.0       0.98      0.66      0.79       154
        41.0       0.86      0.94      0.90        52
        42.0       0.82      0.79      0.80       247
        43.0       0.87      0.81      0.84       198
        44.0       0.95      0.91      0.93       529
        45.0       0.82      0.92      0.87       540
        46.0       0.75      0.15      0.25        20
        47.0       1.00      0.65      0.79        80
        48.0       0.99      0.98      0.99      1466
        49.0       0.96      0.85      0.90       148
        50.0       0.79      0.98      0.87      1453
        51.0       0.67      0.17      0.27        12
        52.0       0.99      0.82      0.90       151
        53.0       0.95      0.96      0.96       903
        54.0       0.88      0.70      0.78       108
        55.0       0.76      0.97      0.85        93
        56.0       0.93      0.85      0.89        33
        57.0       0.92      0.71      0.80        49
        58.0       0.89      0.86      0.88       154

    accuracy                           0.85     29892
   macro avg       0.86      0.74      0.77     29892
weighted avg       0.86      0.85      0.85     29892


===confusion_matrix===

[[798   0   1 ...   0   0   0]
 [  0  31   0 ...   0   0   0]
 [  2   0 138 ...   0   0   0]
 ...
 [  0   0   0 ...  28   0   1]
 [  0   0   0 ...   1  35   8]
 [  0   0   0 ...   0   3 133]]

===multilabel confusion matrix===

[[[28869   112]
  [  113   798]]

 [[29838     1]
  [   22    31]]

 [[29697    16]
  [   41   138]]

 [[29865     2]
  [   20     5]]

 [[29753    27]
  [   62    50]]

 [[29378    23]
  [  191   300]]

 [[29823     5]
  [   12    52]]

 [[29854     1]
  [   31     6]]

 [[29667    19]
  [   30   176]]

 [[29817     4]
  [   16    55]]

 [[29475    13]
  [   49   355]]

 [[29863    13]
  [    9     7]]

 [[29447    67]
  [   78   300]]

 [[29684    17]
  [   52   139]]

 [[29810     6]
  [   72     4]]

 [[29818     8]
  [   27    39]]

 [[29746     5]
  [   31   110]]

 [[29617    93]
  [   36   146]]

 [[29880     0]
  [    3     9]]

 [[29854     0]
  [    5    33]]

 [[27489   241]
  [  178  1984]]

 [[29721     3]
  [   10   158]]

 [[28326    96]
  [  433  1037]]

 [[28546    87]
  [  234  1025]]

 [[28905    31]
  [  136   820]]

 [[29541    68]
  [   19   264]]

 [[25827   146]
  [  706  3213]]

 [[29260   101]
  [   42   489]]

 [[29880     0]
  [    5     7]]

 [[26727   820]
  [  377  1968]]

 [[28833   444]
  [  101   514]]

 [[29860     0]
  [   11    21]]

 [[27795   648]
  [  301  1148]]

 [[28501   498]
  [   97   796]]

 [[28453    62]
  [  184  1193]]

 [[29870     0]
  [   12    10]]

 [[28907   141]
  [  121   723]]

 [[28701    49]
  [  163   979]]

 [[29573     5]
  [   47   267]]

 [[29826    10]
  [   23    33]]

 [[29736     2]
  [   52   102]]

 [[29832     8]
  [    3    49]]

 [[29601    44]
  [   53   194]]

 [[29671    23]
  [   38   160]]

 [[29335    28]
  [   46   483]]

 [[29244   108]
  [   44   496]]

 [[29871     1]
  [   17     3]]

 [[29812     0]
  [   28    52]]

 [[28408    18]
  [   24  1442]]

 [[29739     5]
  [   22   126]]

 [[28054   385]
  [   36  1417]]

 [[29879     1]
  [   10     2]]

 [[29740     1]
  [   27   124]]

 [[28947    42]
  [   33   870]]

 [[29774    10]
  [   32    76]]

 [[29771    28]
  [    3    90]]

 [[29857     2]
  [    5    28]]

 [[29840     3]
  [   14    35]]

 [[29721    17]
  [   21   133]]]

===scores report===
metrics	scores
Accuracy	0.8458
MCC	0.8383
log_loss	0.6206
f1 score weighted	0.8473
f1 score macro	0.7704
f1 score micro	0.8458
roc_auc ovr	0.9913
roc_auc ovo	0.9894
precision	0.8634
recall	0.8458

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f2d7c4e26d0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f2d7c4e25b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f2d7c4e2730>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f2d7c4e2910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([24., 12., 12., ..., 32., 34., 56.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.94      0.83      0.88       912
         1.0       1.00      0.70      0.82        53
         2.0       0.88      0.80      0.84       179
         3.0       0.62      0.20      0.30        25
         4.0       0.90      0.40      0.56       112
         5.0       0.77      0.74      0.75       492
         6.0       0.94      0.74      0.83        65
         7.0       0.87      0.53      0.66        38
         8.0       0.86      0.86      0.86       206
         9.0       0.84      0.68      0.75        71
        10.0       0.95      0.87      0.91       405
        11.0       1.00      0.41      0.58        17
        12.0       0.82      0.76      0.79       377
        13.0       0.88      0.76      0.81       191
        14.0       0.67      0.16      0.26        76
        15.0       0.75      0.61      0.67        66
        16.0       0.92      0.78      0.84       140
        17.0       0.73      0.86      0.79       182
        18.0       0.92      1.00      0.96        11
        19.0       0.97      0.86      0.91        37
        20.0       0.93      0.93      0.93      2163
        21.0       0.92      0.93      0.93       169
        22.0       0.88      0.78      0.83      1469
        23.0       0.81      0.90      0.86      1259
        24.0       0.94      0.87      0.90       956
        25.0       0.92      0.88      0.90       282
        26.0       0.90      0.88      0.89      3919
        27.0       0.95      0.91      0.93       531
        28.0       0.75      1.00      0.86        12
        29.0       0.62      0.87      0.72      2346
        30.0       0.71      0.72      0.71       615
        31.0       0.96      0.81      0.88        32
        32.0       0.88      0.72      0.79      1450
        33.0       0.89      0.75      0.82       893
        34.0       0.84      0.91      0.88      1376
        35.0       0.77      0.45      0.57        22
        36.0       0.81      0.86      0.84       843
        37.0       0.90      0.87      0.88      1142
        38.0       0.87      0.89      0.88       314
        39.0       0.94      0.52      0.67        56
        40.0       0.81      0.58      0.67       154
        41.0       1.00      0.94      0.97        52
        42.0       0.95      0.79      0.87       247
        43.0       0.77      0.86      0.81       198
        44.0       0.95      0.87      0.91       529
        45.0       0.91      0.91      0.91       539
        46.0       1.00      0.26      0.42        19
        47.0       0.76      0.72      0.74        80
        48.0       0.87      0.99      0.92      1466
        49.0       0.92      0.89      0.90       148
        50.0       0.95      0.94      0.94      1453
        51.0       0.00      0.00      0.00        12
        52.0       0.95      0.91      0.93       151
        53.0       0.94      0.96      0.95       903
        54.0       0.92      0.74      0.82       108
        55.0       0.98      0.88      0.93        93
        56.0       0.80      0.85      0.82        33
        57.0       0.83      0.90      0.86        49
        58.0       0.83      0.87      0.85       154

    accuracy                           0.86     29892
   macro avg       0.86      0.76      0.79     29892
weighted avg       0.87      0.86      0.86     29892


===confusion_matrix===

[[753   0   0 ...   0   0   1]
 [  0  37   1 ...   0   0   0]
 [  0   0 143 ...   0   0   0]
 ...
 [  0   0   0 ...  28   2   0]
 [  0   0   0 ...   0  44   4]
 [  0   0   0 ...   1   4 134]]

===multilabel confusion matrix===

[[[28933    47]
  [  159   753]]

 [[29839     0]
  [   16    37]]

 [[29693    20]
  [   36   143]]

 [[29864     3]
  [   20     5]]

 [[29775     5]
  [   67    45]]

 [[29291   109]
  [  130   362]]

 [[29824     3]
  [   17    48]]

 [[29851     3]
  [   18    20]]

 [[29656    30]
  [   28   178]]

 [[29812     9]
  [   23    48]]

 [[29469    18]
  [   54   351]]

 [[29875     0]
  [   10     7]]

 [[29452    63]
  [   90   287]]

 [[29681    20]
  [   46   145]]

 [[29810     6]
  [   64    12]]

 [[29813    13]
  [   26    40]]

 [[29743     9]
  [   31   109]]

 [[29651    59]
  [   26   156]]

 [[29880     1]
  [    0    11]]

 [[29854     1]
  [    5    32]]

 [[27578   151]
  [  161  2002]]

 [[29709    14]
  [   11   158]]

 [[28273   150]
  [  318  1151]]

 [[28371   262]
  [  121  1138]]

 [[28887    49]
  [  126   830]]

 [[29587    23]
  [   34   248]]

 [[25607   366]
  [  468  3451]]

 [[29334    27]
  [   46   485]]

 [[29876     4]
  [    0    12]]

 [[26300  1246]
  [  316  2030]]

 [[29091   186]
  [  170   445]]

 [[29859     1]
  [    6    26]]

 [[28295   147]
  [  407  1043]]

 [[28915    84]
  [  220   673]]

 [[28277   239]
  [  117  1259]]

 [[29867     3]
  [   12    10]]

 [[28882   167]
  [  119   724]]

 [[28634   116]
  [  152   990]]

 [[29538    40]
  [   36   278]]

 [[29834     2]
  [   27    29]]

 [[29717    21]
  [   65    89]]

 [[29840     0]
  [    3    49]]

 [[29635    10]
  [   51   196]]

 [[29643    51]
  [   27   171]]

 [[29340    23]
  [   68   461]]

 [[29305    48]
  [   47   492]]

 [[29873     0]
  [   14     5]]

 [[29794    18]
  [   22    58]]

 [[28203   223]
  [   14  1452]]

 [[29733    11]
  [   17   131]]

 [[28369    70]
  [   91  1362]]

 [[29880     0]
  [   12     0]]

 [[29734     7]
  [   13   138]]

 [[28938    51]
  [   36   867]]

 [[29777     7]
  [   28    80]]

 [[29797     2]
  [   11    82]]

 [[29852     7]
  [    5    28]]

 [[29834     9]
  [    5    44]]

 [[29710    28]
  [   20   134]]]

===scores report===
metrics	scores
Accuracy	0.8568
MCC	0.8494
log_loss	0.5869
f1 score weighted	0.8567
f1 score macro	0.7910
f1 score micro	0.8568
roc_auc ovr	0.9913
roc_auc ovo	0.9890
precision	0.8660
recall	0.8568

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f2d7c4e26d0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f2d7c4e25b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f2d7c4e2730>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f2d7c4e2910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 1, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([24., 24., 24., ..., 17., 30., 30.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.86      0.87      0.87       912
         1.0       0.83      0.77      0.80        52
         2.0       0.85      0.81      0.83       179
         3.0       0.32      0.28      0.30        25
         4.0       0.74      0.38      0.51       112
         5.0       0.90      0.74      0.82       492
         6.0       0.84      0.86      0.85        65
         7.0       0.84      0.42      0.56        38
         8.0       0.87      0.83      0.85       205
         9.0       0.88      0.69      0.77        71
        10.0       0.94      0.87      0.90       405
        11.0       1.00      0.65      0.79        17
        12.0       0.63      0.84      0.72       377
        13.0       0.92      0.76      0.83       190
        14.0       0.40      0.05      0.09        76
        15.0       0.84      0.72      0.77        67
        16.0       0.76      0.88      0.81       140
        17.0       0.80      0.83      0.81       183
        18.0       1.00      0.92      0.96        12
        19.0       0.91      0.86      0.89        37
        20.0       0.91      0.93      0.92      2162
        21.0       0.99      0.94      0.96       169
        22.0       0.79      0.82      0.80      1470
        23.0       0.88      0.86      0.87      1259
        24.0       0.85      0.93      0.89       956
        25.0       0.87      0.92      0.90       282
        26.0       0.90      0.88      0.89      3918
        27.0       0.95      0.91      0.93       531
        28.0       1.00      0.85      0.92        13
        29.0       0.86      0.75      0.80      2346
        30.0       0.69      0.73      0.71       615
        31.0       0.92      0.75      0.83        32
        32.0       0.75      0.84      0.79      1450
        33.0       0.84      0.83      0.84       893
        34.0       0.82      0.90      0.86      1376
        35.0       0.67      0.55      0.60        22
        36.0       0.82      0.85      0.83       843
        37.0       0.90      0.87      0.89      1142
        38.0       0.88      0.91      0.89       314
        39.0       0.46      0.47      0.47        55
        40.0       0.94      0.66      0.78       154
        41.0       1.00      0.75      0.86        52
        42.0       0.94      0.75      0.84       247
        43.0       0.92      0.89      0.90       197
        44.0       0.84      0.91      0.87       530
        45.0       0.97      0.88      0.92       540
        46.0       0.50      0.05      0.10        19
        47.0       0.89      0.61      0.72        79
        48.0       0.93      0.99      0.96      1465
        49.0       0.83      0.94      0.88       149
        50.0       0.93      0.95      0.94      1453
        51.0       0.67      0.17      0.27        12
        52.0       0.97      0.87      0.92       152
        53.0       0.85      0.97      0.91       903
        54.0       0.74      0.82      0.78       108
        55.0       0.86      0.91      0.89        93
        56.0       0.84      0.97      0.90        32
        57.0       0.81      0.96      0.88        50
        58.0       0.92      0.87      0.89       154

    accuracy                           0.86     29892
   macro avg       0.83      0.77      0.79     29892
weighted avg       0.86      0.86      0.86     29892


===confusion_matrix===

[[796   0   3 ...   0   0   0]
 [  1  40   0 ...   0   0   0]
 [  0   0 145 ...   0   0   0]
 ...
 [  0   0   0 ...  31   0   0]
 [  0   0   0 ...   0  48   1]
 [  0   0   0 ...   0   4 134]]

===multilabel confusion matrix===

[[[28853   127]
  [  116   796]]

 [[29832     8]
  [   12    40]]

 [[29688    25]
  [   34   145]]

 [[29852    15]
  [   18     7]]

 [[29765    15]
  [   69    43]]

 [[29361    39]
  [  126   366]]

 [[29816    11]
  [    9    56]]

 [[29851     3]
  [   22    16]]

 [[29662    25]
  [   34   171]]

 [[29814     7]
  [   22    49]]

 [[29463    24]
  [   53   352]]

 [[29875     0]
  [    6    11]]

 [[29329   186]
  [   59   318]]

 [[29690    12]
  [   46   144]]

 [[29810     6]
  [   72     4]]

 [[29816     9]
  [   19    48]]

 [[29713    39]
  [   17   123]]

 [[29672    37]
  [   32   151]]

 [[29880     0]
  [    1    11]]

 [[29852     3]
  [    5    32]]

 [[27530   200]
  [  143  2019]]

 [[29721     2]
  [   10   159]]

 [[28097   325]
  [  265  1205]]

 [[28486   147]
  [  172  1087]]

 [[28773   163]
  [   67   889]]

 [[29572    38]
  [   22   260]]

 [[25601   373]
  [  474  3444]]

 [[29335    26]
  [   46   485]]

 [[29879     0]
  [    2    11]]

 [[27272   274]
  [  598  1748]]

 [[29078   199]
  [  169   446]]

 [[29858     2]
  [    8    24]]

 [[28037   405]
  [  236  1214]]

 [[28862   137]
  [  150   743]]

 [[28238   278]
  [  131  1245]]

 [[29864     6]
  [   10    12]]

 [[28892   157]
  [  130   713]]

 [[28640   110]
  [  144   998]]

 [[29538    40]
  [   28   286]]

 [[29807    30]
  [   29    26]]

 [[29731     7]
  [   52   102]]

 [[29840     0]
  [   13    39]]

 [[29633    12]
  [   61   186]]

 [[29680    15]
  [   22   175]]

 [[29270    92]
  [   48   482]]

 [[29337    15]
  [   64   476]]

 [[29872     1]
  [   18     1]]

 [[29807     6]
  [   31    48]]

 [[28321   106]
  [   14  1451]]

 [[29715    28]
  [    9   140]]

 [[28342    97]
  [   70  1383]]

 [[29879     1]
  [   10     2]]

 [[29736     4]
  [   20   132]]

 [[28839   150]
  [   23   880]]

 [[29753    31]
  [   19    89]]

 [[29785    14]
  [    8    85]]

 [[29854     6]
  [    1    31]]

 [[29831    11]
  [    2    48]]

 [[29726    12]
  [   20   134]]]

===scores report===
metrics	scores
Accuracy	0.8625
MCC	0.8553
log_loss	0.5560
f1 score weighted	0.8607
f1 score macro	0.7886
f1 score micro	0.8625
roc_auc ovr	0.9918
roc_auc ovo	0.9901
precision	0.8642
recall	0.8625

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f2d7c4e26d0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f2d7c4e25b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f2d7c4e2730>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f2d7c4e2910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([24., 24., 24., ..., 20., 22., 34.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.79      0.92      0.85       912
         1.0       1.00      0.83      0.91        52
         2.0       0.92      0.75      0.82       179
         3.0       0.00      0.00      0.00        24
         4.0       0.88      0.44      0.58       112
         5.0       0.98      0.57      0.72       492
         6.0       0.96      0.77      0.85        64
         7.0       1.00      0.29      0.45        38
         8.0       0.94      0.83      0.88       205
         9.0       0.94      0.71      0.81        70
        10.0       0.86      0.90      0.88       405
        11.0       0.58      0.41      0.48        17
        12.0       0.80      0.73      0.76       378
        13.0       0.83      0.79      0.81       191
        14.0       0.67      0.05      0.10        76
        15.0       0.93      0.55      0.69        67
        16.0       0.88      0.74      0.80       140
        17.0       0.74      0.79      0.76       183
        18.0       1.00      0.92      0.96        12
        19.0       1.00      0.95      0.97        37
        20.0       0.95      0.92      0.93      2162
        21.0       0.97      0.92      0.95       168
        22.0       0.72      0.87      0.79      1470
        23.0       0.95      0.84      0.89      1259
        24.0       0.98      0.88      0.93       955
        25.0       0.90      0.95      0.93       282
        26.0       0.88      0.89      0.89      3918
        27.0       0.94      0.89      0.92       532
        28.0       1.00      1.00      1.00        13
        29.0       0.68      0.86      0.76      2346
        30.0       0.59      0.81      0.68       616
        31.0       0.96      0.72      0.82        32
        32.0       0.72      0.83      0.77      1449
        33.0       0.78      0.83      0.81       893
        34.0       0.97      0.85      0.91      1377
        35.0       0.93      0.64      0.76        22
        36.0       0.86      0.86      0.86       844
        37.0       0.95      0.87      0.91      1142
        38.0       0.97      0.87      0.91       314
        39.0       1.00      0.68      0.81        56
        40.0       0.94      0.65      0.77       153
        41.0       0.97      0.76      0.86        51
        42.0       0.88      0.76      0.82       246
        43.0       0.92      0.84      0.88       197
        44.0       0.98      0.85      0.91       530
        45.0       0.93      0.90      0.91       540
        46.0       1.00      0.05      0.10        20
        47.0       0.96      0.69      0.80        80
        48.0       0.99      0.97      0.98      1465
        49.0       0.86      0.89      0.88       148
        50.0       0.98      0.94      0.96      1453
        51.0       0.33      0.15      0.21        13
        52.0       0.92      0.87      0.89       151
        53.0       0.96      0.96      0.96       904
        54.0       0.93      0.80      0.86       108
        55.0       0.94      0.88      0.91        93
        56.0       0.94      0.88      0.91        33
        57.0       0.78      1.00      0.88        50
        58.0       0.91      0.90      0.90       153

    accuracy                           0.86     29892
   macro avg       0.87      0.76      0.79     29892
weighted avg       0.87      0.86      0.86     29892


===confusion_matrix===

[[839   0   0 ...   0   0   0]
 [  0  43   0 ...   0   0   0]
 [  2   0 134 ...   0   0   0]
 ...
 [  0   0   0 ...  29   1   0]
 [  0   0   0 ...   0  50   0]
 [  0   0   0 ...   1   6 137]]

===multilabel confusion matrix===

[[[28759   221]
  [   73   839]]

 [[29840     0]
  [    9    43]]

 [[29701    12]
  [   45   134]]

 [[29868     0]
  [   24     0]]

 [[29773     7]
  [   63    49]]

 [[29393     7]
  [  210   282]]

 [[29826     2]
  [   15    49]]

 [[29854     0]
  [   27    11]]

 [[29677    10]
  [   35   170]]

 [[29819     3]
  [   20    50]]

 [[29427    60]
  [   40   365]]

 [[29870     5]
  [   10     7]]

 [[29443    71]
  [  102   276]]

 [[29671    30]
  [   40   151]]

 [[29814     2]
  [   72     4]]

 [[29822     3]
  [   30    37]]

 [[29738    14]
  [   37   103]]

 [[29659    50]
  [   39   144]]

 [[29880     0]
  [    1    11]]

 [[29855     0]
  [    2    35]]

 [[27635    95]
  [  181  1981]]

 [[29719     5]
  [   13   155]]

 [[27920   502]
  [  191  1279]]

 [[28574    59]
  [  205  1054]]

 [[28918    19]
  [  110   845]]

 [[29581    29]
  [   14   268]]

 [[25520   454]
  [  446  3472]]

 [[29332    28]
  [   59   473]]

 [[29879     0]
  [    0    13]]

 [[26603   943]
  [  332  2014]]

 [[28926   350]
  [  117   499]]

 [[29859     1]
  [    9    23]]

 [[27984   459]
  [  246  1203]]

 [[28796   203]
  [  154   739]]

 [[28475    40]
  [  202  1175]]

 [[29869     1]
  [    8    14]]

 [[28926   122]
  [  117   727]]

 [[28702    48]
  [  152   990]]

 [[29569     9]
  [   42   272]]

 [[29836     0]
  [   18    38]]

 [[29733     6]
  [   53   100]]

 [[29840     1]
  [   12    39]]

 [[29620    26]
  [   58   188]]

 [[29680    15]
  [   32   165]]

 [[29353     9]
  [   79   451]]

 [[29315    37]
  [   55   485]]

 [[29872     0]
  [   19     1]]

 [[29810     2]
  [   25    55]]

 [[28415    12]
  [   48  1417]]

 [[29723    21]
  [   16   132]]

 [[28405    34]
  [   93  1360]]

 [[29875     4]
  [   11     2]]

 [[29729    12]
  [   19   132]]

 [[28950    38]
  [   38   866]]

 [[29778     6]
  [   22    86]]

 [[29794     5]
  [   11    82]]

 [[29857     2]
  [    4    29]]

 [[29828    14]
  [    0    50]]

 [[29726    13]
  [   16   137]]]

===scores report===
metrics	scores
Accuracy	0.8621
MCC	0.8549
log_loss	0.5555
f1 score weighted	0.8626
f1 score macro	0.7913
f1 score micro	0.8621
roc_auc ovr	0.9923
roc_auc ovo	0.9906
precision	0.8743
recall	0.8621

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f2d7c4e26d0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f2d7c4e25b0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f2d7c4e2730>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f2d7c4e2910>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([24., 12., 12., ..., 32., 34., 34.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.92      0.85      0.88       911
         1.0       0.90      0.72      0.80        53
         2.0       0.89      0.79      0.84       180
         3.0       0.71      0.20      0.31        25
         4.0       0.47      0.56      0.51       111
         5.0       0.62      0.79      0.70       491
         6.0       1.00      0.86      0.92        64
         7.0       0.73      0.30      0.42        37
         8.0       0.83      0.82      0.83       205
         9.0       0.73      0.82      0.77        71
        10.0       0.95      0.84      0.89       404
        11.0       1.00      0.06      0.11        17
        12.0       0.85      0.80      0.83       378
        13.0       0.91      0.73      0.81       191
        14.0       0.56      0.13      0.21        76
        15.0       0.85      0.52      0.64        66
        16.0       0.96      0.74      0.84       140
        17.0       0.84      0.74      0.79       182
        18.0       1.00      1.00      1.00        12
        19.0       1.00      0.70      0.83        37
        20.0       0.97      0.90      0.93      2162
        21.0       0.98      0.91      0.94       168
        22.0       0.87      0.80      0.83      1470
        23.0       0.93      0.82      0.87      1259
        24.0       0.95      0.87      0.91       955
        25.0       0.77      0.96      0.86       283
        26.0       0.86      0.90      0.88      3919
        27.0       0.91      0.92      0.92       532
        28.0       1.00      0.92      0.96        13
        29.0       0.60      0.91      0.72      2345
        30.0       0.87      0.57      0.69       616
        31.0       1.00      0.81      0.90        32
        32.0       0.87      0.74      0.80      1449
        33.0       0.82      0.81      0.82       893
        34.0       0.91      0.91      0.91      1377
        35.0       1.00      0.41      0.58        22
        36.0       0.90      0.85      0.87       844
        37.0       0.95      0.86      0.90      1142
        38.0       0.76      0.92      0.83       314
        39.0       0.89      0.55      0.68        56
        40.0       0.96      0.59      0.73       153
        41.0       0.94      0.90      0.92        52
        42.0       0.94      0.72      0.82       247
        43.0       0.86      0.89      0.87       197
        44.0       0.98      0.84      0.90       529
        45.0       0.88      0.92      0.90       540
        46.0       1.00      0.05      0.10        20
        47.0       0.79      0.62      0.70        80
        48.0       0.96      0.98      0.97      1466
        49.0       0.89      0.90      0.90       148
        50.0       0.87      0.94      0.90      1453
        51.0       0.00      0.00      0.00        12
        52.0       0.99      0.78      0.87       151
        53.0       0.93      0.97      0.95       904
        54.0       0.84      0.69      0.76       108
        55.0       0.94      0.99      0.96        93
        56.0       0.97      0.91      0.94        33
        57.0       1.00      0.86      0.92        50
        58.0       0.88      0.94      0.91       154

    accuracy                           0.86     29892
   macro avg       0.87      0.74      0.78     29892
weighted avg       0.87      0.86      0.86     29892


===confusion_matrix===

[[770   0   0 ...   0   0   0]
 [  0  38   0 ...   0   0   0]
 [  0   0 142 ...   0   0   0]
 ...
 [  0   0   0 ...  30   0   0]
 [  0   0   0 ...   1  43   5]
 [  0   0   0 ...   0   0 144]]

===multilabel confusion matrix===

[[[28917    64]
  [  141   770]]

 [[29835     4]
  [   15    38]]

 [[29695    17]
  [   38   142]]

 [[29865     2]
  [   20     5]]

 [[29712    69]
  [   49    62]]

 [[29168   233]
  [  103   388]]

 [[29828     0]
  [    9    55]]

 [[29851     4]
  [   26    11]]

 [[29652    35]
  [   36   169]]

 [[29800    21]
  [   13    58]]

 [[29469    19]
  [   63   341]]

 [[29875     0]
  [   16     1]]

 [[29462    52]
  [   76   302]]

 [[29688    13]
  [   52   139]]

 [[29808     8]
  [   66    10]]

 [[29820     6]
  [   32    34]]

 [[29748     4]
  [   36   104]]

 [[29685    25]
  [   47   135]]

 [[29880     0]
  [    0    12]]

 [[29855     0]
  [   11    26]]

 [[27670    60]
  [  220  1942]]

 [[29721     3]
  [   15   153]]

 [[28245   177]
  [  290  1180]]

 [[28561    72]
  [  228  1031]]

 [[28889    48]
  [  121   834]]

 [[29528    81]
  [   11   272]]

 [[25423   550]
  [  400  3519]]

 [[29314    46]
  [   43   489]]

 [[29879     0]
  [    1    12]]

 [[26123  1424]
  [  215  2130]]

 [[29224    52]
  [  265   351]]

 [[29860     0]
  [    6    26]]

 [[28284   159]
  [  382  1067]]

 [[28845   154]
  [  172   721]]

 [[28387   128]
  [  126  1251]]

 [[29870     0]
  [   13     9]]

 [[28967    81]
  [  129   715]]

 [[28693    57]
  [  157   985]]

 [[29489    89]
  [   26   288]]

 [[29832     4]
  [   25    31]]

 [[29735     4]
  [   63    90]]

 [[29837     3]
  [    5    47]]

 [[29634    11]
  [   68   179]]

 [[29667    28]
  [   22   175]]

 [[29353    10]
  [   84   445]]

 [[29283    69]
  [   41   499]]

 [[29872     0]
  [   19     1]]

 [[29799    13]
  [   30    50]]

 [[28369    57]
  [   25  1441]]

 [[29728    16]
  [   15   133]]

 [[28228   211]
  [   92  1361]]

 [[29880     0]
  [   12     0]]

 [[29740     1]
  [   33   118]]

 [[28925    63]
  [   29   875]]

 [[29770    14]
  [   34    74]]

 [[29793     6]
  [    1    92]]

 [[29858     1]
  [    3    30]]

 [[29842     0]
  [    7    43]]

 [[29719    19]
  [   10   144]]]

===scores report===
metrics	scores
Accuracy	0.8566
MCC	0.8494
log_loss	0.5967
f1 score weighted	0.8571
f1 score macro	0.7757
f1 score micro	0.8566
roc_auc ovr	0.9916
roc_auc ovo	0.9889
precision	0.8708
recall	0.8566

===TRAIN MODELS with CV===

train_model_cv	Accuracy	MCC	log_loss	f1 score weighted	f1 score macro	f1 score micro	roc_auc ovr	roc_auc ovo	precision	recall
0	0.8458450421517463	0.8383490676013111	0.6205625810677731	0.8473147813263691	0.770356215573416	0.8458450421517463	0.9913199920013047	0.9893893398718219	0.8634127401871959	0.8458450421517463
1	0.85675097015924	0.8494106619447179	0.5869341883589037	0.8566820889589838	0.7909520647129719	0.85675097015924	0.9912660456755654	0.989042328126847	0.8659849765599867	0.85675097015924
2	0.86247156429814	0.8552871763293085	0.5560265396157177	0.8606996127120314	0.7886096921518135	0.86247156429814	0.9917950656878948	0.9900631596306071	0.8641984457319511	0.86247156429814
3	0.8621370266291984	0.8549083653242208	0.5555426771846018	0.8626062465307678	0.7913022998992802	0.8621370266291986	0.992293031857554	0.9905575210346595	0.8742980548269017	0.8621370266291984
4	0.8565837013247691	0.8494043847936056	0.5967475598772964	0.8570774459789691	0.7757152041492071	0.8565837013247691	0.9916273311105135	0.9888812491762656	0.8708342585660284	0.8565837013247691
mean	0.8567576609126188	0.8494719311986328	0.5831627092208584	0.8568760351014241	0.7833870952973379	0.8567576609126188	0.9916602932665665	0.9895867195680402	0.8677456951744128	0.8567576609126188
std	0.006011685659385651	0.006117161154892622	0.024886785678085784	0.005271637771472616	0.008669676859290086	0.006011685659385671	0.0003717107337584279	0.0006328443537242367	0.004169658221175226	0.006011685659385651

Accuracy
MCC
log_loss
f1 score weighted
f1 score macro
f1 score micro
roc_auc ovr
roc_auc ovo
precision
recallFinished train_model_cv in 65079.9868 secs

