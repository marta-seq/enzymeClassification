{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import operator\n",
    "import random\n",
    "from random import sample\n",
    "import gc\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import logging\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# logging.disable(logging.WARNING)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "# from silence_tensorflow import silence_tensorflow\n",
    "# silence_tensorflow()\n",
    "import tensorflow as tf\n",
    "from itertools import chain\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # tf.config.experimental.set_visible_devices(gpus[:1], 'GPU')\n",
    "\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "from collections import Counter\n",
    "\n",
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.FATAL)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from nlf_blosum_encoding import blosum_encode\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, ConvLSTM2D, BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, matthews_corrcoef, classification_report,\n",
    "    multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPool1D, MaxPool2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, TimeDistributed, Bidirectional, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Convolution2D, GRU, TimeDistributed, Reshape,\n",
    "    MaxPooling2D, Convolution1D, BatchNormalization, Masking\n",
    "\n",
    "# input is 21 categorical 200 paded aa sequences\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from get_ec import get_ec_1_level, binarize_labels, get_ec_2_level_more_than_x_samples, remove_zeros,\n",
    "    get_ec_3_level_more_than_x_samples,\n",
    "    get_ec_complete_more_than_x_samples\n",
    "from dataset_characterization import get_counts\n",
    "from get_prot_representation import pad_sequence, deal_with_strange_aa\n",
    "from deep_ml import DeepML\n",
    "\n",
    "\n",
    "\n",
    "def divide_dataset(fps_x, fps_y, test_size=0.2, val_size=0.1):\n",
    "    # divide in train, test and validation\n",
    "    x_train_1, x_test, y_train_1, y_test = train_test_split(fps_x, fps_y, test_size=test_size, random_state=42,\n",
    "                                                            shuffle=True, stratify=fps_y)\n",
    "\n",
    "    # iterative_train_test_split(fps_x, fps_y, test_size=test_size)\n",
    "    train_percentage = 1 - test_size\n",
    "    val_size = val_size / train_percentage\n",
    "\n",
    "    x_train, x_dval, y_train, y_dval = train_test_split(x_train_1, y_train_1, test_size=val_size, random_state=42,\n",
    "                                                        shuffle=True, stratify=y_train_1)\n",
    "\n",
    "    # stratify=y_train_1, shuffle=True)\n",
    "\n",
    "    return x_train, x_test, x_dval, y_train, y_test, y_dval\n",
    "\n",
    "# PAD ZEROS 200 20 aa  X = 0 categorical encoding\n",
    "def pad_sequence(df, seq_len=700, padding='pre', truncating='pre', alphabet = \"XARNDCEQGHILKMFPSTWYV\"):\n",
    "    # sequences_original = df['sequence'].tolist()\n",
    "    # sequences=[]\n",
    "    # for seq in sequences_original:\n",
    "    #     seq1 = seq.replace('B', 'N')  # asparagine N / aspartic acid  D - asx - B\n",
    "    #     seq2 = seq1.replace('Z', 'Q')  # glutamine Q / glutamic acid  E - glx - Z\n",
    "    #     seq3 = seq2.replace('U',\n",
    "    #                         'C')  # selenocisteina, the closest is the cisteine. but it is a different aminoacid . take care.\n",
    "    #     seq4 = seq3.replace('O', 'K')  # Pyrrolysine to lysine\n",
    "    #     sequences.append(seq4)\n",
    "    sequences = df\n",
    "\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    # {'X': 0,\n",
    "    #  'A': 1,\n",
    "    #  'R': 2,\n",
    "    #  'N': 3,\n",
    "    #  'D': 4,...\n",
    "    sequences_integer_ecoded = []\n",
    "    for seq in sequences:\n",
    "        # seq = seq.replace('X', 0)  # unknown character eliminated\n",
    "        # define a mapping of chars to integers\n",
    "        # integer encode input data\n",
    "        integer_encoded = [char_to_int[char] for char in seq]\n",
    "        sequences_integer_ecoded.append(integer_encoded)\n",
    "    fps_x = pad_sequences(sequences_integer_ecoded, maxlen=seq_len, padding=padding, truncating=truncating, value=0.0)   # (4042, 200)\n",
    "    return fps_x\n",
    "\n",
    "\n",
    "# FROM UNIREF 90 DATASET\n",
    "# Get dataset\n",
    "hot_90 = pd.read_csv('/home/amsequeira/deepbio/datasets/ecpred/ecpred_uniprot_uniref_90.csv', low_memory=False)\n",
    "lev_1_single_label = get_ec_1_level(hot_90, single_label=True)\n",
    "# lev_1_single_label = get_ec_2_level_more_than_x_samples(hot_90, x=50, single_label=True)\n",
    "# lev_1_single_label = get_ec_3_level_more_than_x_samples(hot_90, x=50, single_label=True)\n",
    "# lev_1_single_label = get_ec_complete_more_than_x_samples(hot_90, x=50, single_label=True)\n",
    "\n",
    "lev_1_single_label = lev_1_single_label.loc[lev_1_single_label['sequence'].str.contains('!!!') == False]\n",
    "lev_1_single_label = remove_zeros(column='ec_single_label', data=lev_1_single_label)  # without zeros\n",
    "lev_1_single_label = lev_1_single_label.dropna(subset=['sequence'])\n",
    "print(get_counts(column=lev_1_single_label['ec_single_label']))\n",
    "seq_len = 500\n",
    "label = lev_1_single_label['ec_single_label']\n",
    "fps_y_encoded, fps_y_hot, ecs = binarize_labels(label)\n",
    "alphabet = \"ARNDCEQGHILKMFPSTWYV\"\n",
    "alphabet_x = \"XARNDCEQGHILKMFPSTWYV\"\n",
    "alphabet_all_characters = \"XARNDCEQGHILKMFPSTWYVBZUO\"\n",
    "sequences = deal_with_strange_aa(sequences=lev_1_single_label['sequence'],\n",
    "                                 alphabet=alphabet)  # in this case will substitute strange aa and supress X\n",
    "\n",
    "# from truncating import get_middle, get_terminals\n",
    "# seq_new_list = []\n",
    "# for seq in sequences:\n",
    "#     # seq_new = get_middle(seq,seq_len)\n",
    "#     seq_new = get_terminals(seq,seq_len)\n",
    "#     seq_new_list.append(seq_new)\n",
    "# print(len(max(seq_new_list, key=len)))\n",
    "# sequences = seq_new_list\n",
    "# print(len(max(sequences, key=len)))\n",
    "print(tf.executing_eagerly())\n",
    "print('execution eager')\n",
    "fps_x = pad_sequence(sequences, seq_len=seq_len, padding='post', truncating='pre',\n",
    "                     alphabet=\"XARNDCEQGHILKMFPSTWYV\")\n",
    "# remake sequence is the sequence padded with haracters of aa. the alphabet needs to have X. for the A not be the Zero and not be padded.\n",
    "\n",
    "print(fps_x)\n",
    "print(fps_x.shape)\n",
    "fps_x_hot = to_categorical(fps_x)\n",
    "# # (174756, 1500, 21)\n",
    "print(fps_x_hot.shape)\n",
    "# # print(fps_y_encoded.shape)\n",
    "# # print(label.shape)\n",
    "# print(lev_1_single_label.shape)\n",
    "fps_x_hot_flat = fps_x_hot.reshape(fps_x_hot.shape[0], fps_x_hot.shape[1] * fps_x_hot.shape[2])\n",
    "\n",
    "x_train, x_test, x_dval, y_train, y_test, y_dval =\n",
    "    divide_dataset(fps_x_hot, fps_y_encoded, test_size=0.2, val_size=0.2)\n",
    "vector_size = x_train.shape[1]\n",
    "final_units = fps_y_hot.shape[1]\n",
    "\n",
    "# https://github.com/philipperemy/keract\n",
    "# https://medium.com/asap-report/visualizing-lstm-networks-part-i-f1d3fa6aace7\n",
    "# https://www.mathworks.com/help/deeplearning/ug/visualize-features-of-lstm-network.html\n",
    "\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "class attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = K.softmax(et)\n",
    "        at = K.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention, self).get_config()\n",
    "\n",
    "\n",
    "def bilstm_attention(input_dim, number_classes,\n",
    "                     n_features=20,\n",
    "                     optimizer='Adam',\n",
    "                     lstm_layers=(64, 64, 32),\n",
    "                     activation='tanh',\n",
    "                     recurrent_activation='sigmoid',\n",
    "                     dropout_rate=(0.1, 0.1, 0.1),\n",
    "                     l1=1e-5, l2=1e-4,\n",
    "                     dense_layers=(32, 16),\n",
    "                     dropout_rate_dense=(0.1, 0.1),\n",
    "                     dense_activation=\"relu\", loss='sparse_categorical_crossentropy'):\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        # input dim timesteps = seq size , features. 21 features per character\n",
    "        model.add(Input(shape=(input_dim, n_features,), dtype='float32', name='main_input'))\n",
    "        # add initial dropout\n",
    "\n",
    "        # model.add(Masking(mask_value=0, input_shape=(n_in, 1)))\n",
    "        model.add(Masking(mask_value=0))\n",
    "        for layer in range(len(lstm_layers)):\n",
    "            model.add(Bidirectional(\n",
    "                LSTM(units=lstm_layers[layer], return_sequences=True, activation=activation,\n",
    "                     recurrent_activation=recurrent_activation,\n",
    "                     kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
    "                     dropout=dropout_rate[layer], recurrent_dropout=0.0), input_shape=(input_dim, 20,)))\n",
    "            # model.add(LSTM(units=lstm_layers[layer], return_sequences=True, activation=activation,\n",
    "            #          recurrent_activation=recurrent_activation,\n",
    "            #          kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
    "            #          dropout=dropout_rate[layer], recurrent_dropout=0.0)\n",
    "\n",
    "        # receives LSTM with return sequences =True\n",
    "\n",
    "        # add attention\n",
    "        # model.add(Attention(return_sequences=False)) # receive 3D and output 2D\n",
    "        model.add(attention())\n",
    "        # a, context = attention()(model)\n",
    "        # add denses\n",
    "        for layer in range(len(dense_layers)):\n",
    "            model.add(Dense(units=dense_layers[layer], activation=dense_activation,\n",
    "                            kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(dropout_rate_dense[layer]))\n",
    "\n",
    "        # Add Classification Dense, Compile model and make it ready for optimization\n",
    "        model.add(Dense(number_classes, activation='softmax'))\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=bilstm_attention, input_dim=vector_size, number_classes=final_units,\n",
    "                        n_features=21)\n",
    "\n",
    "fps_x_hot = fps_x_hot.astype(np.int8)\n",
    "\n",
    "dl_path = '/home/amsequeira/enzymeClassification/models/try_attent'\n",
    "report_name = str(dl_path +\n",
    "                  'try_attentions')\n",
    "model_name = str(dl_path)\n",
    "\n",
    "model_reuse = load_model(model_name)\n",
    "model.summary()\n",
    "model_reuse.summary()\n",
    "########################################################################################################################\n",
    "# try keract\n",
    "# https://github.com/philipperemy/keract\n",
    "import keract\n",
    "activations = keract.get_activations(model_reuse, x_train[:1], layer_names=None, nodes_to_evaluate=None, output_format='simple', nested=False, auto_compile=True)\n",
    "[print(k, '->', v.shape, '- Numpy array') for (k, v) in activations.items()]\n",
    "# activations = keract.get_activations(model_reuse, x_train[:1],\n",
    "# layer_names=['bidirectional_12', 'bidirectional_13', 'bidirectional_14','attention_4', 'dense_12', 'dense_13', 'dense_14'],\n",
    "# nodes_to_evaluate=None, output_format='simple', nested=False, auto_compile=True)\n",
    "keract.display_activations(activations, cmap=None, save=False, directory='.', data_format='channels_last', fig_size=(24, 24), reshape_1d_layers=False)\n",
    "\n",
    "########################################################################################################################\n",
    "# https://github.com/mjDelta/attention-mechanism-keras/blob/master/attention_lstm.py\n",
    "from matplotlib import pyplot as plt\n",
    "TIME_STEPS=500\n",
    "def get_activation(model,layer_name,inputs):\n",
    "    layer=[l for l in model.layers if l.name==layer_name][0]\n",
    "\n",
    "    func=K.function([model.input],[layer.output])\n",
    "\n",
    "    return func([inputs])[0]\n",
    "\n",
    "act = get_activation(model_reuse,layer_name='attention_4',inputs=x_test[:50])\n",
    "attention_probs=np.mean(get_activation(model_reuse,\"attention_4\",x_test[:50]),axis=2).flatten()\n",
    "\n",
    "# todo\n",
    "# see mean of activations per time step\n",
    "# put by class\n",
    "layer_name = 'bidirectional_14'\n",
    "act = get_activation(model_reuse,layer_name=layer_name,inputs=x_test[:500])\n",
    "act_probs=np.mean(act,axis=0) # get means of lstm units for each timestep\n",
    "act_probs2=np.mean(act_probs,axis=1) # get means of sequences for each timestep\n",
    "plt.plot(act_probs2)\n",
    "plt.title(\"LSTM attention probs\")\n",
    "plt.show()\n",
    "\n",
    "# get weights it does not give nay information. is values from units of layers\n",
    "# https://stackoverflow.com/questions/57012563/interpreting-get-weight-in-lstm-model-in-keras\n",
    "def get_weights(model,layer_name):\n",
    "    layer=[l for l in model.layers if l.name==layer_name][0]\n",
    "    config = layer.get_config()\n",
    "    wei = layer.get_weights()\n",
    "    print(layer.get_config(), layer.get_weights())\n",
    "    return wei\n",
    "\n",
    "wei = get_weights(model_reuse,layer_name='bidirectional_14')\n",
    "\n",
    "\n",
    "# #################################################################################################\n",
    "# https://towardsdatascience.com/visualising-lstm-activations-in-keras-b50206da96ff\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import re\n",
    "\n",
    "# Imports for visualisations\n",
    "from IPython.display import HTML as html_print\n",
    "from IPython.display import display\n",
    "import keras.backend as K\n",
    "from tensorflow.python.keras import backend\n",
    "\n",
    "\n",
    "# layers\n",
    "# [<tensorflow.python.keras.layers.core.Masking at 0x7f00fb0bd550>,\n",
    "# <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f00fb0bd910>,\n",
    "# <tensorflow.python.keras.layers.wrappers.Bidirectional at 0x7f00fb0bddc0>,\n",
    "# <tensorflow.python.keras.layers.wrappers.Bidirectional at 0x7f00fafebf40>,\n",
    "# <tensorflow.python.keras.layers.wrappers.Bidirectional at 0x7f00faf9cf40>,\n",
    "# <tensorflow.python.keras.saving.saved_model.load.attention at 0x7f00fafb7c40>,    5\n",
    "# <tensorflow.python.keras.layers.core.Dense at 0x7f00faf46d30>,\n",
    "# <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f00faf4cc40>,\n",
    "# <tensorflow.python.keras.layers.core.Dropout at 0x7f00faf5d580>,\n",
    "# <tensorflow.python.keras.layers.core.Dense at 0x7f00faf60220>,\n",
    "# <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f00faf660a0>,\n",
    "# <tensorflow.python.keras.layers.core.Dropout at 0x7f00faf6d9a0>,\n",
    "# <tensorflow.python.keras.layers.core.Dense at 0x7f00faf72640>]\n",
    "# Backend Function to get Intermediate Layer Output\n",
    "# visualise outputs of second LSTM layer i.e. third layer in the whole architecture.\n",
    "# attn_func will return a hidden state vector of size 512. These will be activations of LSTM layer with 512 units.\n",
    "def get_activation(model,layer_name,inputs):\n",
    "    layer=[l for l in model.layers if l.name==layer_name][0]\n",
    "\n",
    "    func=K.function([model.input],[layer.output])\n",
    "    # dont know difference. the first one is from above. the second from the url\n",
    "    # func = K.function(inputs = [model.get_input_at(0), backend.symbolic_learning_phase()],\n",
    "    #                 outputs = [layer.output])\n",
    "    return func([inputs])[0]\n",
    "\n",
    "\n",
    "# These helper functions will help us visualise character sequence with each of their activation values. We are\n",
    "# passing the activations through sigmoid function as we need values in a scale that can denote their importance to the\n",
    "# whole output. get_clr function helps get appropriate colour for a given value.\n",
    "# get html element\n",
    "def cstr(s, color='black'):\n",
    "    if s == ' ':\n",
    "        return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
    "    else:\n",
    "        return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
    "\n",
    "# print html\n",
    "def print_color(t):\n",
    "    display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
    "\n",
    "# get appropriate color for value\n",
    "def get_clr(value):\n",
    "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
    "                                                          '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
    "              '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
    "              '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
    "    value = int((value * 100) / 5)\n",
    "    return colors[value]\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    z = 1/(1 + np.exp(-x))\n",
    "    return z\n",
    "# After applying sigmoid on the layer output, the values lie in the range 0 to 1. Closer the number is to 1, higher\n",
    "# importance it has. If the number is closer to 0, it is meant to not contribute in any major way to the final prediction.\n",
    "# The importance of these cells is denoted by the colour, where Blue denotes lower importance and Red denotes higher\n",
    "# importance.\n",
    "\n",
    "\n",
    "# visualize function takes as input the predicted sequence, the sigmoid values for each character in the sequence and the\n",
    "# cell number to visualise. Based on the value of the output, character is printed with an appropriate background colour.\n",
    "def visualize(output_values, seq, cell_no):\n",
    "    print(\"\\nCell Number:\", cell_no, \"\\n\")\n",
    "    text_colours = []\n",
    "    for i in range(len(seq)):\n",
    "        text = (seq[i], get_clr(output_values[i][int(cell_no-1)]))\n",
    "        text_colours.append(text)\n",
    "    print_color(text_colours)\n",
    "    text_colours.show_batch()\n",
    "    plt.show()\n",
    "\n",
    "# Get Predictions from random sequence\n",
    "# get_predictions function randomly chooses an input seed sequence and gets the predicted\n",
    "# sequence for that seed sequence.\n",
    "\n",
    "def get_predictions(model_name, layer_name, data):\n",
    "    # start = np.random.randint(0, len(data)-1)\n",
    "    # pattern = data[start]\n",
    "    result_list, output_values = [], []\n",
    "\n",
    "    # Prediction\n",
    "    prediction = model_reuse.predict(data, verbose=0)\n",
    "\n",
    "    # LSTM Activations\n",
    "    output = get_activation(model = model_name, layer_name=layer_name, inputs = data)[0]\n",
    "    output = sigmoid(output)\n",
    "    output_values.append(output)\n",
    "\n",
    "    # Saving generated characters\n",
    "    result_list.append(prediction)\n",
    "    return output, prediction\n",
    "\n",
    "# More than 90% of the cells do not show any understandable patterns. I visualised all 512 cells manually and noticed\n",
    "# three of them (189, 435, 463) to show some understandable patterns.\n",
    "seq_test=fps_x_hot[0]\n",
    "true_seq = sequences[0]\n",
    "seq_test = seq_test.reshape(1,seq_test.shape[0], seq_test.shape[1]) # , seq_test.shape[1])\n",
    "output_values, result_list = get_predictions(model_reuse, 'bidirectional_14', seq_test)\n",
    "\n",
    "for cell_no in [16, 32, 64]:\n",
    "    visualize(output_values,true_seq , cell_no)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}