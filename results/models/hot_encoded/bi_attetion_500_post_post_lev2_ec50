/home/amsequeira/enzymeClassification/models/hot_encoded/bi_attetion_500_post_post_lev2_ec50
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f8e9c0756a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f8e9c075580>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f8e9c075700>]
===TRAIN MODELS===

run_model
('Training Accuracy mean: ', 0.590297798315684)
('Validation Accuracy mean: ', 0.44307128470095375)
('Training Loss mean: ', 1.6672381285637143)
('Validation Loss mean: ', 2.3601190211280945)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking (Masking)            (None, 500, 21)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, 500, 128)          44032     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 500, 128)          98816     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 500, 64)           41216     
_________________________________________________________________
attention (attention)        (None, 64)                564       
_________________________________________________________________
dense (Dense)                (None, 32)                2080      
_________________________________________________________________
batch_normalization (BatchNo (None, 32)                128       
_________________________________________________________________
dropout (Dropout)            (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16)                64        
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 53)                901       
=================================================================
Total params: 188,329
Trainable params: 188,233
Non-trainable params: 96
_________________________________________________________________Finished run_model in 6203.8601 secs


===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f8e9c0758e0>, 'x_test': None, 'y_test': None, 'model': None}
report

              precision    recall  f1-score   support

           0       0.83      0.93      0.87       402
           1       1.00      0.74      0.85        19
           2       0.93      0.68      0.79        82
           3       0.71      0.31      0.43        16
           4       0.78      0.40      0.53        62
           5       0.94      0.78      0.85       277
           6       0.89      0.86      0.87        36
           7       0.53      0.31      0.39        26
           8       0.93      0.78      0.85        73
           9       0.83      0.69      0.75        29
          10       0.94      0.69      0.80       156
          11       0.93      0.65      0.77       168
          12       0.86      0.77      0.82        83
          13       0.80      0.30      0.44        53
          14       0.78      0.68      0.72        31
          15       0.67      0.77      0.71        52
          16       0.81      0.81      0.81        95
          17       0.96      0.88      0.92       884
          18       0.92      0.92      0.92        48
          19       0.84      0.77      0.80       781
          20       0.91      0.87      0.89       592
          21       0.91      0.94      0.93       385
          22       0.88      0.89      0.89       128
          23       0.79      0.93      0.86      1888
          24       0.92      0.86      0.89       168
          25       0.74      0.77      0.76      1295
          26       0.73      0.78      0.75       381
          27       1.00      0.14      0.25        14
          28       0.75      0.88      0.81       769
          29       0.66      0.87      0.75       372
          30       0.93      0.77      0.84       631
          31       0.83      0.45      0.59        11
          32       0.73      0.86      0.79       316
          33       0.87      0.81      0.84       405
          34       0.95      0.75      0.84        96
          35       0.77      0.38      0.51        26
          36       0.88      0.57      0.69        65
          37       1.00      0.95      0.98        22
          38       0.80      0.78      0.79       121
          39       0.89      0.83      0.86       113
          40       0.96      0.88      0.92       208
          41       0.89      0.88      0.88       194
          42       1.00      0.76      0.86        46
          43       0.98      0.99      0.99       431
          44       0.94      0.91      0.92        66
          45       0.92      0.93      0.93       489
          46       0.91      0.77      0.83        62
          47       0.90      0.60      0.72       263
          48       0.76      0.65      0.70        49
          49       0.72      0.94      0.82        31
          50       0.94      1.00      0.97        16
          51       0.94      0.81      0.87        21
          52       0.79      0.85      0.82        73

    accuracy                           0.84     13120
   macro avg       0.86      0.75      0.79     13120
weighted avg       0.84      0.84      0.83     13120


===confusion_matrix===

[[373   0   1 ...   0   0   0]
 [  0  14   1 ...   0   0   0]
 [  0   0  56 ...   0   0   0]
 ...
 [  0   0   0 ...  16   0   0]
 [  0   0   0 ...   0  17   4]
 [  0   0   0 ...   0   0  62]]

===multilabel confusion matrix===

[[[12639    79]
  [   29   373]]

 [[13101     0]
  [    5    14]]

 [[13034     4]
  [   26    56]]

 [[13102     2]
  [   11     5]]

 [[13051     7]
  [   37    25]]

 [[12828    15]
  [   61   216]]

 [[13080     4]
  [    5    31]]

 [[13087     7]
  [   18     8]]

 [[13043     4]
  [   16    57]]

 [[13087     4]
  [    9    20]]

 [[12957     7]
  [   48   108]]

 [[12944     8]
  [   58   110]]

 [[13027    10]
  [   19    64]]

 [[13063     4]
  [   37    16]]

 [[13083     6]
  [   10    21]]

 [[13048    20]
  [   12    40]]

 [[13007    18]
  [   18    77]]

 [[12203    33]
  [  102   782]]

 [[13068     4]
  [    4    44]]

 [[12223   116]
  [  180   601]]

 [[12475    53]
  [   75   517]]

 [[12700    35]
  [   22   363]]

 [[12977    15]
  [   14   114]]

 [[10780   452]
  [  137  1751]]

 [[12939    13]
  [   24   144]]

 [[11482   343]
  [  295  1000]]

 [[12630   109]
  [   85   296]]

 [[13106     0]
  [   12     2]]

 [[12121   230]
  [   89   680]]

 [[12579   169]
  [   50   322]]

 [[12452    37]
  [  144   487]]

 [[13108     1]
  [    6     5]]

 [[12702   102]
  [   44   272]]

 [[12665    50]
  [   78   327]]

 [[13020     4]
  [   24    72]]

 [[13091     3]
  [   16    10]]

 [[13050     5]
  [   28    37]]

 [[13098     0]
  [    1    21]]

 [[12976    23]
  [   27    94]]

 [[12995    12]
  [   19    94]]

 [[12905     7]
  [   25   183]]

 [[12904    22]
  [   24   170]]

 [[13074     0]
  [   11    35]]

 [[12681     8]
  [    4   427]]

 [[13050     4]
  [    6    60]]

 [[12592    39]
  [   33   456]]

 [[13053     5]
  [   14    48]]

 [[12839    18]
  [  104   159]]

 [[13061    10]
  [   17    32]]

 [[13078    11]
  [    2    29]]

 [[13103     1]
  [    0    16]]

 [[13098     1]
  [    4    17]]

 [[13031    16]
  [   11    62]]]

===scores report===
metrics	scores
Accuracy	0.8361
MCC	0.8262
log_loss	0.6746
f1 score weighted	0.8345
f1 score macro	0.7855
f1 score micro	0.8361
roc_auc ovr	0.9862
roc_auc ovo	0.9845
precision	0.8444
recall	0.8361

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f8e9c0756a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f8e9c075580>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f8e9c075700>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f8e9c0758e0>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([21., 21., 21., ..., 19., 28., 30.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.69      0.84      0.76       402
         1.0       0.80      0.42      0.55        19
         2.0       0.78      0.62      0.69        82
         3.0       0.00      0.00      0.00        16
         4.0       0.36      0.13      0.19        62
         5.0       0.70      0.57      0.63       277
         6.0       0.92      0.67      0.77        36
         7.0       0.80      0.15      0.26        26
         8.0       0.74      0.49      0.59        72
         9.0       0.67      0.67      0.67        30
        10.0       0.78      0.77      0.78       156
        11.0       0.62      0.49      0.55       168
        12.0       0.65      0.51      0.57        83
        13.0       0.30      0.17      0.22        53
        14.0       0.50      0.42      0.46        31
        15.0       0.60      0.46      0.52        52
        16.0       0.74      0.57      0.65        94
        17.0       0.78      0.87      0.82       885
        18.0       0.95      0.79      0.86        48
        19.0       0.68      0.76      0.72       781
        20.0       0.80      0.80      0.80       591
        21.0       0.82      0.73      0.77       385
        22.0       0.81      0.84      0.82       128
        23.0       0.77      0.83      0.80      1888
        24.0       0.82      0.68      0.74       169
        25.0       0.65      0.70      0.67      1296
        26.0       0.62      0.60      0.61       381
        27.0       1.00      0.29      0.44        14
        28.0       0.63      0.65      0.64       769
        29.0       0.53      0.60      0.56       372
        30.0       0.87      0.78      0.82       631
        31.0       1.00      0.27      0.43        11
        32.0       0.49      0.68      0.57       316
        33.0       0.77      0.66      0.71       405
        34.0       0.82      0.60      0.69        96
        35.0       0.17      0.04      0.06        26
        36.0       0.77      0.37      0.50        65
        37.0       1.00      0.62      0.76        21
        38.0       0.67      0.69      0.68       121
        39.0       0.95      0.64      0.76       114
        40.0       0.84      0.72      0.78       207
        41.0       0.68      0.74      0.71       194
        42.0       0.68      0.77      0.72        47
        43.0       0.95      0.92      0.93       431
        44.0       0.90      0.78      0.83        67
        45.0       0.86      0.83      0.84       488
        46.0       0.81      0.63      0.71        62
        47.0       0.93      0.89      0.91       264
        48.0       0.78      0.63      0.70        49
        49.0       0.74      0.87      0.80        30
        50.0       0.85      0.73      0.79        15
        51.0       0.90      0.86      0.88        21
        52.0       0.80      0.86      0.83        73

    accuracy                           0.73     13120
   macro avg       0.73      0.62      0.65     13120
weighted avg       0.74      0.73      0.73     13120


===confusion_matrix===

[[337   0   0 ...   0   0   0]
 [  2   8   0 ...   0   0   0]
 [  0   0  51 ...   0   0   0]
 ...
 [  0   0   0 ...  11   1   0]
 [  0   0   0 ...   0  18   1]
 [  0   0   0 ...   2   0  63]]

===multilabel confusion matrix===

[[[12565   153]
  [   65   337]]

 [[13099     2]
  [   11     8]]

 [[13024    14]
  [   31    51]]

 [[13102     2]
  [   16     0]]

 [[13044    14]
  [   54     8]]

 [[12775    68]
  [  120   157]]

 [[13082     2]
  [   12    24]]

 [[13093     1]
  [   22     4]]

 [[13036    12]
  [   37    35]]

 [[13080    10]
  [   10    20]]

 [[12931    33]
  [   36   120]]

 [[12902    50]
  [   85    83]]

 [[13014    23]
  [   41    42]]

 [[13046    21]
  [   44     9]]

 [[13076    13]
  [   18    13]]

 [[13052    16]
  [   28    24]]

 [[13007    19]
  [   40    54]]

 [[12019   216]
  [  113   772]]

 [[13070     2]
  [   10    38]]

 [[12057   282]
  [  184   597]]

 [[12410   119]
  [  120   471]]

 [[12673    62]
  [  103   282]]

 [[12966    26]
  [   20   108]]

 [[10772   460]
  [  315  1573]]

 [[12926    25]
  [   54   115]]

 [[11341   483]
  [  390   906]]

 [[12597   142]
  [  151   230]]

 [[13106     0]
  [   10     4]]

 [[12056   295]
  [  269   500]]

 [[12553   195]
  [  150   222]]

 [[12414    75]
  [  137   494]]

 [[13109     0]
  [    8     3]]

 [[12579   225]
  [  102   214]]

 [[12633    82]
  [  138   267]]

 [[13011    13]
  [   38    58]]

 [[13089     5]
  [   25     1]]

 [[13048     7]
  [   41    24]]

 [[13099     0]
  [    8    13]]

 [[12958    41]
  [   38    83]]

 [[13002     4]
  [   41    73]]

 [[12884    29]
  [   57   150]]

 [[12857    69]
  [   50   144]]

 [[13056    17]
  [   11    36]]

 [[12670    19]
  [   36   395]]

 [[13047     6]
  [   15    52]]

 [[12568    64]
  [   85   403]]

 [[13049     9]
  [   23    39]]

 [[12839    17]
  [   29   235]]

 [[13062     9]
  [   18    31]]

 [[13081     9]
  [    4    26]]

 [[13103     2]
  [    4    11]]

 [[13097     2]
  [    3    18]]

 [[13031    16]
  [   10    63]]]

===scores report===
metrics	scores
Accuracy	0.7348
MCC	0.7183
log_loss	1.1587
f1 score weighted	0.7319
f1 score macro	0.6517
f1 score micro	0.7348
roc_auc ovr	0.9707
roc_auc ovo	0.9670
precision	0.7384
recall	0.7348

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f8e9c0756a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f8e9c075580>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f8e9c075700>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f8e9c0758e0>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([21., 11., 11., ..., 25., 30., 28.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.76      0.70      0.73       402
         1.0       0.67      0.53      0.59        19
         2.0       0.93      0.46      0.61        81
         3.0       1.00      0.06      0.12        16
         4.0       0.53      0.26      0.35        62
         5.0       0.62      0.64      0.63       277
         6.0       0.96      0.67      0.79        36
         7.0       0.89      0.31      0.46        26
         8.0       0.84      0.37      0.51        73
         9.0       0.79      0.38      0.51        29
        10.0       0.89      0.63      0.74       156
        11.0       0.61      0.51      0.56       168
        12.0       0.62      0.59      0.60        83
        13.0       0.00      0.00      0.00        53
        14.0       0.79      0.34      0.48        32
        15.0       0.94      0.31      0.46        52
        16.0       0.75      0.57      0.65        95
        17.0       0.78      0.85      0.82       884
        18.0       0.70      0.58      0.64        48
        19.0       0.66      0.73      0.69       782
        20.0       0.67      0.77      0.72       591
        21.0       0.89      0.70      0.78       385
        22.0       0.83      0.79      0.81       128
        23.0       0.72      0.81      0.77      1888
        24.0       0.83      0.66      0.74       169
        25.0       0.53      0.74      0.62      1295
        26.0       0.52      0.65      0.58       381
        27.0       0.82      0.64      0.72        14
        28.0       0.63      0.59      0.61       769
        29.0       0.55      0.55      0.55       371
        30.0       0.86      0.79      0.82       631
        31.0       0.50      0.36      0.42        11
        32.0       0.58      0.59      0.58       316
        33.0       0.85      0.60      0.70       405
        34.0       0.73      0.49      0.59        96
        35.0       0.75      0.12      0.20        26
        36.0       0.93      0.40      0.56        65
        37.0       0.68      0.68      0.68        22
        38.0       0.62      0.60      0.61       121
        39.0       0.99      0.67      0.80       113
        40.0       0.86      0.75      0.80       208
        41.0       0.75      0.68      0.72       193
        42.0       0.67      0.61      0.64        46
        43.0       0.89      0.96      0.92       431
        44.0       0.81      0.70      0.75        66
        45.0       0.90      0.72      0.80       489
        46.0       0.90      0.60      0.72        62
        47.0       0.91      0.85      0.88       264
        48.0       0.65      0.41      0.50        49
        49.0       0.69      0.81      0.75        31
        50.0       0.91      0.62      0.74        16
        51.0       0.84      0.76      0.80        21
        52.0       0.83      0.78      0.80        73

    accuracy                           0.71     13120
   macro avg       0.75      0.58      0.63     13120
weighted avg       0.72      0.71      0.71     13120


===confusion_matrix===

[[282   0   0 ...   0   0   0]
 [  0  10   0 ...   0   0   0]
 [  0   0  37 ...   0   0   0]
 ...
 [  0   0   0 ...  10   0   1]
 [  0   0   0 ...   0  16   2]
 [  0   0   0 ...   0   3  57]]

===multilabel confusion matrix===

[[[12628    90]
  [  120   282]]

 [[13096     5]
  [    9    10]]

 [[13036     3]
  [   44    37]]

 [[13104     0]
  [   15     1]]

 [[13044    14]
  [   46    16]]

 [[12735   108]
  [  101   176]]

 [[13083     1]
  [   12    24]]

 [[13093     1]
  [   18     8]]

 [[13042     5]
  [   46    27]]

 [[13088     3]
  [   18    11]]

 [[12952    12]
  [   57    99]]

 [[12898    54]
  [   82    86]]

 [[13007    30]
  [   34    49]]

 [[13062     5]
  [   53     0]]

 [[13085     3]
  [   21    11]]

 [[13067     1]
  [   36    16]]

 [[13007    18]
  [   41    54]]

 [[12026   210]
  [  129   755]]

 [[13060    12]
  [   20    28]]

 [[12051   287]
  [  215   567]]

 [[12307   222]
  [  136   455]]

 [[12702    33]
  [  115   270]]

 [[12972    20]
  [   27   101]]

 [[10647   585]
  [  350  1538]]

 [[12928    23]
  [   57   112]]

 [[10990   835]
  [  337   958]]

 [[12514   225]
  [  134   247]]

 [[13104     2]
  [    5     9]]

 [[12085   266]
  [  316   453]]

 [[12579   170]
  [  166   205]]

 [[12408    81]
  [  134   497]]

 [[13105     4]
  [    7     4]]

 [[12667   137]
  [  130   186]]

 [[12671    44]
  [  163   242]]

 [[13007    17]
  [   49    47]]

 [[13093     1]
  [   23     3]]

 [[13053     2]
  [   39    26]]

 [[13091     7]
  [    7    15]]

 [[12955    44]
  [   49    72]]

 [[13006     1]
  [   37    76]]

 [[12887    25]
  [   53   155]]

 [[12884    43]
  [   61   132]]

 [[13060    14]
  [   18    28]]

 [[12637    52]
  [   17   414]]

 [[13043    11]
  [   20    46]]

 [[12590    41]
  [  137   352]]

 [[13054     4]
  [   25    37]]

 [[12834    22]
  [   40   224]]

 [[13060    11]
  [   29    20]]

 [[13078    11]
  [    6    25]]

 [[13103     1]
  [    6    10]]

 [[13096     3]
  [    5    16]]

 [[13035    12]
  [   16    57]]]

===scores report===
metrics	scores
Accuracy	0.7080
MCC	0.6894
log_loss	1.2304
f1 score weighted	0.7057
f1 score macro	0.6339
f1 score micro	0.7080
roc_auc ovr	0.9640
roc_auc ovo	0.9618
precision	0.7226
recall	0.7080

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f8e9c0756a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f8e9c075580>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f8e9c075700>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f8e9c0758e0>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([21., 21., 21., ..., 28., 28., 17.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.68      0.83      0.75       401
         1.0       0.92      0.55      0.69        20
         2.0       0.81      0.63      0.71        82
         3.0       0.40      0.12      0.19        16
         4.0       0.55      0.34      0.42        62
         5.0       0.71      0.63      0.67       277
         6.0       0.86      0.86      0.86        36
         7.0       0.71      0.20      0.31        25
         8.0       0.72      0.52      0.60        73
         9.0       0.68      0.52      0.59        29
        10.0       0.82      0.72      0.77       156
        11.0       0.65      0.52      0.58       168
        12.0       0.64      0.57      0.60        83
        13.0       0.32      0.11      0.16        54
        14.0       0.48      0.32      0.38        31
        15.0       0.77      0.51      0.61        53
        16.0       0.74      0.58      0.65        95
        17.0       0.89      0.82      0.85       884
        18.0       0.86      0.79      0.82        47
        19.0       0.69      0.75      0.72       782
        20.0       0.79      0.76      0.77       592
        21.0       0.84      0.73      0.78       385
        22.0       0.81      0.87      0.84       128
        23.0       0.79      0.81      0.80      1887
        24.0       0.80      0.71      0.75       168
        25.0       0.66      0.71      0.69      1295
        26.0       0.64      0.65      0.65       381
        27.0       0.88      0.50      0.64        14
        28.0       0.63      0.62      0.62       768
        29.0       0.52      0.68      0.59       372
        30.0       0.87      0.79      0.83       631
        31.0       0.50      0.10      0.17        10
        32.0       0.52      0.64      0.57       316
        33.0       0.54      0.71      0.61       405
        34.0       0.74      0.68      0.71        96
        35.0       0.67      0.15      0.25        26
        36.0       0.83      0.36      0.51        66
        37.0       0.83      0.68      0.75        22
        38.0       0.61      0.65      0.63       121
        39.0       0.86      0.84      0.85       113
        40.0       0.85      0.71      0.77       208
        41.0       0.80      0.74      0.77       194
        42.0       0.76      0.67      0.71        46
        43.0       0.94      0.93      0.93       431
        44.0       0.82      0.83      0.83        66
        45.0       0.84      0.84      0.84       489
        46.0       0.70      0.68      0.69        62
        47.0       0.81      0.89      0.85       263
        48.0       0.69      0.58      0.63        50
        49.0       0.95      0.58      0.72        31
        50.0       0.74      0.88      0.80        16
        51.0       0.75      0.86      0.80        21
        52.0       0.77      0.88      0.82        73

    accuracy                           0.74     13120
   macro avg       0.73      0.63      0.66     13120
weighted avg       0.74      0.74      0.73     13120


===confusion_matrix===

[[333   0   0 ...   0   0   0]
 [  0  11   0 ...   0   0   0]
 [  0   0  52 ...   0   0   0]
 ...
 [  0   0   0 ...  14   1   0]
 [  0   0   0 ...   0  18   2]
 [  0   0   0 ...   0   3  64]]

===multilabel confusion matrix===

[[[12563   156]
  [   68   333]]

 [[13099     1]
  [    9    11]]

 [[13026    12]
  [   30    52]]

 [[13101     3]
  [   14     2]]

 [[13041    17]
  [   41    21]]

 [[12772    71]
  [  102   175]]

 [[13079     5]
  [    5    31]]

 [[13093     2]
  [   20     5]]

 [[13032    15]
  [   35    38]]

 [[13084     7]
  [   14    15]]

 [[12940    24]
  [   43   113]]

 [[12905    47]
  [   81    87]]

 [[13011    26]
  [   36    47]]

 [[13053    13]
  [   48     6]]

 [[13078    11]
  [   21    10]]

 [[13059     8]
  [   26    27]]

 [[13006    19]
  [   40    55]]

 [[12146    90]
  [  159   725]]

 [[13067     6]
  [   10    37]]

 [[12068   270]
  [  193   589]]

 [[12407   121]
  [  144   448]]

 [[12683    52]
  [  105   280]]

 [[12966    26]
  [   17   111]]

 [[10832   401]
  [  353  1534]]

 [[12923    29]
  [   49   119]]

 [[11356   469]
  [  376   919]]

 [[12599   140]
  [  132   249]]

 [[13105     1]
  [    7     7]]

 [[12072   280]
  [  293   475]]

 [[12516   232]
  [  120   252]]

 [[12413    76]
  [  132   499]]

 [[13109     1]
  [    9     1]]

 [[12621   183]
  [  115   201]]

 [[12468   247]
  [  117   288]]

 [[13001    23]
  [   31    65]]

 [[13092     2]
  [   22     4]]

 [[13049     5]
  [   42    24]]

 [[13095     3]
  [    7    15]]

 [[12948    51]
  [   42    79]]

 [[12992    15]
  [   18    95]]

 [[12885    27]
  [   60   148]]

 [[12891    35]
  [   51   143]]

 [[13064    10]
  [   15    31]]

 [[12662    27]
  [   29   402]]

 [[13042    12]
  [   11    55]]

 [[12554    77]
  [   77   412]]

 [[13040    18]
  [   20    42]]

 [[12802    55]
  [   28   235]]

 [[13057    13]
  [   21    29]]

 [[13088     1]
  [   13    18]]

 [[13099     5]
  [    2    14]]

 [[13093     6]
  [    3    18]]

 [[13028    19]
  [    9    64]]]

===scores report===
metrics	scores
Accuracy	0.7359
MCC	0.7199
log_loss	1.1354
f1 score weighted	0.7349
f1 score macro	0.6626
f1 score micro	0.7359
roc_auc ovr	0.9702
roc_auc ovo	0.9704
precision	0.7423
recall	0.7359

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f8e9c0756a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f8e9c075580>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f8e9c075700>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f8e9c0758e0>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([21., 11., 11., ..., 17., 19., 50.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.72      0.80      0.76       401
         1.0       1.00      0.30      0.46        20
         2.0       0.70      0.71      0.70        82
         3.0       0.67      0.13      0.22        15
         4.0       0.44      0.34      0.38        62
         5.0       0.64      0.70      0.67       278
         6.0       0.86      0.69      0.77        36
         7.0       0.77      0.40      0.53        25
         8.0       0.85      0.64      0.73        73
         9.0       0.76      0.45      0.57        29
        10.0       0.85      0.67      0.75       156
        11.0       0.74      0.57      0.65       168
        12.0       0.62      0.57      0.59        83
        13.0       0.25      0.04      0.06        54
        14.0       0.63      0.55      0.59        31
        15.0       0.56      0.42      0.48        53
        16.0       0.67      0.58      0.62        95
        17.0       0.88      0.83      0.86       884
        18.0       0.91      0.85      0.88        47
        19.0       0.74      0.73      0.73       781
        20.0       0.78      0.78      0.78       592
        21.0       0.84      0.77      0.80       385
        22.0       0.84      0.85      0.85       129
        23.0       0.78      0.84      0.81      1887
        24.0       0.89      0.76      0.82       168
        25.0       0.62      0.75      0.68      1295
        26.0       0.62      0.64      0.63       381
        27.0       1.00      0.31      0.47        13
        28.0       0.64      0.65      0.65       769
        29.0       0.54      0.61      0.58       372
        30.0       0.91      0.79      0.85       631
        31.0       0.67      0.36      0.47        11
        32.0       0.54      0.66      0.59       316
        33.0       0.71      0.68      0.70       405
        34.0       0.82      0.64      0.72        95
        35.0       0.57      0.16      0.25        25
        36.0       0.57      0.45      0.50        66
        37.0       0.90      0.82      0.86        22
        38.0       0.75      0.69      0.72       121
        39.0       0.85      0.73      0.78       113
        40.0       0.78      0.76      0.77       208
        41.0       0.84      0.73      0.78       194
        42.0       0.86      0.52      0.65        46
        43.0       0.91      0.93      0.92       431
        44.0       0.82      0.80      0.81        66
        45.0       0.89      0.83      0.86       489
        46.0       0.75      0.74      0.75        62
        47.0       0.84      0.92      0.88       263
        48.0       0.76      0.44      0.56        50
        49.0       0.73      0.71      0.72        31
        50.0       1.00      0.75      0.86        16
        51.0       0.94      0.73      0.82        22
        52.0       0.74      0.82      0.78        73

    accuracy                           0.75     13120
   macro avg       0.75      0.63      0.67     13120
weighted avg       0.75      0.75      0.74     13120


===confusion_matrix===

[[322   0   0 ...   0   0   1]
 [  0   6   0 ...   0   0   0]
 [  0   0  58 ...   0   0   0]
 ...
 [  0   0   0 ...  12   0   0]
 [  0   0   0 ...   0  16   4]
 [  0   0   0 ...   0   1  60]]

===multilabel confusion matrix===

[[[12594   125]
  [   79   322]]

 [[13100     0]
  [   14     6]]

 [[13013    25]
  [   24    58]]

 [[13104     1]
  [   13     2]]

 [[13031    27]
  [   41    21]]

 [[12733   109]
  [   83   195]]

 [[13080     4]
  [   11    25]]

 [[13092     3]
  [   15    10]]

 [[13039     8]
  [   26    47]]

 [[13087     4]
  [   16    13]]

 [[12945    19]
  [   51   105]]

 [[12919    33]
  [   72    96]]

 [[13008    29]
  [   36    47]]

 [[13060     6]
  [   52     2]]

 [[13079    10]
  [   14    17]]

 [[13050    17]
  [   31    22]]

 [[12998    27]
  [   40    55]]

 [[12139    97]
  [  147   737]]

 [[13069     4]
  [    7    40]]

 [[12143   196]
  [  214   567]]

 [[12400   128]
  [  132   460]]

 [[12677    58]
  [   88   297]]

 [[12970    21]
  [   19   110]]

 [[10797   436]
  [  298  1589]]

 [[12936    16]
  [   40   128]]

 [[11220   605]
  [  325   970]]

 [[12587   152]
  [  137   244]]

 [[13107     0]
  [    9     4]]

 [[12073   278]
  [  266   503]]

 [[12558   190]
  [  145   227]]

 [[12437    52]
  [  130   501]]

 [[13107     2]
  [    7     4]]

 [[12626   178]
  [  108   208]]

 [[12603   112]
  [  129   276]]

 [[13012    13]
  [   34    61]]

 [[13092     3]
  [   21     4]]

 [[13031    23]
  [   36    30]]

 [[13096     2]
  [    4    18]]

 [[12971    28]
  [   37    84]]

 [[12993    14]
  [   31    82]]

 [[12867    45]
  [   49   159]]

 [[12899    27]
  [   53   141]]

 [[13070     4]
  [   22    24]]

 [[12650    39]
  [   30   401]]

 [[13042    12]
  [   13    53]]

 [[12583    48]
  [   85   404]]

 [[13043    15]
  [   16    46]]

 [[12809    48]
  [   20   243]]

 [[13063     7]
  [   28    22]]

 [[13081     8]
  [    9    22]]

 [[13104     0]
  [    4    12]]

 [[13097     1]
  [    6    16]]

 [[13026    21]
  [   13    60]]]

===scores report===
metrics	scores
Accuracy	0.7462
MCC	0.7305
log_loss	1.0629
f1 score weighted	0.7449
f1 score macro	0.6728
f1 score micro	0.7462
roc_auc ovr	0.9721
roc_auc ovo	0.9701
precision	0.7517
recall	0.7462

===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f8e9c0756a0>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f8e9c075580>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f8e9c075700>]
===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f8e9c0758e0>, 'x_test': array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       ...,

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 1],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0],
        [1, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],

       [[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 1, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]]], dtype=int8), 'y_test': array([21., 21., 21., ..., 47., 26., 30.], dtype=float32), 'model': None}
report

              precision    recall  f1-score   support

         0.0       0.78      0.76      0.77       402
         1.0       0.92      0.58      0.71        19
         2.0       0.75      0.54      0.62        82
         3.0       0.00      0.00      0.00        16
         4.0       0.52      0.23      0.31        62
         5.0       0.74      0.57      0.64       278
         6.0       1.00      0.64      0.78        36
         7.0       0.71      0.19      0.30        26
         8.0       0.76      0.56      0.65        73
         9.0       0.83      0.50      0.62        30
        10.0       0.84      0.64      0.73       156
        11.0       0.54      0.66      0.59       169
        12.0       0.75      0.57      0.64        83
        13.0       0.00      0.00      0.00        53
        14.0       0.76      0.42      0.54        31
        15.0       0.59      0.33      0.42        52
        16.0       0.80      0.58      0.67        95
        17.0       0.79      0.84      0.81       885
        18.0       0.83      0.79      0.81        48
        19.0       0.73      0.75      0.74       781
        20.0       0.76      0.76      0.76       592
        21.0       0.81      0.76      0.78       384
        22.0       0.81      0.76      0.78       128
        23.0       0.67      0.88      0.76      1887
        24.0       0.84      0.67      0.74       168
        25.0       0.70      0.67      0.68      1295
        26.0       0.61      0.54      0.57       381
        27.0       0.75      0.43      0.55        14
        28.0       0.68      0.68      0.68       769
        29.0       0.56      0.59      0.57       372
        30.0       0.88      0.80      0.83       630
        31.0       1.00      0.09      0.17        11
        32.0       0.50      0.66      0.57       316
        33.0       0.71      0.70      0.70       405
        34.0       0.67      0.69      0.68        95
        35.0       0.00      0.00      0.00        25
        36.0       0.92      0.34      0.49        65
        37.0       1.00      0.86      0.93        22
        38.0       0.86      0.66      0.75       121
        39.0       0.90      0.75      0.82       113
        40.0       0.82      0.79      0.80       208
        41.0       0.79      0.73      0.76       194
        42.0       0.69      0.57      0.63        47
        43.0       0.91      0.96      0.94       431
        44.0       0.85      0.88      0.87        66
        45.0       0.86      0.83      0.85       488
        46.0       0.80      0.70      0.75        63
        47.0       0.91      0.82      0.86       263
        48.0       0.88      0.47      0.61        49
        49.0       0.86      0.60      0.71        30
        50.0       0.78      0.93      0.85        15
        51.0       0.95      0.82      0.88        22
        52.0       0.76      0.86      0.81        73

    accuracy                           0.73     13119
   macro avg       0.74      0.61      0.65     13119
weighted avg       0.74      0.73      0.73     13119


===confusion_matrix===

[[307   0   0 ...   0   0   0]
 [  0  11   0 ...   0   0   0]
 [  0   0  44 ...   0   0   0]
 ...
 [  0   0   0 ...  14   0   0]
 [  0   0   0 ...   0  18   3]
 [  0   0   0 ...   0   1  63]]

===multilabel confusion matrix===

[[[12630    87]
  [   95   307]]

 [[13099     1]
  [    8    11]]

 [[13022    15]
  [   38    44]]

 [[13102     1]
  [   16     0]]

 [[13044    13]
  [   48    14]]

 [[12785    56]
  [  120   158]]

 [[13083     0]
  [   13    23]]

 [[13091     2]
  [   21     5]]

 [[13033    13]
  [   32    41]]

 [[13086     3]
  [   15    15]]

 [[12944    19]
  [   56   100]]

 [[12855    95]
  [   58   111]]

 [[13020    16]
  [   36    47]]

 [[13063     3]
  [   53     0]]

 [[13084     4]
  [   18    13]]

 [[13055    12]
  [   35    17]]

 [[13010    14]
  [   40    55]]

 [[12039   195]
  [  145   740]]

 [[13063     8]
  [   10    38]]

 [[12119   219]
  [  192   589]]

 [[12384   143]
  [  144   448]]

 [[12666    69]
  [   92   292]]

 [[12968    23]
  [   31    97]]

 [[10424   808]
  [  222  1665]]

 [[12929    22]
  [   56   112]]

 [[11456   368]
  [  432   863]]

 [[12608   130]
  [  176   205]]

 [[13103     2]
  [    8     6]]

 [[12102   248]
  [  243   526]]

 [[12570   177]
  [  151   221]]

 [[12420    69]
  [  129   501]]

 [[13108     0]
  [   10     1]]

 [[12595   208]
  [  108   208]]

 [[12596   118]
  [  122   283]]

 [[12991    33]
  [   29    66]]

 [[13093     1]
  [   25     0]]

 [[13052     2]
  [   43    22]]

 [[13097     0]
  [    3    19]]

 [[12985    13]
  [   41    80]]

 [[12997     9]
  [   28    85]]

 [[12875    36]
  [   44   164]]

 [[12887    38]
  [   52   142]]

 [[13060    12]
  [   20    27]]

 [[12648    40]
  [   16   415]]

 [[13043    10]
  [    8    58]]

 [[12566    65]
  [   82   406]]

 [[13045    11]
  [   19    44]]

 [[12834    22]
  [   48   215]]

 [[13067     3]
  [   26    23]]

 [[13086     3]
  [   12    18]]

 [[13100     4]
  [    1    14]]

 [[13096     1]
  [    4    18]]

 [[13026    20]
  [   10    63]]]

===scores report===
metrics	scores
Accuracy	0.7344
MCC	0.7177
log_loss	1.0846
f1 score weighted	0.7296
f1 score macro	0.6510
f1 score micro	0.7344
roc_auc ovr	0.9717
roc_auc ovo	0.9690
precision	0.7374
recall	0.7344

===TRAIN MODELS with CV===

train_model_cv	Accuracy	MCC	log_loss	f1 score weighted	f1 score macro	f1 score micro	roc_auc ovr	roc_auc ovo	precision	recall
0	0.7347560975609756	0.7182611920707451	1.1587334317721139	0.7318939808495798	0.6516850805755798	0.7347560975609757	0.9707068161144206	0.9669997858453047	0.7384342232588084	0.7347560975609756
1	0.7080030487804878	0.6894359878625964	1.2303871713511985	0.705656340473654	0.633912027063583	0.7080030487804878	0.963995472548123	0.961795008036288	0.722567202342394	0.7080030487804878
2	0.7358993902439024	0.7199177463890525	1.1353803735592598	0.7348948344340872	0.6626375279588121	0.7358993902439025	0.9701659000847838	0.9704094962799594	0.7423305378713214	0.7358993902439024
3	0.7461890243902439	0.7304816241176747	1.0629392875744477	0.7448913372912741	0.6727619945791546	0.7461890243902439	0.9721100368017133	0.970063592993039	0.7517344873499807	0.7461890243902439
4	0.734430977970882	0.7177364553087275	1.084572868373533	0.7295584890952334	0.6509855945362529	0.734430977970882	0.9716804984784998	0.9689705542276303	0.737395288312348	0.734430977970882
mean	0.7318557077892983	0.7151666011497593	1.1344026265261105	0.7293789964287656	0.6543964449426765	0.7318557077892983	0.9697317448055081	0.9676476874764444	0.7384923478269705	0.7318557077892983
std	0.012694793828203819	0.013677488169198117	0.05898472773544643	0.012962862012336991	0.012990809334296846	0.012694793828203833	0.0029495080258254997	0.0031587274946263157	0.009432954348584521	0.012694793828203819

Accuracy
MCC
log_loss
f1 score weighted
f1 score macro
f1 score micro
roc_auc ovr
roc_auc ovo
precision
recallFinished train_model_cv in 30281.5624 secs

