/home/amsequeira/enzymeClassification/models/visualization/hot_500_post_1_ec90/bi_attetion_hot_500_post_90_1
self
x_train
y_train
x_test
y_test
number_classes
problem_type
x_dval
y_dval
model
epochs
batch_size
callbacks
reduce_lr
early_stopping
checkpoint
tensorboard
early_stopping_patience
reduce_lr_patience
reduce_lr_factor
reduce_lr_min
path
report_name
verbose
validation_split
shuffle
class_weights
===Callbacks===

generate_callbacks
[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f6a2c25d730>, <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7f6a2c25d6d0>, <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f6a2c25d310>]
===TRAIN MODELS===

run_model
('Training Accuracy mean: ', 0.8852972451890453)
('Validation Accuracy mean: ', 0.6716230240430725)
('Training Loss mean: ', 0.44857134343533034)
('Validation Loss mean: ', 1.2694742210795371)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking (Masking)            (None, 500, 21)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, 500, 128)          44032     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 500, 128)          98816     
_________________________________________________________________
bidirectional_2 (Bidirection (None, 500, 64)           41216     
_________________________________________________________________
attention (attention)        (None, 64)                564       
_________________________________________________________________
dense (Dense)                (None, 32)                2080      
_________________________________________________________________
batch_normalization (BatchNo (None, 32)                128       
_________________________________________________________________
dropout (Dropout)            (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                528       
_________________________________________________________________
batch_normalization_1 (Batch (None, 16)                64        
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 7)                 119       
=================================================================
Total params: 187,547
Trainable params: 187,451
Non-trainable params: 96
_________________________________________________________________Finished run_model in 18954.2988 secs


===SCORING TEST SET ===

model_complete_evaluate
{'self': <deep_ml.DeepML object at 0x7f6a2c25d670>, 'x_test': None, 'y_test': None, 'model': None}
report

              precision    recall  f1-score   support

           0       0.91      0.94      0.93      3813
           1       0.95      0.95      0.95     10869
           2       0.90      0.93      0.92      6897
           3       0.95      0.95      0.95      2585
           4       0.94      0.97      0.95      1616
           5       0.98      0.99      0.98      3258
           6       0.99      0.71      0.83      1372

    accuracy                           0.94     30410
   macro avg       0.95      0.92      0.93     30410
weighted avg       0.94      0.94      0.94     30410


===confusion_matrix===

[[ 3581    87   101    17    10    17     0]
 [   84 10325   329    54    46    29     2]
 [  111   289  6397    47    35    15     3]
 [   29    27    72  2443     6     8     0]
 [    5    11    27    12  1561     0     0]
 [    5    16    23     1     3  3210     0]
 [  105   143   132    11     3     5   973]]

===multilabel confusion matrix===

[[[26258   339]
  [  232  3581]]

 [[18968   573]
  [  544 10325]]

 [[22829   684]
  [  500  6397]]

 [[27683   142]
  [  142  2443]]

 [[28691   103]
  [   55  1561]]

 [[27078    74]
  [   48  3210]]

 [[29033     5]
  [  399   973]]]

===scores report===
metrics	scores
Accuracy	0.9369
MCC	0.9192
log_loss	0.2584
f1 score weighted	0.9362
f1 score macro	0.9281
f1 score micro	0.9369
roc_auc ovr	0.9938
roc_auc ovo	0.9941
precision	0.9379
recall	0.9369
